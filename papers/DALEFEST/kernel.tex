The implementation of weak head reduction given before is well-known to be very inefficient, because the body of the abstraction of a $\beta$-redex is immediately and completely traversed when the redex is fired, unless the reduction of $\lambda$Prolog is implemented via explicit substitutions like in Teyjus. Moreover, the strategy implemented by \verb+whd*+ is call-by-name, that is inefficient per se.

To speed up reduction, we now provide a different implementation based on the reduction machine for call-by-need called Wadsworth Abstract Machine~\cite{beniamino}. The machine is a modified Krivine Abstract Machine (KAM~\cite{kam}) that records in the machine environment both terms and their normal forms, the latter being computed lazily by a new invocation of the reduction machine that is fired only when the normal form is required for the first time, i.e. when the variable bound to that value occurs in head position during reduction.

In standard implementations, variables are represented by De Bruijn indexes and machine environments are stacks, indexed by the variables. Since we do not use and do not want to use indexes, we take a different approach. The machine environment is represented by a set of \verb+(val N T V NF)+ assumptions that we dynamically add to the $\lambda$Prolog environment using logical implication. Their meaning is that to the variable \verb+N+ we associate a value \verb+V+ of type \verb+T+ and its normal form \verb+NF+. The latter is initialised with a fresh metavariable when the binding for \verb+N+ is destroyed by a $\beta$-reduction, and it is filled in when the value is requested for the first time. To compute the term to be aligned to \verb+NF+ we first start a new machine on \verb+V+ and then we decode the final machine state, an operation that we call \emph{unwind}.  We call a variable \verb+N+ for which a \verb+val N _ _ _+ assumption is present \emph{val-bound}.

Fetching the normal form of \verb+x+ from the environment via a call to \verb+val x _ _ NF+ amounts to fetching a clause from the current program, that is performed efficiently in any reasonable implementation of $\lambda$Prolog thanks to clause indexing.  ELPI, as most Prolog engines, indexes clauses on the head predicate \emph{and its first argument}: since \verb+x+ is a fresh constant there is little risk of having collisions.  Even if the indexing ignores the predicate argument the cost of the lookup is at worst $O(n)$ where $n$ is the size of the reduction machine environment.  In line with what one would obtain by representing the environment as a linked list and variables with De Bruijn indexes.

An assumption $A$ introduced in $\lambda$Prolog via $A \Rightarrow B$ is only visible in $B$. Therefore, once a variable is bound via an assumption $A$ of the form \verb+(val x t n nf)+, we need either 1) to be sure that the rest of the computations that requires \verb+x+ is performed in $B$; or 2) to reintroduce syntactically the binding again around every term that escapes the scope of the assumption.

We force 1) by coding the reduction machine in Continuation Passing Style (CPS): the \verb+whd1+ predicate takes in input the head and stack of the machine together with a continuation \verb+K+, and apply \verb+K+ to the new machine head and stack. Moreover, it also passes to \verb+K+ the list of val-bound variables.
The \verb+whd*+ predicate is also implemented in CPS style, and it is responsible for composing together the lists of val-bound variables.

Such list of variables is used when reduction is over to end the CPS style by performing 1).  For this purpose we introduce the term constructor for local abbreviations \verb+abbr+ (also called \verb+let .. in+ in several functional programming languages).  For example the \verb+whd_unwind t NF+ predicate computes the normal form \verb+NF+ of \verb+t+ by calling \verb+whd*+ on \verb+t+ and passing a continuation that unwinds the final machine state by introducing the explicit binder \verb+abbr ty val n \ ..+ for each val-bound variable \verb+n+.  The signature and type-checking rule for \verb+abbr+ follow:

\begin{Verbatim}
type abbr term -> term -> (term -> term) -> term.  % local definition (let-in)

of (abbr TY TE F) (abbr TY TE G) :-
 of TE TY', sub TY' TY, pi x \ of x TY => val x TY TE NF => of (F x) (G x).
\end{Verbatim}

The code of the reduction machine is the following:

\begin{Verbatim}
type whd1 term -> list term -> (list var -> term -> list term -> prop) -> prop.

% KAM-like rules in CPS style
whd1 (app M N) S K :- K [] M [N|S].
whd1 (lam T F1) [N|NS] K :- pi x \ val x T N NF => K [x] (F1 x) NS.
whd1 X S K :- val X _ N NF, if (var NF) (whd_unwind N NF), K [] NF S.

% reflexive, transitive closure
whd* T1 S1 K :- whd1 T1 S1
 (vl1 \ t1 \ s1 \ whd* t1 s1
  (vl2 \ t2 \ s2 \ sigma VL \ append vl1 vl2 VL, K VL t2 s2)), !.
whd* T1 S1 K :- K [] T1 S1.

% Whd followed by machine unwinding.
type whd_unwind term -> term -> prop.
whd_unwind N NF :-
 whd* N [] (l \ t \ s \ sigma TS \ unwind_stack s t TS, put_abbr l TS NF).

% unwind_stack takes an head and a stack and decodes them to a term
unwind_stack [] T T.
unwind_stack [X|XS] T O :- unwind_stack XS (app T X) O.

% put_abbr takes a list of variables and a term and wraps the latter
% with local definitions for the variables in the list
put_abbr [] NF NF.
put_abbr [X|XS] I (abbr T N K) :- val X T N _, put_abbr XS I (K X).
\end{Verbatim}

The predicate \verb+match_sort+ is implemented trivially. The one for \verb+match_arr+ is slightly more delicate: the input \verb+T+ is put in weak head normal form by \verb+whd*+; the continuation retrieves the term and checks that it is a dependent product, projecting out the two subterms; the subterms may contain variables bound in the environment; therefore, before returning them, it is necessary to close them using technique 2) above by means of \verb+abbr+s via \verb+put_abbr+, like \verb+whd_unwind+ does.

\begin{Verbatim}
type match_sort term -> @univ -> prop.
match_sort T I :- whd* T [] (l \ t \ s \ t = sort I, s = []).

type match_arr term -> term -> (term -> term) -> prop.
match_arr T A F :-
 whd* T [] (l \ t \ s \ sigma A' \sigma F' \
  s = [], t = arr A' F', put_abbr l A' A,
  pi x \ put_abbr l (F' x) (F x)).
\end{Verbatim}

Efficient reduction is not sufficient to obtain a quick term comparison
routine.  In particular both Matita and Coq implement the same heuristic: in
place of putting the two terms to be compared in weak head normal form
immediately, the two terms are lazily reduced on demand during comparison, that
at every step tries $\alpha$-conversion (i.e. $\lambda$Prolog equality) and
congruence rules to avoid unnecessary reductions.

In order to perform weak head reductions lazily, we obtain \verb+conv+ and \verb+sub+ as two instances of a \verb+comp+ comparison predicate that works on two reduction machine statuses. The third argument is \verb+eq+ if the intended comparison is \verb+conv+ while \verb+leq+ for \verb+sub+.
\begin{Verbatim}
type conv term -> term -> prop.
type sub term -> term -> prop.
type comp term -> list term -> eq_or_leq ->  term -> list term -> prop.

conv T1 T2 :- comp T1 [] eq T2 [].
sub T1 T2 :- comp T1 [] leq T2 [].

% fast path + axiom rules
comp T1 S1 _ T1 S1 :- !.
comp (sort I) [] leq (sort J) [] :- lt I J.
% congruence + fast path rule
comp X S1 _ X S2 :- map S1 S2 conv, !.
% congruence rules
comp (lam T1 F1) [] eq (lam T2 F2) [] :- conv T1 T2, pi x \ conv (F1 x) (F2 x).
comp (arr T1 F1) [] D (arr T2 F2) [] :- conv T1 T2, pi x \ comp (F1 x) [] D (F2 x) [].
% reduction rules
comp T1 S1 D T2 S2 :- whd1 T1 S1 (_ \ t1 \ s1 \ comp t1 s1 D T2 S2), !.
comp T1 S1 D T2 S2 :- whd1 T2 S2 (_ \ t2 \ s2 \ comp T1 S1 D t2 s2), !.
\end{Verbatim}

\paragraph{Extensions with global definitions, declarations, primitive inductive types, case analysis and structural recursive definitions.}
We implemented all the extensions mentioned in a modular way. To activate one extension, it is sufficient to accumulate in the kernel a $\lambda$Prolog file that adds new constructors to the \verb+term+ and to the reduction machine stack type, and the relative clauses to the \verb+whd1+, \verb+conv+ and \verb+of+ predicates.

We omit the implementation of the extensions in the paper. All together, they provide a kernel that is functionally equivalent to the one of Matita. The reduction strategies implemented differ though, the one of Matita being more complex than call-by-need. We plan in the future to synchronise the two strategies in order to compare the performance of the two kernels.

To test the kernel, we branched it to Matita, feeding it with every term that is type-checked by Matita when checking the arithmetic library of the system.
According to our measurements the kernel presented in the paper is 15 times slower than the interpreted version of Matita (to be fair versus ELPI that is an interpreter). The trade off is that the code is more elegant, much simpler and shorter (e.g. 91 lines saved just from the lifting of De Bruijn indexes). A previous implementation of a type-checker for Automath was only 5 times slower~\cite{elpiLPAR}, suggesting the possibility to optimize the code further.
