The implementation of weak head reduction given before is well known to be very inefficient, because the body of the abstraction of a $\beta$-redex is immediately completely traversed when the redex is fired. Moreover, the argument is immediately duplicated once for each occurrence of the bound variable in the body. Finally, that implementation follows the call-by-name strategy, that is inefficient per se.

To speed up reduction, we now provide a different implementation based on the reduction machine for call-by-need called WAM in~\cite{beniamino}. The machine is a modified Krivine Abstract Machine (KAM~\cite{kam}) that records in the machine environment both terms and their normal forms, the latter being computed lazily by a new invocation of the reduction machine that is fired only when the normal form is required for the first time, i.e. when the variable bound to that value occurs in head position during reduction.

In standard implementations, variables are represented by De Brujin indexes and machine environments are stacks, indexed by the variables. Since we do not use and do not want to use indexes, we took a different solution. The machine environment is represented by a set of \verb+val N T V NF+ of assumptions that we dinamically add to the $\lambda$Prolog environment using logical implication. Their meaning is that to the variable \verb+N+ we associate a value \verb+V+ of type \verb+T+ and its normal form \verb+NF+. The latter is initialized with a fresh metavariable when the binding is created by a $\beta$-reduction, and it is filled when the value is requested for the first time. To fill the value, we first start a new machine on \verb+V+ and then we decode the final machine state to \verb+NF+, an operation that we call \emph{unwind}.

In any reasonable implementation of $\lambda$Prolog, fetching the normal form of \verb+x+ from the environment via a call to \verb+val x _ _ NF+ consists in fetching a clause from the current program, that is doubly indexed over the name of the clauses and the value of their first argument (if any). The index is implemented using hash tables. Therefore the look up costs $O(n)$ in the worst case where $n$ is the number of conflicts in the hash table, i.e. the length of the machine environment. This is consistent with the cost of lookup when machines are implemented as stacks and variables as De Brujin indexes. In the average case, however, the look-up is likely to be performed in $O(1)$, even if the constant hidden in the big-O notation is likely to be large.

An assumption $A$ introduced in $\lambda$Prolog via $A \Rightarrow B$ is only visible in $B$. Therefore, once a variable is bound via a \verb+x t n nf+ assumption, we need to be sure that either the rest of the computations that requires the binding is performed in $B$; or we need to reintroduce syntactically the binding again around every term that escapes the scope of the assumption.

We implement the first technique implementing weak head reduction in Continuation Passing Style (CPS): the \verb+whd1+ predicates takes in input the head and stack of the machine together with a continuation \verb+K+, and apply \verb+K+ to the new machine head and stack. Moreover, it also pass to \verb+K+ the list of variables that are bound in $\lambda$Prolog environment. The \verb+whd*+ predicate is also implemented in CPS style, and it is responsible for composing together the lists of variables bound during the computation.

The list of variables bound in $\lambda$Prolog environment can be used after the reduction to implement the second technique: the \verb+whd_unwind t NF+ computes the normal form \verb+NF+ of \verb+t+ by calling \verb+whd*+ on \verb+t+ passing a continuation that unwinds the final machine state by also wrapping it with local definitions \verb+abbr ty val n \ \ldots+ for each \verb+val n ty val _+ in $\lambda$Prolog environment. An \verb+abbr+, also called \verb+let .. in+ in several functional programming languages, is therefore a construct to capture explicit sharing in the term syntax. Its signature and type-checking rule is:

\begin{Verbatim}
type abbr term -> term -> (term -> term) -> term.  % local definition (let-in)

of (abbr TY TE F) U :- of TE TY', sub TY' TY, pi x \ of x TY => of (F x) U.
\end{Verbatim}

The code of the reduction machine is the following:

\begin{Verbatim}
type whd1 term -> list term -> (list var -> term -> list term -> prop) -> prop.

% KAM-like rules in CPS style
whd1 (app M N) S K :- K [] M [N|S].
whd1 (lam T F1) [N|NS] K :- pi x \ val x T N NF => K [x] (F1 x) NS.
whd1 X S K :- val X _ N NF, if ($is_flex NF) (whd_unwind N NF), K [] NF S.

% reflexive, transitive closure
whd* T1 S1 K :- whd1 T1 S1
 (vl1 \ t1 \ s1 \ whd* t1 s1
 (vl2 \ t2 \ s2 \ sigma VL \ append vl1 vl2 VL, K VL t2 s2)), !.
whd* T1 S1 K :- K [] T1 S1.

% Whd followed by machine unwinding.
type whd_unwind term -> term -> prop.
whd_unwind N NF :-
 whd* N [] (l \ t \ s \ sigma TS \ unwind_stack s t TS, put_abbr l TS NF).

% unwind_stack takes an head and a stack and decodes them to a term
unwind_stack [] T T.
unwind_stack [X|XS] T O :- unwind_stack XS (app T X) O.

% put_abbr takes a list of variables and a term and wraps the latter
% with local definitions for the variables in the list
put_abbr [] NF NF.
put_abbr [X|XS] I (abbr T N K) :- val X T N _, put_abbr XS I (K X).
\end{Verbatim}

The predicate \verb+match_sort+ is implemented trivially. The one for \verb+match_arr+ is slightly more delicate: the input \verb+T+ is put in weak head normal form by \verb+whd*+; the continuation retrieves the term and checks that it is a dependent product, retrieving the two subterms; the subterms may contain variables bound in the environment; therefore, before returning them, it is necessary to close them by means of \verb+abbr+s via \verb+put_abbr+, like \verb+whd_unwind+ does.

\begin{Verbatim}
type match_sort term -> @univ -> prop.
match_sort T I :- whd* T [] (l \ t \ s \ t = sort I, s = []).

type match_arr term -> term -> (term -> term) -> prop.
match_arr T A F :-
 whd* T [] (l \ t \ s \ sigma A' \sigma F' \
  s = [], t = arr A' F', put_abbr l A' A,
  pi x \ put_abbr l (F' x) (F x)).
\end{Verbatim}

A second optimization we implemented speeds up the conversion check by following the same heuristics implemented in Matita and, similarly, in Coq: in place of putting the two terms to be compared in weak head normal form at the beginning, the two terms are lazily reduced on demand during conversion, that at every step tries $\alpha$-conversion (i.e. $\lambda$Prolog equality) and congruence rules to avoid unnecessary reductions.

In order to perform weak head lazily, we obtain \verb+conv+ and \verb+sub+ as two instances of a \verb+comp+ comparison predicate that works on two reduction machine statuses. The third argument is \verb+eq+ if the intended comparison is \verb+conv+, \verb+leq+ otherwise.

\begin{Verbatim}
type conv term -> term -> prop.
type sub term -> term -> prop.
type comp term -> list term -> eq_or_leq ->  term -> list term -> prop.

conv T1 T2 :- comp T1 [] eq T2 [].
sub T1 T2 :- comp T1 [] leq T2 [].

% fast path + axiom rules
comp T1 S1 _ T1 S1 :- !.
comp (sort I) [] leq (sort J) [] :- lt I J.
% congruence + fast path rule
comp X S1 _ X S2 :- forall2 S1 S2 conv, !.
% congruence rules
comp (lam T1 F1) [] _ (lam T2 F2) [] :- conv T1 T2, pi x \ conv (F1 x) (F2 x).
comp (arr T1 F1) [] D (arr T2 F2) [] :- conv T1 T2, pi x \ comp (F1 x) [] D (F2 x) [].
% reduction rules
comp T1 S1 D T2 S2 :- whd1 T1 S1 (_ \ t1 \ s1 \ comp t1 s1 D T2 S2), !.
comp T1 S1 D T2 S2 :- whd1 T2 S2 (_ \ t2 \ s2 \ comp T1 S1 D t2 s2), !.
\end{Verbatim}

\paragraph{Extensions with global definitions, declarations, primitive inductive types, case analysis and structural recursive definitions.}
We implemented all the extensions mentioned in $\lambda$Prolog in a modular way. To activate one extension, it is sufficient to accumulate in the kernel a $\lambda$Prolog file that adds new constructors to the \verb+term+ and to the reduction machine stack type, and the relative clauses to the \verb+whd1, conv+ and \verb+of+ predicates.

We omit the implementation of the extensions in the paper. Activated all together, they provide a kernel that is functionally equivalent to the one of Matita. The reduction strategies implemented differ though, the one of Matita being more complex than call-by-need. We plan in the future to synchronize the two strategies in order to reliably compare the performance of the two kernels.

To test the kernel, we branched it to Matita, feeding it with every term that is type-checked by Matita when checking the arithmetic library of the system.

XXXXXXXXXXXXX parlare dei tempi
