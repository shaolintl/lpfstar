We build a kernel for CIC modularly, by first developing a kernel for a generic PTS (file \verb+kernel_pts.elpi+) and then extending it with inductive types (file \verb+kernel_inductives.elpi+ that accumulates \verb+kernel_pts.elpi+). In order to use the kernel, ones needs to accumulate it together with another file that implements the PTS rules, like \verb+pts_cc.elpi+ for those of the Calculus of Constructions (CC) or \verb+pts_cc_predicative.elpi+ for those of Luo's ECC.

Our kernel for a generic PTS consists of the same type-checking rules of Section\~ref{sec:intro}, but differs in the implementation of the \verb+conv+ and \verb+sub+ judgements: as customary, we replaced the trivial implementation with a reduction machine implementing call-by-need. The machine, described in~\cite{beniamino} under the name UUUU, is a simple variation over the KAM~\cite{XXX}.

A status of the machine encodes $\lambda$-terms via sharing. In order to preserve sharing when the status is decoded, we augmented the syntax of terms with typed local definitions (the \verb+let ... in+s of ML/Coq). Local definitions are typed as follows.

\begin{verbatim}
type abbr term -> term -> (term -> term) -> term.  % local definition (let-in)

of (abbr TY TE F) U :- of TE TY', sub TY' TY, pi x \ of x TY => of (F X) U.
\end{verbatim}

Here are the most relevant machine rules.

\begin{verbatim}
type conv term -> term -> prop.
type sub term -> term -> prop.

type convm term -> list term -> term -> list term -> prop.

conv T1 T2 :- convm T1 [] T2 [].

type whd1 term -> list term -> (list var -> term -> list term -> prop) -> prop.

% KAM-like rules
whd1 (app M N) S K :- K [] M [N|S].
whd1 (lam T F1) [N|NS] K :- pi x \ val x T N NF => K [x] (F1 x) NS.
whd1 X S K :- val X _ N NF, orelse (not ($is_flex NF)) (whd_unwind N NF), K [] NF S.

whd* T1 S1 K :- whd1 T1 S1 (vl1 \ t1 \ s1 \ whd* t1 s1 (vl2 \ t2 \ s2 \ sigma VL \ append vl1 vl2 VL, K VL t2 s2)), !.
whd* T1 S1 K :- K [] T1 S1.

whd_unwind N NF :- whd* N [] (l \ t \ s \ sigma TS \ unwind_stack s t TS, put_abbr l TS NF).

unwind_stack [] T T.
unwind_stack [X|XS] T O :- unwind_stack XS (app T X) O.

put_abbr [] NF NF.
put_abbr [X|XS] I (abbr T N K) :- val X T N _, put_abbr XS I (K X).

% fast path rule
convm T1 S1 T1 S1 :- !.
% congruence + fast path rule
convm X S1 X S2 :- forall2 conv S1 S2, !.
% congruence rules
convm (lam T1 F1) [] (lam T2 F2) [] :- conv T1 T2, pi x \ conv (F1 x) (F2 x).
convm (arr T1 F1) [] (arr T2 F2) [] :- conv T1 T2, pi x \ conv (F1 x) (F2 x).
% reduction rules
convm T1 S1 T2 S2 :- whd1 T1 S1 (_ \ t1 \ s1 \ convm t1 s1 T2 S2), !.
convm T1 S1 T2 S2 :- whd1 T2 S2 (_ \ t2 \ s2 \ convm T1 S1 t2 s2).

type subm term -> list term -> term -> list term -> prop.

sub T1 T2 :- whd* T1 [] (_ \ t1 \ s1 \ whd* T2 [] (_ \ subm t1 s1)).
subm T1 S1 T2 S2 :- convm T1 S1 T2 S2.
subm (sort I) [] (sort J) [] :- lt I J.
subm (arr A1 F1) [] (arr A2 F2) [] :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
\end{verbatim}


====================================================
PER FERRUCCIO: STO LAVORANDO QUI SOPRA, SOTTO E' ROBA TUA
====================================================

A kernel for CIC must define two main relations on CIC terms:
\verb+type_step T U+
asserting that \verb+U+ is an inferred type of \verb+T+,
and \verb+conv U1 U2+ 
asserting that \verb+U1+ and \verb+U2+ are convertible
(we take the name \verb+type_step+ from XXXXXXXXXXXX).

Our prototype achieves this goal by taking advantage of the next features of $\lambda$Prolog.

\begin{itemize}

\item
We can extend the existing predicates on new forms of terms
simply by adding clauses concerning these forms. 
Therefore the kernel's architecture can be modular,
in that a core kernel designed for an arbitrary full pure type system (PTS)
underlies a kernel extension taking care of global constants,
inductive types and recursive definitions.
We define the sort hierarchy of CIC in a separate component of the kernel as well.
This component describes the signature of CIC as a PTS,
and the subsumption relation on CIC sorts.

\item
In CIC, both conversion and type inference occur w.r.t. an environment,
and we can use the implicit environment provided by the
$\lambda$Prolog engine, instead of defining our own environment explicitly.
The environment must maintain the expected type of constants and variables,
as well as their $\delta$-expansion when appropriate.
At the moment, we take the data on global constants from the
environment of the Matita ITP, with which we interface the extended
component of our kernel. XXXXXXXXXX

\end{itemize}

Conversion is asserted by levels by the predicate \verb+conv_whnf U1 U2+,
that expects \verb+U1+ and \verb+U2+ in weak head normal form (WHNF).
Of course a metavariable optionally applied to arguments is a partial
term in WHNF, so an elaborator can introduce unification as a
generalization of conversion, just by extending this specific predicate.

Inference of a type \verb+U+ for \verb+T+ is directed by the syntax of \verb+T+
according to the aforementioned predicate \verb+type_step T U+.
When \verb+T+ is the application of \verb+T0+ to some arguments,
and given \verb+type_step T0 U0+,
we check these arguments against the $\Pi$-abstractions (i.e. the
universal quantifications) arising from \verb+U0+ after reduction to
WHNF by levels. Here, we traverse each quantification by issuing a $\Pi$-reduction
(i.e. the equivalent of a $\beta$-reduction on a $\Pi$).

The responsibility of this check lies with the predicate \verb+whd_pi U0 U+,
by which we compute the outcome \verb+U+ after the quantifications of
\verb+U0+ are traversed. Indeed \verb+U+ is an inferred type for \verb+T+.

If an application has an unknown type \verb+X+,
an elaborator may extend this predicate to guess the quantifications
that must occur in \verb+X+ on the basis of the application's arguments.

The elaborator may as well extend the predicate \verb+type_step T U+
in order to handle the situations in which
the expected type of a metavariable \verb+T+ is known.

Finally, note that we issue both $\beta$-reductions and $\Pi$-reductions
by adding an explicit substitution to the context in which computation occurs.
As we issue this substitution lazily,
our kernel does not need the ``copying'' predicate on terms
(following the terminology of $\lambda$Prolog).
