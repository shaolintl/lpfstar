\documentclass{easychair}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

% \title{Higher Order Constraint Logic Programming with Applications to Interactive Theorem Proving}
\title{The pragmatic construction of ELPI}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science and Engineering, University of Bologna \email{ferruccio.guidi@unibo.it} \and
  Department of Computer Scienc and Engineeringe, University of Bologna \email{claudio.sacerdoticoen@unibo.it} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{F. Guidi, C. Sacerdoti Coen, E. Tassi}
\titlerunning{whatever}

\begin{document}
\maketitle

\begin{abstract}
Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

In this paper we present a first proposal for an higher order constraint logic
programming language obtained as an extension of $\lambda$Prolog. We also present
applications to the implementation of an elaborator for the Calculus of
Inductive Constructions, i.e. the core of a modern interactive theorem prover
in the style of Coq, Agda or Matita.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\paragraph{A pragmatic reconstruction of $\lambda$Prolog.}

In~\cite{jlp98} BelleannÃ©e et. alt. propose a pragmatic reconstruction
of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order
Logic Programming (HOLP) language introduced by Dale Miller and
Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the
minimal extension of Prolog that allows to program by structural
induction on $\lambda$-terms. According to their reconstruction, in
order to achieve that goal, Prolog needs to first be augmented with
$\lambda$-abstractions in the term syntax; then types are added to
drive full higher-order unification; then universal quantification in
goals and $\eta$-equivalence are required to express relations between
$\lambda$-abstractions and their bodies; and finally implication in
goals is needed to allow for structural induction definitions of
predicates.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can
easily encode all kind of binders without the need to take care of
binding representation, $\alpha$-conversion, renaming and
instantiation. Structural induction over syntax with binders is also
made trivial by combining universal quantification and implication
following the very same pattern used in Logical Frameworks like LF
(also called $\lambda$P). Indeed, LF, endowed with an HOLP semantics
like in ???(ELF? Beluga? MMT?) is just a sub-language of
$\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the
following two lines program to compute the simple type of a
$\lambda$-expression:

\begin{verbatim}
kind term type.
type app term -> term -> term.
type lam (term -> term) -> term.

type arr typ -> typ -> typ.

type of term -> typ -> o.

of (app M N) B :- of M (arr A B), of N A.
of (lam F) (arr A B) :- pi x\ of x A => of (F x) B.
\end{verbatim}

\paragraph{$\lambda$Prolog for proof-checking.}

According to the Curry-Howard isomorphism, the program above can also
be interpreted as a proof-checker for minimal propositional logic. By
escalating the encoded $\lambda$-calculus to more complex terms and
types, it is possible to obtain a proof-checker for a much richer
logic, like the Calculus of Inductive Constructions that, up to some
variations, is the common logic shared by the interactive theorem
provers (ITPs) Coq~\cite{}, Lean~\cite{}, Matita~\cite{} and
Agda~\cite{}. For example, in~\cite{us} we implemented in
$\lambda$Prolog a type-checker for Landau's Grundlagen XXXXXXXX.

Indeed, all the ITPs mentioned above are implemented following
basically the same architecture. At the core of the system there is
the \emph{kernel}, that is the trusted code base (together with the
compiler and run-time of the programming language the system is
written on). The kernel just implements the type-checker together with
all the judgements required for type-checking, namely: well formation
of contexts and environments, substitution, reduction, convertibility.
The last three judgements are necessary because the type system has
dependent types and therefore types need to be compared up to
computation.

This scales quite well, e.g. to systems with dependent types and
universes:

\begin{verbatim}
type conv term -> term -> o.

conv (app M1 N1) (app M2 N2) :- conv M1 M2, conv N1 N2.
conv (app (lam F) N) T :- conv (F N) T.
conv T (app (lam F) N) T :- conv T (F N).
conv (lam F1) (lam F2) :- pi x\ conv x x => conv (F1 x) (F2 x).
\end{verbatim}

\noindent
that is needed for dependent types

\begin{verbatim}
type arr term -> (term -> term) -> term.

of (app M N) BN :- of M (arr A1 Bx), of N A2, sub A2 A1, BN = Bx N.
of (lam F) (arr A B) :- pi x\ of x A => of (F x) (B x).
\end{verbatim}

\noindent
and for hierarchy of sorts

\begin{verbatim}
type sort int -> term.

of (sort I) (sort J) :- succ I J.
of (arr A Bx) (sort K) :-
  of A (sort I), (pi x\ of x A => of (Bx x) (sort J)), max I J K.
conv (sort I) (sort I).
sub A B :- conv A B.
sub (sort I) (sort J) :- I < J.
sub (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
\end{verbatim}

\noindent
we now have the predicative, universal fragment of Luo's ECC~\cite{luo}, wow!


\paragraph{From proof-checking to interactive proving.}

The kernel is ultimately responsible for guaranteeing that a proof
built using an ITP is fully sound. However, in practice the user never
interacts with the kernel and the remaining parts of the system do not
depend on the behaviour of the kernel. Where the real intelligence of
the system lies is instead the second layer, called \emph{elaborator}
or \emph{refiner}~\cite{??,??,??}.


In this paper we investigate if $\lambda$Prolog can do that too, if not
what shall be added.  We start by detailing what an elaborator is and
why the state of the art is not satisfactory.  We then present the
issues
we faced when coding an elaborator in $\lambda$Prolog and how similar issues are
dealt with in first order Prolog.  We then present ELPI, our extension to
$\lambda$Prolog.  Finally we present a kernel and a prototype refiner for CIC
written in ELPI.  We then conclude.

\section{The elaborator component of today's ITPs}

An elaborator takes in input a \emph{partial term} and optionally a
type, and returns the closest term similar to the input one such that
the term has the expected type. Both the input and output terms are
partial in the sense that subterms can be omitted and replaced with
named holes to be later filled in or, in logic programming
terminology, with \emph{existentially quantified metavariables}. For
example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are
quantified outside the term represents the $\lambda$-abstraction over
$x$ of a type yet to be determined of the application of $f$ to two
arguments, both to be yet determined and such that $x$ can happear
free only in the first. Elaborating the term versus the expected type
$\mathbb{N} \to \mathbb{B}$ will instantiate $T$ with $\mathbb{N}$ and
verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it
allows to interpret the terms that are input by the user, usually by
means of a user-friendly syntax where information can be omitted,
mathematical notation is used and abused, subtyping is assumed even if
elements of the first type can only be coerced to elements of the
second by inserting a function call in the elaborated term. A better
elaborator therefore gives to the user the feeling of a more
intelligent and user friendly system. On the other hand, via
Curry-Howard, a partial term is a partial proof and an ITP is all
about instantiating holes in partial proofs with new partial terms to
advance in the proof. The elaborator is thus the mechanism that takes
a partial sub-proof and makes it fit in the global one to progress on
a particular proof obligation. In other words, all tactics of the ITP
ultimately produce partial proof terms that are elaborated. The more
advanced is the elaborator, the simpler the code implementing tactics
can be.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{State of the art in implementation of elaborators.}

The elaborators of the majority of the provers mentioned above are all
implemented according to the same schema: the syntax of terms is
augmented with explicitly substituted existential variables; the
judgements of the kernel are re-implemented from scratch, generalizing
them to take in account metavariables and elaboration. In particular:
reduction acts on metavariables too; conversion becomes narrowing,
i.e. higher order unification under a mixed prefix~\cite{???} in
presence of rewriting rules; unification requires metavariable
instantiation, that is implemented lazily for efficiency; type
checking is generalized to elaboration, for example by replacing all
calls to conversion with calls to narrowing and by threading around a
new environment for the signature of metavariables.

The approach is sub-optimal in many ways:
\begin{enumerate}
\item A lot of code is duplicated with the kernel.
\item A lot of code is essentially logic independent, but it needs to
	be implemented again for every system: mostly code that deals
	with binders ($\alpha$-conversion, capture avoiding
	substitution, renaming) and code that deals with existentially
	quantified metavariables (explicit substitution management,
	name capturing instantiation, higher order unification under a
	mixed prefix).
\item There is no guarantee that elaborated terms will be actually
	accepted by the kernel.
\item Most of the problems tackled, like higher order unification, are
	only semi-decidable. For efficiency reasons a lot of
	incomplete heuristics are implemented to speed up the system
	and reduce backtracking. The heuristics, however, are quite
	ad-hoc and they interact with one another in unpredictable
	ways. Because they are hidden in the code, the whole system
	becomes unpredictable to the user, unless an heavy layer is
	put around the elaborator to recover predicibility by reducing
	the invocations of the elaborator (cfr.~\cite{SOZEAU?}).
\item The elaborator is the brain of the system, but it is oblivious
	of the pragmatic ways to use the knowledge in the prover
	library, e.g. to automatically fill in
	gaps~\cite{mathcomponents}, to coerce data from one type to
	another~\cite{coercivesubtyping} or to enrich data to resolve
	mathematical abuse of
	notation~\cite{nonuniformunificationhints}. Therefore the
	recent trend is to let the user drive the behaviour of the
	elaborator in user space by writing pieces of declarative code
	in disguise that are integrated in the core algorithm. The
	languages to write this code are obviously high level, hiding
	the user from the intricacies of representation of bound
	variables, metavariables, etc. The overall program logic is
	therefore splitted in multiple languages, defying the hope for
	static analysis and predictability.
\end{enumerate}

\paragraph{Motivations and approach}

The motivation of our research is to try to improve over the latter
issues by identifying an high level (logic) programming language as
general as possible, but suitable at least for the implementation of
elaborators. In particular:
\begin{enumerate}
\item The programming language should take care of representation of
	bindings and metavariables, and implement all related basic
	operations. This would solve issues 2) and 5) above, allowing
	both the core implementors and the users to work on the same,
	high level code.
\item Issue 3) should become less severe, because the now high level
	code would be inspectable by the user that could also replace
	it in case of need.
\item The rules of the kernel should not be re-implemented in the
	elaborator. On the contrary, and in the usual spirit of logic
	programming, it should be possible to implement the
	accumulator by adding rules to the kernel. Moreover, the
	language should make it easy to prove that the additional
	rules grant that the elaborated terms are well-typed according
	to the core set of rules of the kernel.
\end{enumerate}

The requirement for the language to be a logic one is motivated both
by our hope to reuse or at least extend $\lambda$Prolog and by the
observation that the third layer of an ITP, the ones of tactics, is
better implemented on a language where backtracking is naturally
implemented and controlled.

$\lambda$Prolog already has existentially quantified metavariables and the related operations. Therefore, in order to satisfy requirement 1 it seems sufficient to use a shallow embedding of the syntax: the metavariables of the partial terms are directly encoded as metavariables of $\lambda$Prolog, similarly to how binders are encoded via the $\lambda$-abstractions of $\lambda$Prolog. Does this idea work out of the box? Unfortunately not at all.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\lambda$Prolog meets partial terms: problems}


\paragraph{Failure of $\lambda$Prolog as an high level language for
implementing elaborators.}
We already know from~\cite{jlp98} that $\lambda$Prolog is the minimal
extension of Prolog that allows to implement inductive predicates over
syntax containing binders. Does it work when applied to data that is
meant to containt existentially quantified metavariables too?

Consider the $\lambda$-term \verb+(lam a\ P a)+ that encodes a
partial proof of $\forall A, A -> A$.
If we run the following query, the computation diverges:

\begin{verbatim}
goal> of (lam a\P a) (arr (sort i) a\ arr a _\ a)
\end{verbatim}

Indeed, \verb+P+ is flexible and the \verb+of (app M N) ..+
rule applies indefinitely.

Indeed the \verb+of+ predicate inherits from Prolog a generative semantics:
when called recursively on a flexible input, it blindly enumerates all
instances trying to find the ones that are well typed. Even when proof
search does not diverge, the behaviour obtained is not the one
expected from an elaborator for an \emph{interactive} prover: the
elaborator is not meant to fill in the term, unless the choice is
obliged. On the contrary, it should leave the metavariable not
instantiated, but the system should \emph{remember} the need for
veryfing if the predicate holds later on, when the metavariable gets
instantiated. In the example above, type-checking AAAAAAAAAA forces
the system to remember to BBBBBBBBBB that corresponds to the new proof
obligation CCCC.

This is not a problem specific of $\lambda$Prolog, also Prolog has it.
Indeed all modern Prolog engines provide a \verb+var/1+ built-in
to test/guard predicates against flexible input, provide one/many
variants of \verb+delay/2+ to suspend a goal until the input becomes rigid, and
provide modes declarations to both statically/dynamically detect problematic calls to be (semi)automatically delayed. These mechanisms, however, have never been standardized.

For example, by using the \verb+delay+ pack of SWI-Prolog~\cite{???}, the goal \verb+plus(X,1,Y)+ is delayed until either \verb+X+ or \verb+Y+ are instantiated.
Delayed goals can be thought as \emph{constraints} over the metavariables occurring in them. In the example above, the programmer is imposing a constraint between \verb+X+ and \verb+Y+.

In many situations, the constraints that accumulate over a metavariable are not independent and sets of constraints can be rewritten in order to simplify them. Rewriting a set of constraints is called \emph{constraint propagation} and the programming languages that allow the user to declare constraints and that propagate them are called \emph{Constraint Programming Languages} (CLPs). For example, the set $\{0 <= N <= 4, 2 <= N <= 5\}$ can be rewritten into $\{2 <= N <= 4\}$ without changing the set of ground solutions.

Most CLPs do not allow the user to define new constraints and propagation rules in user space. A notable exception is the first order language CHR~\cite{chr}. In CHR the user declares predicates and then gives a set of rewriting rules of the form $S_1 \setminus S_2 ~|~ G \iff S_3$ whose declarative semantics is that
$\bigwedge S_1 \wedge G \Rightarrow (\bigwedge S_2 \iff \bigwedge S_3)$ and whose operational semantics is to match the current set of constraints against both $S_1$ and $S_2$ and, if the clause $G$ holds, replace the constraints in $S_2$ with the ones in $S_3$. All syntactic components can be omitted: $G$ defaults ot \verb+true+ and the sets $S_1,S_2,S_3$ to the emptyset. The $\iff$ symbol is also omitted when $S_3$ is, and $|$ is omitted when $G$ is. The semantics of the language is completed by a strategy that fixes the order in which propagation rules are fired and in what cases propagation rules can be backtracked. Multiple such semantics for CHR have been proposed.

For example, the rules for type-checking ECC call the predicates \verb+<,max,succ+ over integers. The corresponding elaborator may be faced with sorts like \verb+sort M+ where \verb+M+ is a metavariable. The elaborator is asked to compute a suitable assignment to the metavariables that respects all constraints. Without breaking consistency, the successor predicate can be relaxed to a strictly less than, the max to a generic upper bound and the set of integers to an unspecified partial order, de facto replacing the predicates \verb+<,max,succ+ with generic predicates \verb+leq,ltn+ (lax and strict inequality over an unspecified set). Finally, the two predicates, once turned into constraints, admit the following complete set of CHR propagation rules able to detect strict cicles that correspond to unsatisfiability of the constraints.

\label{chruniverses}
\begin{verbatim}
succ I J :- ltn I J.
I < J :- ltn I J.
max I J K :- leq I K, leq J K.

constraint leq ltn {
  % incompatibility
  rule (leq X Y) (ltn Y X) <=> false.
  rule (ltn X Y) (ltn Y X) <=> false.
  rule (ltn X X) <=> false.
  
  % reflexivity
  rule \ (leq X X).

  % antisymmetry
  rule (leq X Y) \ (leq Y X) <=> (Y = X).

  % transitivity
  rule (leq X Y) (leq Y Z) <=> (leq X Z).
  rule (leq X Y) (ltn Y Z) <=> (ltn X Z).
  rule (ltn X Y) (leq Y Z) <=> (ltn X Z).
  rule (ltn X Y) (ltn Y Z) <=> (ltn X Z).

  % idempotence
  rule (leq X Y) \ (leq X Y).
  rule (ltn X Y) \ (ltn X Y).
}
\end{verbatim}

Implementing in $\lambda$Prolog the part of the elaborator for ECC that deals with universe constraints is much harder and less elegant than the previous approach based on CHR. Indeed, the only solution is to keep an explicit list of constraints that become a third argument of the type-checking judgement and that needs to be threaded around the program, polluting the code and not allowing to reuse the type-checking code for the elaborator. Moreover, every time an additional constraint is added to the list, some special predicate to fire the propagation rules according to some strategy needs to be called, de facto forcing the user to implement the boilerplate code for constraint propagation.

Motivated by the observations in this section, we implemented a new, efficient~\cite{???} interpreter for $\lambda$Prolog, called \emph{ELPI} (Embedded Lambda Prolog Interpreter) and we augmented the language with the possibility to delay arbitrary predicates turning them into constraints, and to propagate constraints using CHR style rewriting rules and additional mechanisms.

The language implemented by ELPI is described in Section~\ref{sec:elpi}.
In addition to the new programming constructs that deal with constraints, it slightly differs from the version of $\lambda$Prolog implemented in Teyjus in a few minor aspects:
\begin{itemize}
\item In ELPI all types and type and sort declarations can be omitted, whereas they are mandatory in Teyjus. Originally, and according to~\cite{jlp98}, types were necessary to implement Huet's algorithm for higher order unification. However, for several years now the unification algorithm of Teyjus only solves equations in the pattern fragment discovered by Miller~\cite{patternfrag}, which admits most general unifiers and reduce unification to a decidable problem. All other equations are automatically delayed by Teyjus to be fired only when the equation is instantiated to one in the pattern fragment. At the level of the implementation, the code of ELPI that implements this delay is shared with the one to delay arbitrary predicates.
\item The module system of Teyjus is not implemented. Only the \verb+accumulate+ directive is honored for backward compatibility and with a different semantics. In ELPI we provide instead explicit existentially quantified local constants whose scope spans over a set of program clauses. This mechanism gives in a simple way predicate and constructor hiding that are provided differently by the module system of Teyjus.
\item In a few corner cases, the parsing of an expression by Teyjus is influenced by the types. In particular, types are used to disambiguate between lists of elements and a singleton list of conjunctions. The syntax is disambiguated in a different way in ELPI.
\end{itemize}

Despite the differences above, we tried very hard to maintain backward compatibility with Teyjus and its standard library. Indeed, we are able to execute in ELPI all the code from Teyjus that we collected from the Web, up to a very few minor changes to the source code, mostly due to 2 and 3.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ELPI = $\lambda$Prolog + (HO)CHR}\label{sec:elpi}

\subsection{incomplete terms, delay}

The hi level directive is

\begin{verbatim}
delay (of X T) on X.
\end{verbatim}

The low level one (implemented)

\begin{verbatim}
mode (of i o).
of X T :- var X, delay (of X T) [X].
\end{verbatim}

Note that the mode makes it so that \verb+i+ arguments are never
instantiated by backchaining, so other rules (generative) would not
apply.  This is so common we provide the following syntactic sugar.

\begin{verbatim}
of (?? as X) T :- delay (of X T) [X].
\end{verbatim}

where \verb+??+ is a symbol unifying only with flexible, and
\verb+f (t as X) :- c+ de-sugars to \verb+f X :- X = t, c+,
i.e. it is a syntax to name a subterm reminiscent of what OCaml
pattern matching let one do.

Delayed goals become constraints keyed on the flexible variable
(\verb+X+ in the example above).  \emph{As soon} as \verb+X+ gets
instantiated, the delayed goal is resumed.  Symmetrically, as soon
as a contraint is added propagation is triggered.

If we go back to our previous example

\begin{verbatim}
goal> of (lam a\P a) (arr (sort I) a\ arr a _\ a).
\end{verbatim}

would end with a delayed goal

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{verbatim}

a subsequent query

\begin{verbatim}
goal> P = a\ lam pa\ Q a pa .
\end{verbatim}

would resume the goal

\begin{verbatim}
of x (sort I) ?- of (lam pa\ Q x pa) (arr x _\ x)
\end{verbatim}

that would progress into the again suspended goal

\begin{verbatim}
of x (sort I), of y x ?- of (Q x y) x
\end{verbatim}

ITPs build the proof term by instantiation, and one has to check
if it fits typing.  Here one cannot forget the check, since it is
the programming language that resumes type checking whenever there is
proof progress (by uvar instantiation).

\subsection{delayed goals, HO constraint propagation}

CHR seen before does 1st order constraints, here one wants to deal
with $\lambda$Prolog goals as constraints.  Eg one wants to write  propagation
rules like this one

\begin{verbatim}
constraint of {
  rule (G ?- of X T1) \ (G ?- of X T2) <=> (G ?- conv T1 T2).
}
\end{verbatim}

it reflects uniqueness of typing: if \verb+X+ is used non linearly,
then each occurrence must have the same type.

the proof theoretic semantics of $\lambda$Prolog suggests that G has to be unified
up to commutativity of cunjunction, and that heigen variables have to
be unified as in equivariate unification (i.e. an injective map).

For example,


\begin{verbatim}
goal> of (lam a\ lam pa\ P a pa) (arr (sort I) a\ arr a _\ a),
      of (lam a\ lam pa\ P a pa) (arr (sort J) a\ arr a _\ a).
\end{verbatim}

generates 2 suspended goals

\begin{verbatim}
of x (sort I), of y x ?- of (P x y) x
of w (sort J), of z w ?- of (P w z) w
\end{verbatim}

........

of course one would like G to be compared like that

\begin{verbatim}
constraint of {
  map _ [] [].
  map F [X|XS] [Y|YS] :- F X Y, map F XS YS.
  conv-of (of X T1) (of X T2) :- conv T1 T2.

  rule (nabla xv\ G2 xv ?- of (X xv) T1)
     \ (nabla yv\ G2 yv ?- of (X yv) T2)
     > (xv ~ yv)
   % after that point, G1 G2 T1 T2 are aligned on the names
   % and G1 G2 are sorted accordingly (or map is made more complex)
     | (pi zv\ map conv-of (G1 zv) (G2 zv))
     <=> (nabla zv\ G1 zv ?- conv T1 T2).
}
\end{verbatim}

or even better to have such semantics built-in in the former example
but what if outside llam?


DIRE CHE BASTA AGGIUNGERE un \$delay MA FORSE QUI CI VORREBBE QUELLO AD ALTISSIMO LIVELLO CHE NON ABBIAMO

FARLO VEDERE SULL'ESEMPIO?

AGGIUNGERE ESEMPIO SU PROPAGAZIONE CONSTRAINTS A LA CHR?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application}

\subsection{Kernel for CIC}\label{sec:kernel}

\input{kernel}

\subsection{Prototype elaborator}\label{sec:elaborator}
We are developing an elaborator for CIC as a modular extension of the kernel described in Sect.~\ref{sec:kernel}. The elaborator mimics as close as possible the behaviour of the one of Matita 0.99.1~\ref{xxx}, with the exception of the handling of universe constraints that follows Coq XXX~\ref{yyy}.

An alternative very promising choice would have been to mimic the elaborator described in~\ref{zzz} and already presented via typing rules that yield a set of higher order unification constraints to be later solved. In the future, the choice to be closer to Matita will allow us to easily compare the performances of the elaborator written in ELPI with the one of Matita written in OCaml, in order to further optimize the ELPI interpreter.

To implement the elaborator, the following steps need to be taken at the beginning of a file that accumulates at the end the code of the kernel and of the file implementing the PTS rules.
\begin{itemize}
\item The accumulated code for the PTS is not the ones accumulated by the kernel that just verifies the typing inequations, but the one based on the first order CHR rules presented in Section~\ref{chruniverses}.
\item The following predicates defined by structural recursion over a term
 must be given a suitable mode to prevent the generative behaviour:
 AAA (type checking), BBB (reduction), CCC (conversion), plus all derived
 predicates like DDD that infers a type and checks that the inferred type is
 a sort.
\item A line is inserted to delay type checking a flexible term. A suitable set of propagation rules in CHR style can be added to capture unicity of typing.
\item Conversion when at least one of the arguments is a flexible term amounts to higher order narrowing. Instead of delaying the goal, an heuristic already used in Matita immediately rewrites the constraint by solving the unification problem. The heuristic, in the case $M~t = s$ tries to instantiates $M$ with
$\lambda x. s[x/t]$ where the latter operation replaces with $x$ subterms of $s$ that unify with $t$. In practice, in Huet's terminology it always prefer projection to mimic. When $M$ is applied to multiple $t$s or to flexible $t$s, further heuristics are used to pick one of the solution. These heuristics are clearly incomplete, discarding all solutions but one and sometimes failing to find a solution when it exists. However, they are more general than the ones implemented in Coq and, from a practical perspective, they guess most of the time the unifier that is expected by the user when interactive with the ITP.
\item In place of delaying reducing a flexible term, the term is returned in the current state, weakening the semantics of the the BBB predicate. In practice, this is not a problem because the result of reduction is then always passed to the conversion predicate.
\end{itemize}

\begin{verbatim}
mode (t+step i o).
mode (has+sort i o).
mode (conv+whnf i i i i i).
t+step (?? as K) T :- !, $constraint (t+step K T) K.
has+sort (?? as S) T :- !, $constraint (has+sort S T) S.

conv+whnf A B C D E :- $print "##" (conv+whnf A B C D E), fail.

% Bug1: ci possono essere let-in sulle var in Ti
% Bug2: potrebbe servire un <= se il secondo Ã¨ una sorta
%conv+whnf (?? as T1) [] _M T2 L2 :- !, $llam_unif T1 {zip T2 L2}.
conv+whnf T1 L1 _M (?? as V) L2 :- !, bind-list L2 {zip T1 L1} V.

conv+whnf (?? as T1) L1 M T2 L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T1.
conv+whnf T1 L1 M (?? as T2) L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T2.

constraint t+step has+sort conv+whnf r+step {
}

%%% library
mode (zip i i o).
zip HD [] HD :- !.
zip (appl HD TL) Args (appl HD TLArgs) :- !, append TL Args TLArgs.
zip HD Args (appl HD Args).

is_rigid C :- $is_name C. % ; C = const _ ; C = indt _ ; C = indc _

mode (bind-list i i o).
bind-list [] T T' :- copy T T'.
bind-list [ ?? |VS] T R :- !, pi x\ bind-list VS T (R x).
bind-list [appl C AS | VS] T R :- is_rigid C, !,
  pi x\ (pi L\ copy (app[C|L]) x :- conv+args L AS) => bind-list VS T (R x).
bind-list [C|VS] T R :- is_rigid C, !,
  pi x\ copy C x => bind-list VS T (R x).

mode (copy i o).
copy A B :- $print "AA" (copy A B), fail.
copy X Y :- r+step X Y _. % Bang or not???
copy X Y :- $is_name X, X = Y, !.
copy X Y :- $is_name X, r+step X T _, !, copy T Y.
copy (sort _ as C) C :- !.
copy (appl X1 L1) (appl X2 L2) :- copy X1 X2, $print "UU", map copy L1 L2.

\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related works}
\begin{enumerate}
\item Costrutti tipo delay
\item Logical frameworks (Dedukti, MMT, etc.) e brrr Isabelle
\item Da qualche parte: il pre-print by Abel sull'elaborator basato su
 constraint e lean che implementa un constraint solver di qualche tipo
\end{enumerate}

\section{SCALETTA}

Prima possibilitÃ :
\begin{enumerate}
\item Si introduce per bene (??) la sintassi/semantica di CHR
\item Si fa notare quanto sia costoso il passo di firing di una regola CHR
      e si conclude che Ã¨ meglio identificare sotto-frammenti
\item Si introducono i mode + il costrutto delay + i pattern sulle metavariabili
      spiegandoli come il caso comune nel quale un constraint sia immediatamente
      propagabile senza doverne cercare altri
\item (??) Si aggiungono gli altri costrutti di ELPI
\item Kernel modulare di Ferruccio ed esperimenti con Matita
\item Estensione con universi flottanti e refiner
\end{enumerate}

Seconda possibilitÃ : si parte dal kernel di Ferruccio e poi ``on-demand'' si introduce il refiner descrivendo i comandi che usiamo 

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
