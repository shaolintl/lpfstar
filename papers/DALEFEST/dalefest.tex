\documentclass{easychair}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

% \title{Higher Order Constraint Logic Programming with Applications to Interactive Theorem Proving}
\title{The pragmatic construction of ELPI}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science and Engineering, University of Bologna \email{ferruccio.guidi@unibo.it} \and
  Department of Computer Scienc and Engineeringe, University of Bologna \email{claudio.sacerdoticoen@unibo.it} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{F. Guidi, C. Sacerdoti Coen, E. Tassi}
\titlerunning{whatever}

\begin{document}
\maketitle

\begin{abstract}
Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

In this paper we present a first proposal for an higher order constraint logic
programming language obtained as an extension of $\lambda$Prolog. We also present
applications to the implementation of an elaborator for the Calculus of
Inductive Constructions, i.e. the core of a modern interactive theorem prover
in the style of Coq, Agda or Matita.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\paragraph{A pragmatic reconstruction of $\lambda$Prolog.}

In~\cite{jlp98} Belleannée et. alt. propose a pragmatic reconstruction
of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order
Logic Programming (HOLP) language introduced by Dale Miller and
Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the
minimal extension of Prolog that allows to program by structural
induction on $\lambda$-terms. According to their reconstruction, in
order to achieve that goal, Prolog needs to first be augmented with
$\lambda$-abstractions in the term syntax; then types are added to
drive full higher-order unification; then universal quantification in
goals and $\eta$-equivalence are required to express relations between
$\lambda$-abstractions and their bodies; and finally implication in
goals is needed to allow for structural induction definitions of
predicates.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can
easily encode all kind of binders without the need to take care of
binding representation, $\alpha$-conversion, renaming and
instantiation. Structural induction over syntax with binders is also
made trivial by combining universal quantification and implication
following the very same pattern used in Logical Frameworks like LF
(also called $\lambda$P). Indeed, LF, endowed with an HOLP semantics
like in ???(ELF? Beluga? MMT?) is just a sub-language of
$\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the
following two lines program to compute the simple type of a
$\lambda$-expression:

\begin{verbatim}
kind term type.
type app term -> term -> term.
type lam typ -> (term -> term) -> term.

type arr typ -> typ -> typ.

type of term -> typ -> o.

of (app M N) B :- of M (arr A B), of N A.
of (lam A F) (arr A B) :- pi x\ of x A => of (F x) B.
\end{verbatim}

\paragraph{$\lambda$Prolog for proof-checking.}

According to the Curry-Howard isomorphism, the program above can also
be interpreted as a proof-checker for minimal propositional logic. By
escalating the encoded $\lambda$-calculus to more complex terms and
types, it is possible to obtain a proof-checker for a much richer
logic, like the Calculus of Inductive Constructions that, up to some
variations, is the common logic shared by the interactive theorem
provers (ITPs) Coq~\cite{}, Lean~\cite{}, Matita~\cite{} and
Agda~\cite{}. For example, in~\cite{us} we implemented in
$\lambda$Prolog a type-checker for Landau's Grundlagen XXXXXXXX.

Indeed, all the ITPs mentioned above are implemented following
basically the same architecture. At the core of the system there is
the \emph{kernel}, that is the trusted code base (together with the
compiler and run-time of the programming language the system is
written on). The kernel just implements the type-checker together with
all the judgements required for type-checking, namely: well formation
of contexts and environments, substitution, reduction, convertibility.
The last three judgements are necessary because the type system has
dependent types and therefore types need to be compared up to
computation.

\label{sec:intro}
Weak head normal form is straightforward, since one can use the
$\beta$ reduction of $\lambda$Prolog to implement the one of the
object language:

\begin{verbatim}
type whd*, whd1 term -> term -> prop.

whd1 (app M N) R :- whd* M (lam _ F), whd* (F N) R.
whd* A B :- whd1 A A1, !, whd* A1 B.
whd* X X.
\end{verbatim}

The conversion test is a bit more delicate, since it is a performance
critical piece of code (see section~\ref{sec:kernel}).  Here we code
the usual strategy of normalizing terms only if really necessary (what
systems typically do since Automath).

\begin{verbatim}
type conv term -> term -> prop.

conv X X.
conv (app M1 N1) (app M2 N2) :- conv M1 M2, conv N1 N2.
conv (lam _ F1) (lam _ F2) :- pi x\ conv (F1 x) (F2 x).
conv A B :- whd1 A A1, conv A1 B.
conv A B :- whd1 B B1, conv A B1.
\end{verbatim}

To obtain a checker for an object language with dependent types and
a predicative hierarchy of sorts we need to add a new term constructor
and refine the type of \verb+arr+ as follows:

\begin{verbatim}
type lam arr term -> (term -> term) -> term.
type arr term -> (term -> term) -> term.
type sort int -> term.
\end{verbatim}

Both \verb+arr+ and \verb+sort+ are rigid (in head normal form), hence the need to extend the \verb+whd+ predicate.  The conversion test, in presence of
a universe hierarchy needs to take into account variance and cumulativity.
With little effort we define \verb+sub+, the entry point for the conversion 
test, taking care of comparing sorts appropriately.

\begin{verbatim}
type sub term -> term -> prop.
type sub-whd term -> term -> prop.

sub A B :- whd* A A1, whd* B B1, sub-whd A1 B1, !.
sub-whd A B :- conv A B.
sub-whd (sort I) (sort J) :- lt I J.
sub-whd (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
XXXXXXXXXXX due arr non sono mai convertibili così
\end{verbatim}

Finally, the typing rules for all that.  Of course we need to mix
terms and types, hence relax the type of \verb+of+.

\begin{verbatim}
type of term -> term -> o.

of (app M N) BN :-
  of M TM, conv TM (arr A1 Bx), of N A2, sub A2 A1, BN = Bx N.
of (lam A F) (arr A B) :-
  of A SA, conv SA (sort I),
  (pi x\ of x A => of (F x) (B x)),
  of (arr A B) _. % can't put this early, since B flexible
of (sort I) (sort J) :- succ I J.
of (arr A Bx) (sort K) :-
  of A TA, (pi x\ of x A => of (Bx x) TB),
  conv TA (sort I), conv TB (sort J), max I J K.
\end{verbatim}

where \verb+succ+ and \verb+max+ are the predicates governing sorts in
PTS.  We now have the predicative, universal fragment of Luo's
ECC~\cite{luo}, in 22 lines, wow!

Note that the rules are syntax directed, i.e. with explicit calls to
\verb+sub+.  The on-paper presentation is more elegant and compact but 
uses in a fundamental way non determinism (hence results in too much
backtracking) by simply adding:

\begin{verbatim}
of A C :- of A B, sub B C .
\end{verbatim}


\paragraph{From proof-checking to interactive proving.}

The kernel is ultimately responsible for guaranteeing that a proof
built using an ITP is fully sound. However, in practice the user never
interacts with the kernel and the remaining parts of the system do not
depend on the behaviour of the kernel. Where the real intelligence of
the system lies is instead the second layer, called \emph{elaborator}
or \emph{refiner}~\cite{??,??,??}.


In this paper we investigate if $\lambda$Prolog can do that too, if not
what shall be added.  We start by detailing what an elaborator is and
why the state of the art is not satisfactory.  We then present the
issues
we faced when coding an elaborator in $\lambda$Prolog and how similar issues are
dealt with in first order Prolog.  We then present ELPI, our extension to
$\lambda$Prolog.  Finally we present a kernel and a prototype refiner for CIC
written in ELPI.  We then conclude.

\section{The elaborator component of today's ITPs}

An elaborator takes in input a \emph{partial term} and optionally a
type, and returns the closest term similar to the input one such that
the term has the expected type. Both the input and output terms are
partial in the sense that subterms can be omitted and replaced with
named holes to be later filled in or, in logic programming
terminology, with \emph{existentially quantified metavariables}. For
example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are
quantified outside the term represents the $\lambda$-abstraction over
$x$ of a type yet to be determined of the application of $f$ to two
arguments, both to be yet determined and such that $x$ can happear
free only in the first. Elaborating the term versus the expected type
$\mathbb{N} \to \mathbb{B}$ will instantiate $T$ with $\mathbb{N}$ and
verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it
allows to interpret the terms that are input by the user, usually by
means of a user-friendly syntax where information can be omitted,
mathematical notation is used and abused, subtyping is assumed even if
elements of the first type can only be coerced to elements of the
second by inserting a function call in the elaborated term. A better
elaborator therefore gives to the user the feeling of a more
intelligent and user friendly system. On the other hand, via
Curry-Howard, a partial term is a partial proof and an ITP is all
about instantiating holes in partial proofs with new partial terms to
advance in the proof. The elaborator is thus the mechanism that takes
a partial sub-proof and makes it fit in the global one to progress on
a particular proof obligation. In other words, all tactics of the ITP
ultimately produce partial proof terms that are elaborated. The more
advanced is the elaborator, the simpler the code implementing tactics
can be.

\subsection{Implementing an elaborator: state of the art}  %%%%%%

The elaborators of the majority of the provers mentioned above are all
implemented according to the same schema: the syntax of terms is augmented with
explicitly substituted existential variables
and the judgements of the kernel are re-implemented from scratch, generalizing
them to take in account metavariables and elaboration.

In particular one no more works with terms, but always with partial terms and
a metasenv: a data structure assigning to metavariable a typing judgement
and, eventually, an assignment.

All algorithms manipulating terms are extended to open
terms, in particular conversion (implemented by the 
\verb+sub+  and \verb+conv+ predicates in the running example)
becomes narrowing, i.e. higher order unification under a
mixed prefix~\cite{???} in presence of rewriting rules.
Unification requires metavariable instantiation, that is
implemented lazily for efficiency.  Type checking \verb+of+ is generalized to
elaboration, for example by replacing all calls to conversion with calls to
narrowing and by threading around the metasenv.

The approach is sub-optimal in many ways.  We here identify the main problems
in the state of the art.

\begin{description}
\item[programming platform]
Much of this new code has very little to do with the prover or the implemented
logic. In particular code that deals with binders ($\alpha$-conversion,
capture avoiding substitution, renaming) and code that deals with existentially
quantified metavariables (explicit substitution management, name capturing
instantiation).
\item[intricacy of algorithms]
Such code is far from being trivial, since it tackles problems that, like
higher order unification, are only semi-decidable. For efficiency reasons a lot
of incomplete heuristics are implemented to speed up the system and reduce
backtracking. The heuristics, however, are quite ad-hoc and they interact with
one another in unpredictable ways. Because they are hidden in the code, the
whole system becomes unpredictable to the user.
\item[code duplication]
Given such complexity in the elaborator, and the safety requirements of interactive provers, the kernel of the system is kept simple by making it unaware of partial terms.  As a consequence a lot of code is duplicated, and
the elaborator ends up being a very complicated \emph{twin brother of the
kernel} (Huet's terminology).
\item[twins' disagreement]
Worse than that, such twin components need to agree on ground terms.
Typically a proof term is incrementally built by the elaborator:
starting from a meta variable that has the type of
the conjecture the proof commands make progress by instantiating such
meta with partial terms.  Once there are no unresolved meta variables left,
the ground term is checked, again and in its totality, by the kernel.
Bugs in the elaborator are detected by the kernel, letting the user 
\item[extensibility of the elaborator]
Finally, the elaborator is the brain of the system, but it is oblivious of the
pragmatic ways to use the knowledge in the prover library, e.g.  to
automatically fill in gaps~\cite{mathcomponents}, to coerce data from one type
to another~\cite{coercivesubtyping} or to enrich data to resolve mathematical
abuse of notation~\cite{nonuniformunificationhints}. Therefore systems provide
ad-hoc extension points to increase the capabilities of the elaborator.  The
languages to write this code are typically high level, declarative, and
try to hide the intricaces of bound variables, metavariables, etc. to the user.
The global algorithm is therefore split in multiple languages, defying the hope
for static analysis and documentation of the elaborator.
\end{description}

\subsection{The proposed approach: semi shallow embedding}

The motivation of our research is to try to improve over the latter issues.  We
propose to identifying an high level (logic) programming language suitable for
the implementation of elaborators. In particular:
\begin{enumerate}
\item The programming language takes care of the representation of
	\emph{bindings and metavariables} in the spirit of semi
	shallow embedding~\cite{Dunchev:2016:IHH:2966268.2966272},
	solving the \textbf{programming platform} issue.
	It also improves on \textbf{extensibility} by allowing
	both the core implementors and the users to work on the same,
	high level code.
	Finally the \textbf{intricacy of elaboration} is be mitigated.
\item The programming language features a primitive and powerful
	notion of extensibility: programs are organized into clauses,
	and new clauses can be freely added.
	In this way the rules of the kernel are not be re-implemented in the
	elaborator. On the contrary they are extended to cover partial terms,
	solving the \textbf{code duplication} issue.
	Also, the \textbf{twins' disagreement} problem becomes less severe,
	since most code is shared.
	At the same time \textbf{extensibility of the elaborator}
	become less ad-hoc: the user simply declares new clauses.
\item Finally the programming language has a clean semantics, making it
	easy to prove that the extensions to the kernel
	only accept partial terms that are, once completed,
	well-typed according
	to the core set of rules of the kernel.  This completely
	solve the \textbf{twins' disagreement} problem, making it possible to
	merge the kernel and the elaborator.
\end{enumerate}

We envisage such language to be a logic one for two reasons:
first we hope to reuse or at least extend $\lambda$Prolog;
second we observe that another component of each ITP, the one
implementing proof commands, can take real advantage from a 
programming language where backtracking is built in, in
particular to write proof commands performing proof search.

The Higher order Abstract Syntax approach identifies the object
language binders and the meta language ones, obtaining $\alpha$-conversion
and $\beta$-reduction for free.  The semi shallow embedding we propose
\emph{identifies the metavariables of the object language with the 
meta variables of $\lambda$Prolog}.  Indeed such meta variables already
come with automatic instantiation and context management.

At a first sight, the runtime of $\lambda$Prolog seems to already provide
an implementation of the metasenv data structure, and all related operations.
Does this idea work out of the box? Unfortunately not quite.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\lambda$Prolog meets partial terms}

We already know from~\cite{jlp98} that $\lambda$Prolog is the minimal
extension of Prolog that allows to implement inductive predicates over
syntax containing binders. Does it work when applied to data that is
meant to containt existentially quantified metavariables too?

Consider the $\lambda$-term \verb+(lam a\ P a)+ that encodes a
partial proof of $\forall A, A \to A$.
If we run the following query, the computation diverges:

\begin{verbatim}
?- of (lam T a\P a) (arr (sort I) a\ arr a _\ a)
\end{verbatim}

\subsection{Generative semantics and constraint declaration} %%%%%%%%%%%
\label{sec:delay}

Indeed, \verb+P+ is flexible and the \verb+of (app M N) BN+
rule applies indefinitely.

Indeed the \verb+of+ predicate inherits from Prolog a generative semantics:
when called recursively on a flexible input, it enumerates all
instances trying to find the ones that are well typed. Even when proof
search does not diverge, the behaviour obtained is not the one
expected from an elaborator for an \emph{interactive} prover: the
elaborator is not meant to fill in the term, unless the choice is
obliged. On the contrary, it should leave the meta variable not
instantiated and should \emph{remember} the need for
veryfing if the predicate holds later on, when the metavariable gets
instantiated. In the example above, type-checking \verb+(lam T a\P a)+ forces
the system to remember that term of type \verb+(arr a _\ a)+ has to be
provided, that in turn corresponds to the proof obligation
$A : o \vdash A \to A $.

This is not a problem specific of $\lambda$Prolog since Prolog behaves the same.
Nevertheless, all modern Prolog engines provide a \verb+var/1+ built-in to
test/guard predicates against flexible input, provide one/many variants of
\verb+delay/2+ to suspend a goal until the input becomes rigid, and provide
modes declarations to both statically/dynamically detect problematic goals
and to (semi)automatically delay (suspend) them.
These mechanisms, however, have never been standardized.

For example, by using the \verb+delay+ pack of SWI-Prolog~\cite{SWI}, the goal
\verb+plus(X,1,Y)+ is delayed until either \verb+X+ or \verb+Y+ are
instantiated.  Delayed goals can be thought as \emph{constraints} over the
metavariables occurring in them. In the example above, the programmer is
imposing a constraint between \verb+X+ and \verb+Y+.

Going back to our diverging goal, one could imagine to provide a directive
like

\begin{verbatim}
delay (of X T) on X.
\end{verbatim}

that would delay goals of the form \verb+of X T+ whenever \verb+X+
is flexible.  As a result, instead of diverging, the running example
would terminate leaving the following (suspended) goal unsolved:

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{verbatim}

Such suspended goal is to be seen as a typing constraint on
assignments to \verb+P+: when \verb+P+ gets instantiated by a
term \verb+t+, the goal \verb+(of (t x) (arr x _\ x))+ is resumed
hence \verb+(t x)+ is checked to be of type \verb+(arr x _\ x)+.
In turn such check can either:
\begin{itemize}
\item terminate successfully if \verb+t+ has the right type and is ground,
	i.e. \verb+t = x\lam x w\ w+.  This corresponds to a proof step
	that closed the goal, i.e. \verb+t+ is generated by a
	proof command ending a proof branch.
\item fail, rejecting the proposed assignment for \verb+P+, e.g. 
	\verb+t = x\lam x w\ x+.  This corresponds to an erroneous proof step,
	i.e. the user invoked command fails.
\item result in one or more new constraints if \verb+t+ is partial,
	e.g. \verb+t = x\lam x w\Q x w+.  This corresponds to progress
	in the interactive proof construction.
\end{itemize}

Extending $\lambda$Prolog with delay directive makes it possible to represent
the metasenv as the set of delayed typing constraints and, at the same time,
avoiding the generative semantics of predicates like \verb+of+.

\subsection{Meta theory and constraint Propagation} %%%%%%%%%%%%%%%%%%%%%%%%%%

In many situations, the constraints that accumulate over a metavariable are
not independent and sets of constraints can be rewritten in order to simplify
them.

For example, if \verb+P+ does not occur linearly, one could end up with two
distinct constraints it:

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
of x (sort J) ?- of (P x) (arr (T x) y\ S x y)
\end{verbatim}

If the object language features uniqueness of typing, one surely wants to
get rid of the second constraint, and force \verb+T = a\ a+,
\verb+S = a\ b\ a+, and \verb+J = I+.
This way, when \verb+P+ gets instantiated only one goal is resumed and hence
its assigned term is checked only once.  In addition to that, when the
set of constraints is unsatisfiable one wants to fail,
and eventually backtrack.

In standard logic programming, rewriting a set of constraints is called
\emph{constraint propagation} and the programming languages that allow the
user to declare constraints and that propagate them are called
\emph{Constraint Programming Languages} (CLPs).  For example, the set $\{0
	\leq N \leq 4, 2 \leq N \leq 5\}$ can be \marginpar{acronym false}
	rewritten into $\{2 \leq N \leq 4\}$ without changing the set of
	ground solutions.

Most CLPs do not allow the user to define new constraints and propagation
rules in user space. A notable exception is the first order language
CHR~\cite{chr}. In CHR the user declares predicates and then gives a set of
rewriting rules of the form $S_1 \setminus S_2 ~|~ G \iff S_3$ whose
declarative semantics is that $\bigwedge S_1 \wedge G \Rightarrow (\bigwedge
S_2 \iff \bigwedge S_3)$ and whose operational semantics is to match the
current set of constraints against both $S_1$ and $S_2$ and, if the clause $G$
holds, replace the constraints in $S_2$ with the ones in $S_3$. All syntactic
components can be omitted: $G$ defaults ot \verb+true+ and the sets
$S_1,S_2,S_3$ to the emptyset. The $\iff$ symbol is also omitted when $S_3$
is, and $|$ is omitted when $G$ is. The semantics of the language is completed
by a strategy that fixes the order in which propagation rules are fired and in
what cases propagation rules can be backtracked. Multiple such semantics for
CHR have been proposed.

Adding to $\lambda$Prolog a CHR system enables
\begin{itemize}
\item forward reasoning during search. When propagation rules are theorems,
	then they fix well the proof theoretic semantics of $\lambda$Prolog
	and their application happens forward, while the SLD resolution engine
	goes backward
\item implement the optimizations given by the meta theory of the object
	language.  In the running example uniqueness of typing is key to keep
	the evarmap linear in the number of missing sub-proofs
\item easily turn checking into inference.  For example by replacing the
	predicates \verb+<,max,succ+ with generic order constraints
	\verb+leq,ltn+ one gets an elaborator that works with floating sorts,
	rather than fixed integers, and produces an acyclic graph of
	constraints linking these.
\end{itemize}

Extending $\lambda$Prolog with an expressive language to rewrite 
constraints makes it possible to model typical invariants on
the metasenv and benefit from constraint solving/propagation when coding
inference (i.e. postpone an assignment until all constraints are known,
avoiding heuristics).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ELPI = $\lambda$Prolog + CHR}\label{sec:elpi}

Motivated by the observations in the previous section, we implemented a new,
efficient~\cite{elpiLPAR} interpreter for $\lambda$Prolog, called \emph{ELPI}
(Embedded Lambda Prolog Interpreter) and we augmented the language with the
possibility to delay arbitrary predicates turning them into constraints, and to
propagate constraints using CHR style rewriting rules and additional
mechanisms.

In addition to the new programming constructs that deal with constraints, it
slightly differs from the version of $\lambda$Prolog implemented in Teyjus in a
few minor aspects:
\begin{itemize}
\item In ELPI all types and type and sort declarations can be omitted, whereas
	they are mandatory in Teyjus. Originally, and according
	to~\cite{jlp98}, types were necessary to implement Huet's algorithm for
	higher order unification. However, for several years now the
	unification algorithm of Teyjus only solves equations in the pattern
	fragment discovered by Miller~\cite{patternfrag}, which admits most
	general unifiers and reduce unification to a decidable problem. All
	other equations are automatically delayed by Teyjus to be fired only
	when the equation is instantiated to one in the pattern fragment. At
	the level of the implementation, the code of ELPI that implements this
	delay is shared with the one to delay arbitrary predicates.
\item The module system of Teyjus is not implemented. Only the
	\verb+accumulate+ directive is honored for backward compatibility and
	with a different semantics. In ELPI we provide instead explicit
	existentially quantified local constants whose scope spans over a set
	of program clauses. This mechanism gives in a simple way predicate and
	constructor hiding that are provided differently by the module system
	of Teyjus.
\item In a few corner cases, the parsing of an expression by Teyjus is
	influenced by the types. In particular, types are used to disambiguate
	between lists of elements and a singleton list of conjunctions. The
	syntax is disambiguated in a different way in ELPI.
\end{itemize}

Despite the differences above, we tried very hard to maintain backward
compatibility with Teyjus and its standard library. Indeed, we are able to
execute in ELPI all the code from Teyjus that we collected from the Web, up to
a very few minor changes to the source code, mostly due to 2 and 3.

\subsection{Incomplete terms and delay} %%%%%%%%%%%%%%%%%%%%%%%%

In section~\ref{sec:delay} we describe a global directive to
turn active goals into constraints (suspended goals).

\begin{verbatim}
delay (of X T) on X.
\end{verbatim}

The possibility to delay goals is already present in other $\lambda$Prolog
implementations, but to our knowledge the condition for suspension is
hard coded. For example Teyjus suspends goals of the form \verb+A = B+
when they fall outside of the $L_\lambda$ fragment.

To be able to experiment we exposed lower level primitives that, once
combined, let one express the above directive in very flexible ways.

The first primitive is called \verb+mode+ and lets one tag arguments of
a predicate as input ones.

\begin{verbatim}
mode (of i o).
\end{verbatim}

The result is that arguments marked as input are \emph{matched}
against the corresponding arguments in the goal.  This mean the goal is not
instantiated.  

\begin{verbatim}
of (lam T F) T :- ...
of (app M N) T :- ...
of X T :- var X, delay (of X T) on X.
\end{verbatim}

Note that this is stronger than placing the following clause on top
and not using \verb+mode+

\begin{verbatim}
of X T :- var A, !, ...
\end{verbatim}

since hypothetical clauses may be placed by the runtime before this one,
defeating the purpose of the clause.

When a predicate has an argument in input mode we provide some syntactic sugar
to ease the writing of the clause dealing with a flexible input.

\begin{verbatim}
of (?? as X) T :- delay (of X T) on X.
\end{verbatim}

where \verb+??+ is a symbol matching only flexible terms.
The \verb+t as X+ notation
is just naming the term \verb+t+, so that one can use it in the right hand
side of the clause.
It is worth mentioning that giving a special symbol to denote
flexible terms enables better indexing: in input mode the indexing facility
does not retrieve all clauses when the goal is flexible, only the ones mentioning \verb+??+.

Delayed goals become constraints keyed on the flexible variable
(\verb+X+ in the example above).  \emph{As soon} as \verb+X+ gets
instantiated, the delayed goal is resumed.  Symmetrically, as soon
as a constraint is added propagation is triggered (see next section).

If we go back to our previous example, the query

\begin{verbatim}
?- of (lam T a\P a) (arr (sort I) a\ arr a _\ a).
\end{verbatim}

terminates with a delayed constraint

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{verbatim}

ITPs build the proof term by instantiation, and one has to manually
check that when adding an assignment to the metasenv, the assigned term
has a type compatible with the one of the meta variable being assigned.
This mechanics is now implicit: instantiating forces resumption of a goal
and such goal checks if the assigned term is well typed of the type
of the meta variable just assigned.

As a side note, B-Prolog (and Picat) also use matching (and
achieve better performances.  Finally CHR needs matching, so
implementing matching is not a waste.

\subsection{Higher order constraint propagation}

CHR rules are at the meta level, and can be guarded by $\lambda$Prolog
queries.  Hence we have 2 $\lambda$Prolog runtimes: the low one (where the
program generating constraints runs) and the high one (where CHR guards are
executed).

CHR syntax, and HO extensions (alignment)
\begin{verbatim}
CHR = "constraint" NAME { NAME } "{" {CLAUSE} { RULE } "}"
RULE = "rule" PRESENT TO_REMOVE ALIGNMENT GUARD NEW_GOAL "."
PRESENT = { [ TERM ] ?- TERM }
TO_REMOVE = [ "\" [ TERM ] ?- TERM { [ TERM ] ?- TERM } ]
ALIGNMENT = [ ">" VAR "~" VAR { "~" VAR } ]
GUARD = [ "|" TERM ]
NEW_GOAL = [ "<=>" TERM ]
}
\end{verbatim}

\verb+NAME+ is the name of a predicate.
\verb+PRESENT+ and \verb+TO_REMOVE+ a list of terms, they are called heads,
all have disjoint variables.
\verb+ALIGNMENT+ is a list of variables, one per head.
\verb+GUARD+ and \verb+NEW_GOAL+ are a $\lambda$Prolog queries
with variables that can appear in the heads.

A very simple example looks for 2 constraints on the same meta variable
that have the same context (up to permutation).  In such case the second
constraint is replaced by a goal asserting the two types are convertible.

\begin{verbatim}
constraint of {
  rule (G1 ?- of X T1) \ (G2 ?- of Y T2) > X ~ Y
       | (equiv G1 G2) <=> (G1 => conv T1 T2).
}
\end{verbatim}

The type of \verb+G1+ and \verb+G2+ is list, \verb+equiv+
compares two lists as sets, and \verb+G1 => conv T1 T2+
is sugar for \verb+G1x => .. => G1z => conv T1 T2+.

The alignment part \verb+> X ~ Y+ asserts \verb+X+ and \verb+Y+
are the same meta variable and finds a byjection between the
heigen variables visible by \verb+X+ and \verb+Y+.  E.g. if \verb+X+
matches \verb+P x y+ and \verb+Y+ matches \verb+P z w+, then
such byjection is \verb+x->z; y->w+.

A rule operates on the subset of constraints that have \verb+NAME+
as the head predicate.  Triggering of a rule happens as follows:
\begin{enumerate}
\item a new constraint $c$ is added to the global store (delay)
\item all permutations of goals in the store of size $|heads|$
	containing $c$ are candidates, one by one matched against the
	heads. The ones that are successfully matched are $\bar{cs}$
\item constraints are HO aligned (see below) according to the key and
	byjection $j$ is found
\item all constraints are frozen, i.e. flexible terms are replaced by
	fresh constants and the alignment byjection is applied
\item the guard is run in a $\lambda$Prolog runtime, at depth
	$max(\bar{cs})$
\item the new goal is added to the low runtime at depth max with no
	augmented program (the user can use implication)
\end{enumerate}

\subsubsection{Alignment}

\begin{math}
  \nabla x_1\ldots x_n, of~(X~x_1 \ldots x_n) T[x_1\ldots x_n])
 \equiv
  \nabla y_1\ldots y_n, of~(X~y_1 \ldots y_n) T[y_1\ldots y_n])
\end{math}

That is equivariate unification~\cite{xxx}, NP, costly.
Also no nabla vectors, but n-ary context (list) application, so explicit
nabla quantifiers are hard. Pragmatic: implcit nabla on the key, fast for
llam (only 1 permutation).

two modes: alignment \verb+~+ and disjoint union \verb+#+
so that one can mix and match all the terms on the heads and never
confuse. Example use case: CSC?

FINALLY, the rule for uniq of typing for ECC....


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application}

\subsection{Kernel for CIC}\label{sec:kernel}

\input{kernel}

\subsection{Prototype elaborator}\label{sec:elaborator}
We are developing an elaborator for CIC as a modular extension of the kernel described in Sect.~\ref{sec:kernel}. The elaborator mimics as close as possible the behaviour of the one of Matita 0.99.1~\ref{xxx}, with the exception of the handling of universe constraints that follows Coq XXX~\ref{yyy}.

An alternative very promising choice would have been to mimic the elaborator described in~\ref{zzz} and already presented via typing rules that yield a set of higher order unification constraints to be later solved. In the future, the choice to be closer to Matita will allow us to easily compare the performances of the elaborator written in ELPI with the one of Matita written in OCaml, in order to further optimize the ELPI interpreter.

To implement the elaborator, the following steps need to be taken at the beginning of a file that accumulates at the end the code of the kernel and of the file implementing the PTS rules.
\begin{itemize}
\item The accumulated code for the PTS is not the ones accumulated by the kernel that just verifies the typing inequations, but the one based on the first order CHR rules presented in Section~\ref{chruniverses}.
\item The following predicates defined by structural recursion over a term
 must be given a suitable mode to prevent the generative behaviour:
 AAA (type checking), BBB (reduction), CCC (conversion), plus all derived
 predicates like DDD that infers a type and checks that the inferred type is
 a sort.
\item A line is inserted to delay type checking a flexible term. A suitable set of propagation rules in CHR style can be added to capture unicity of typing.
\item Conversion when at least one of the arguments is a flexible term amounts to higher order narrowing. Instead of delaying the goal, an heuristic already used in Matita immediately rewrites the constraint by solving the unification problem. The heuristic, in the case $M~t = s$ tries to instantiates $M$ with
$\lambda x. s[x/t]$ where the latter operation replaces with $x$ subterms of $s$ that unify with $t$. In practice, in Huet's terminology it always prefer projection to mimic. When $M$ is applied to multiple $t$s or to flexible $t$s, further heuristics are used to pick one of the solution. These heuristics are clearly incomplete, discarding all solutions but one and sometimes failing to find a solution when it exists. However, they are more general than the ones implemented in Coq and, from a practical perspective, they guess most of the time the unifier that is expected by the user when interactive with the ITP.
\item In place of delaying reducing a flexible term, the term is returned in the current state, weakening the semantics of the the BBB predicate. In practice, this is not a problem because the result of reduction is then always passed to the conversion predicate.
\end{itemize}

\begin{verbatim}
mode (t+step i o).
mode (has+sort i o).
mode (conv+whnf i i i i i).
t+step (?? as K) T :- !, $constraint (t+step K T) K.
has+sort (?? as S) T :- !, $constraint (has+sort S T) S.

conv+whnf A B C D E :- $print "##" (conv+whnf A B C D E), fail.

% Bug1: ci possono essere let-in sulle var in Ti
% Bug2: potrebbe servire un <= se il secondo è una sorta
%conv+whnf (?? as T1) [] _M T2 L2 :- !, $llam_unif T1 {zip T2 L2}.
conv+whnf T1 L1 _M (?? as V) L2 :- !, bind-list L2 {zip T1 L1} V.

conv+whnf (?? as T1) L1 M T2 L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T1.
conv+whnf T1 L1 M (?? as T2) L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T2.

constraint t+step has+sort conv+whnf r+step {
}

%%% library
mode (zip i i o).
zip HD [] HD :- !.
zip (appl HD TL) Args (appl HD TLArgs) :- !, append TL Args TLArgs.
zip HD Args (appl HD Args).

is_rigid C :- $is_name C. % ; C = const _ ; C = indt _ ; C = indc _

mode (bind-list i i o).
bind-list [] T T' :- copy T T'.
bind-list [ ?? |VS] T R :- !, pi x\ bind-list VS T (R x).
bind-list [appl C AS | VS] T R :- is_rigid C, !,
  pi x\ (pi L\ copy (app[C|L]) x :- conv+args L AS) => bind-list VS T (R x).
bind-list [C|VS] T R :- is_rigid C, !,
  pi x\ copy C x => bind-list VS T (R x).

mode (copy i o).
copy A B :- $print "AA" (copy A B), fail.
copy X Y :- r+step X Y _. % Bang or not???
copy X Y :- $is_name X, X = Y, !.
copy X Y :- $is_name X, r+step X T _, !, copy T Y.
copy (sort _ as C) C :- !.
copy (appl X1 L1) (appl X2 L2) :- copy X1 X2, $print "UU", map copy L1 L2.
\end{verbatim}

\subsection{universes}

Implementing in $\lambda$Prolog the part of the elaborator for ECC that deals
with universe constraints is much harder and less elegant than the previous
approach based on CHR. Indeed, the only solution is to keep an explicit list
of constraints that become a third argument of the type-checking judgement and
that needs to be threaded around the program, polluting the code and not
allowing to reuse the type-checking code for the elaborator. Moreover, every
time an additional constraint is added to the list, some special predicate to
fire the propagation rules according to some strategy needs to be called, de
facto forcing the user to implement the boilerplate code for constraint
propagation.


For example, the rules for type-checking ECC call the predicates \verb+<,max,succ+ over integers. The corresponding elaborator may be faced with sorts like \verb+sort M+ where \verb+M+ is a metavariable. The elaborator is asked to compute a suitable assignment to the metavariables that respects all constraints. Without breaking consistency, the successor predicate can be relaxed to a strictly less than, the max to a generic upper bound and the set of integers to an unspecified partial order, de facto replacing the predicates \verb+<,max,succ+ with generic predicates \verb+leq,ltn+ (lax and strict inequality over an unspecified set). Finally, the two predicates, once turned into constraints, admit the following complete set of CHR propagation rules able to detect strict cicles that correspond to unsatisfiability of the constraints.

\label{chruniverses}
\begin{verbatim}
succ I J :- ltn I J.
I < J :- ltn I J.
max I J K :- leq I K, leq J K.

constraint leq ltn {
  % incompatibility
  rule (leq X Y) (ltn Y X) <=> false.
  rule (ltn X Y) (ltn Y X) <=> false.
  rule (ltn X X) <=> false.
  
  % reflexivity
  rule \ (leq X X).

  % antisymmetry
  rule (leq X Y) \ (leq Y X) <=> (Y = X).

  % transitivity
  rule (leq X Y) (leq Y Z) <=> (leq X Z).
  rule (leq X Y) (ltn Y Z) <=> (ltn X Z).
  rule (ltn X Y) (leq Y Z) <=> (ltn X Z).
  rule (ltn X Y) (ltn Y Z) <=> (ltn X Z).

  % idempotence
  rule (leq X Y) \ (leq X Y).
  rule (ltn X Y) \ (ltn X Y).
}
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related works}
\begin{enumerate}
\item Costrutti tipo delay
\item Logical frameworks (Dedukti, MMT, etc.) e brrr Isabelle
\item Da qualche parte: il pre-print by Abel sull'elaborator basato su
 constraint e lean che implementa un constraint solver di qualche tipo
\end{enumerate}

\section{SCALETTA}

Prima possibilità:
\begin{enumerate}
\item Si introduce per bene (??) la sintassi/semantica di CHR
\item Si fa notare quanto sia costoso il passo di firing di una regola CHR
      e si conclude che è meglio identificare sotto-frammenti
\item Si introducono i mode + il costrutto delay + i pattern sulle metavariabili
      spiegandoli come il caso comune nel quale un constraint sia immediatamente
      propagabile senza doverne cercare altri
\item (??) Si aggiungono gli altri costrutti di ELPI
\item Kernel modulare di Ferruccio ed esperimenti con Matita
\item Estensione con universi flottanti e refiner
\end{enumerate}

Seconda possibilità: si parte dal kernel di Ferruccio e poi ``on-demand'' si introduce il refiner descrivendo i comandi che usiamo 

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
