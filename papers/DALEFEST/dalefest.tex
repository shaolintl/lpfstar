\documentclass{easychair}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

% \title{Higher Order Constraint Logic Programming with Applications to Interactive Theorem Proving}
\title{The pragmatic construction of ELPI}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science and Engineering, University of Bologna \email{ferruccio.guidi@unibo.it} \and
  Department of Computer Scienc and Engineeringe, University of Bologna \email{claudio.sacerdoticoen@unibo.it} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{F. Guidi, C. Sacerdoti Coen, E. Tassi}
\titlerunning{whatever}

\begin{document}
\maketitle

\begin{abstract}
Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

In this paper we present a first proposal for an higher order constraint logic
programming language obtained as an extension of $\lambda$Prolog. We also present
applications to the implementation of an elaborator for the Calculus of
Inductive Constructions, i.e. the core of a modern interactive theorem prover
in the style of Coq, Agda or Matita.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\paragraph{A pragmatic reconstruction of $\lambda$Prolog.}

In~\cite{jlp98} Belleannée et. alt. propose a pragmatic reconstruction
of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order
Logic Programming (HOLP) language introduced by Dale Miller and
Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the
minimal extension of Prolog that allows to program by structural
induction on $\lambda$-terms. According to their reconstruction, in
order to achieve that goal, Prolog needs to first be augmented with
$\lambda$-abstractions in the term syntax; then types are added to
drive full higher-order unification; then universal quantification in
goals and $\eta$-equivalence are required to express relations between
$\lambda$-abstractions and their bodies; and finally implication in
goals is needed to allow for structural induction definitions of
predicates.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can
easily encode all kind of binders without the need to take care of
binding representation, $\alpha$-conversion, renaming and
instantiation. Structural induction over syntax with binders is also
made trivial by combining universal quantification and implication
following the very same pattern used in Logical Frameworks like LF
(also called $\lambda$P). Indeed, LF, endowed with an HOLP semantics
like in ???(ELF? Beluga? MMT?) is just a sub-language of
$\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the
following two lines program to compute the simple type of a
$\lambda$-expression:

\begin{verbatim}
kind term type.
type app term -> term -> term.
type lam (term -> term) -> term.

type arr typ -> typ -> typ.

type of term -> typ -> o.

of (app M N) B :- of M (arr A B), of N A.
of (lam F) (arr A B) :- pi x\ of x A => of (F x) B.
\end{verbatim}

\paragraph{$\lambda$Prolog for proof-checking.}

According to the Curry-Howard isomorphism, the program above can also
be interpreted as a proof-checker for minimal propositional logic. By
escalating the encoded $\lambda$-calculus to more complex terms and
types, it is possible to obtain a proof-checker for a much richer
logic, like the Calculus of Inductive Constructions that, up to some
variations, is the common logic shared by the interactive theorem
provers (ITPs) Coq~\cite{}, Lean~\cite{}, Matita~\cite{} and
Agda~\cite{}. For example, in~\cite{us} we implemented in
$\lambda$Prolog a type-checker for Landau's Grundlagen XXXXXXXX.

Indeed, all the ITPs mentioned above are implemented following
basically the same architecture. At the core of the system there is
the \emph{kernel}, that is the trusted code base (together with the
compiler and run-time of the programming language the system is
written on). The kernel just implements the type-checker together with
all the judgements required for type-checking, namely: well formation
of contexts and environments, substitution, reduction, convertibility.
The last three judgements are necessary because the type system has
dependent types and therefore types need to be compared up to
computation.

This scales quite well, up to systems with dependent types and
universes.

\begin{verbatim}
type conv term -> term -> o.

conv (app M1 N1) (app M2 N2) :- conv M1 M2, conv N1 N2.
conv (app (lam F) N) T :- conv (F N) T.
conv T (app (lam F) N) T :- conv T (F N).
conv (lam F1) (lam F2) :- pi x\ conv x x => conv (F1 x) (F2 x).
\end{verbatim}

\noindent
that is needed for dependent types

\begin{verbatim}
type arr term -> (term -> term) -> term.

of (app M N) BN :- of M (arr A1 Bx), of N A2, sub A2 A1, BN = Bx N.
of (lam F) (arr A B) :- pi x\ of x A => of (F x) (B x).
\end{verbatim}

\noindent
and for hierarchy of sorts

\begin{verbatim}
type sort int -> term.

of (sort I) (sort J) :- J is I + 1.
of (arr A Bx) (sort K) :-
  of A (sort I), (pi x\ of x A => of (Bx x) (sort J)), K is max I J.
conv (sort I) (sort I).
sub A B :- conv A B.
sub (sort I) (sort J) :- I < J.
sub (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
\end{verbatim}

\noindent
we now have the predicative, universal fragment of Luo's ECC~\cite{luo}, wow!


\paragraph{From proof-checking to interactive proving.}

The kernel is ultimately responsible for guaranteeing that a proof
built using an ITP is fully sound. However, in practice the user never
interacts with the kernel and the remaining parts of the system do not
depend on the behaviour of the kernel. Where the real intelligence of
the system lies is instead the second layer, called \emph{elaborator}
or \emph{refiner}~\cite{??,??,??}.


In this paper we investigate if $\lambda$Prolog can do that too, if not
what shall be added.  We start by detailing what an elaborator is and
why the state of the art is not satisfactory.  We then present the
issues
we faced when coding an elaborator in $\lambda$Prolog and how similar issues are
dealt with in first order Prolog.  We then present ELPI, our extension to
$\lambda$Prolog.  Finally we present a kernel and a prototype refiner for CIC
written in ELPI.  We then conclude.

\section{The elaborator component of today's ITPs}

An elaborator takes in input a \emph{partial term} and optionally a
type, and returns the closest term similar to the input one such that
the term has the expected type. Both the input and output terms are
partial in the sense that subterms can be omitted and replaced with
named holes to be later filled in or, in logic programming
terminology, with \emph{existentially quantified metavariables}. For
example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are
quantified outside the term represents the $\lambda$-abstraction over
$x$ of a type yet to be determined of the application of $f$ to two
arguments, both to be yet determined and such that $x$ can happear
free only in the first. Elaborating the term versus the expected type
$\mathbb{N} \to \mathbb{B}$ will instantiate $T$ with $\mathbb{N}$ and
verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it
allows to interpret the terms that are input by the user, usually by
means of a user-friendly syntax where information can be omitted,
mathematical notation is used and abused, subtyping is assumed even if
elements of the first type can only be coerced to elements of the
second by inserting a function call in the elaborated term. A better
elaborator therefore gives to the user the feeling of a more
intelligent and user friendly system. On the other hand, via
Curry-Howard, a partial term is a partial proof and an ITP is all
about instantiating holes in partial proofs with new partial terms to
advance in the proof. The elaborator is thus the mechanism that takes
a partial sub-proof and makes it fit in the global one to progress on
a particular proof obligation. In other words, all tactics of the ITP
ultimately produce partial proof terms that are elaborated. The more
advanced is the elaborator, the simpler the code implementing tactics
can be.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{State of the art in implementation of elaborators.}

The elaborators of the majority of the provers mentioned above are all
implemented according to the same schema: the syntax of terms is
augmented with explicitly substituted existential variables; the
judgements of the kernel are re-implemented from scratch, generalizing
them to take in account metavariables and elaboration. In particular:
reduction acts on metavariables too; conversion becomes narrowing,
i.e. higher order unification under a mixed prefix~\cite{???} in
presence of rewriting rules; unification requires metavariable
instantiation, that is implemented lazily for efficiency; type
checking is generalized to elaboration, for example by replacing all
calls to conversion with calls to narrowing and by threading around a
new environment for the signature of metavariables.

The approach is sub-optimal in many ways:
\begin{enumerate}
\item A lot of code is duplicated with the kernel.
\item A lot of code is essentially logic independent, but it needs to
	be implemented again for every system: mostly code that deals
	with binders ($\alpha$-conversion, capture avoiding
	substitution, renaming) and code that deals with existentially
	quantified metavariables (explicit substitution management,
	name capturing instantiation, higher order unification under a
	mixed prefix).
\item There is no guarantee that elaborated terms will be actually
	accepted by the kernel.
\item Most of the problems tackled, like higher order unification, are
	only semi-decidable. For efficiency reasons a lot of
	incomplete heuristics are implemented to speed up the system
	and reduce backtracking. The heuristics, however, are quite
	ad-hoc and they interact with one another in unpredictable
	ways. Because they are hidden in the code, the whole system
	becomes unpredictable to the user, unless an heavy layer is
	put around the elaborator to recover predicibility by reducing
	the invocations of the elaborator (cfr.~\cite{SOZEAU?}).
\item The elaborator is the brain of the system, but it is oblivious
	of the pragmatic ways to use the knowledge in the prover
	library, e.g. to automatically fill in
	gaps~\cite{mathcomponents}, to coerce data from one type to
	another~\cite{coercivesubtyping} or to enrich data to resolve
	mathematical abuse of
	notation~\cite{nonuniformunificationhints}. Therefore the
	recent trend is to let the user drive the behaviour of the
	elaborator in user space by writing pieces of declarative code
	in disguise that are integrated in the core algorithm. The
	languages to write this code are obviously high level, hiding
	the user from the intricacies of representation of bound
	variables, metavariables, etc. The overall program logic is
	therefore splitted in multiple languages, defying the hope for
	static analysis and predictability.
\end{enumerate}

\paragraph{Motivations and approach}

The motivation of our research is to try to improve over the latter
issues by identifying an high level (logic) programming language as
general as possible, but suitable at least for the implementation of
elaborators. In particular:
\begin{enumerate}
\item The programming language should take care of representation of
	bindings and metavariables, and implement all related basic
	operations. This would solve issues 2) and 5) above, allowing
	both the core implementors and the users to work on the same,
	high level code.
\item Issue 3) should become less severe, because the now high level
	code would be inspectable by the user that could also replace
	it in case of need.
\item The rules of the kernel should not be re-implemented in the
	elaborator. On the contrary, and in the usual spirit of logic
	programming, it should be possible to implement the
	accumulator by adding rules to the kernel. Moreover, the
	language should make it easy to prove that the additional
	rules grant that the elaborated terms are well-typed according
	to the core set of rules of the kernel.
\end{enumerate}

The requirement for the language to be a logic one is motivated both
by our hope to reuse or at least extend $\lambda$Prolog and by the
observation that the third layer of an ITP, the ones of tactics, is
better implemented on a language where backtracking is naturally
implemented and controlled.

$\lambda$Prolog already has existentially quantified metavariables and the related operations. Therefore, in order to satisfy requirement 1 it seems sufficient to use a shallow embedding of the syntax: the metavariables of the partial terms are directly encoded as metavariables of $\lambda$Prolog, similarly to how binders are encoded via the $\lambda$-abstractions of $\lambda$Prolog. Does this idea work out of the box? Unfortunately not at all.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\lambda$Prolog meets partial terms: problems}


\paragraph{Failure of $\lambda$Prolog as an high level language for
implementing elaborators.}
We already know from~\cite{jlp98} that $\lambda$Prolog is the minimal
extension of Prolog that allows to implement inductive predicates over
syntax containing binders. Does it work when applied to data that is
meant to containt existentially quantified metavariables too?

Consider the $\lambda$-term \verb+(lam a\ P a)+ that encodes a
partial proof of $\forall A, A -> A$.
If we run the following query, the computation diverges:

\begin{verbatim}
goal> of (lam a\P a) (arr (sort i) a\ arr a _\ a)
\end{verbatim}

Indeed, \verb+P+ is flexible and the \verb+of (app M N) ..+
rule applies indefinitely.

Indeed the \verb+of+ predicate inherits from Prolog a generative semantics:
when called recursively on a flexible input, it blindly enumerates all
instances trying to find the ones that are well typed. Even when proof
search does not diverge, the behaviour obtained is not the one
expected from an elaborator for an \emph{interactive} prover: the
elaborator is not meant to fill in the term, unless the choice is
obliged. On the contrary, it should leave the metavariable not
instantiated, but the system should \emph{remember} the need for
veryfing if the predicate holds later on, when the metavariable gets
instantiated. In the example above, type-checking AAAAAAAAAA forces
the system to remember to BBBBBBBBBB that corresponds to the new proof
obligation CCCC.

This is not a problem specific of $\lambda$Prolog, also Prolog has it.
Indeed all modern Prolog engines provide a \verb+var/1+ built-in
to test/guard predicates against flexible input, provide one/many
variants of \verb+delay/2+ to suspend a goal until the input becomes rigid, and
provide modes declarations to both statically/dynamically detect problematic calls to be (semi)automatically delayed. These mechanisms, however, have never been standardized.

For example, by using the \verb+delay+ pack of SWI-Prolog~\cite{???}, the goal \verb+plus(X,1,Y)+ is delayed until either \verb+X+ or \verb+Y+ are instantiated.
Delayed goals can be thought as \emph{constraints} over the metavariables occurring in them. In the example above, the programmer is imposing a constraint between \verb+X+ and \verb+Y+.

In many situations, the constraints that accumulate over a metavariable are not independent and sets of constraints can be rewritten in order to simplify them. Rewriting a set of constraints is called \emph{constraint propagation} and the programming languages that allow the user to declare constraints and that propagate them are called \emph{Constraint Programming Languages} (CLPs). For example, the set $\{0 <= N <= 4, 2 <= N <= 5\}$ can be rewritten into $\{2 <= N <= 4\}$ without changing the set of ground solutions.

Most CLPs do not allow the user to define new constraints and propagation rules in user space. A notable exception is the first order language CHR~\cite{chr}. In CHR the user declares predicates and then gives a set of rewriting rules of the form $S_1 \setminus S_2 \iff G~|~S_3$ whose declarative semantics is that
$\big\wedge S_1 \wedge G \Rightarrow (\big\wedge S_2 \iff \big\wedge S_3)$ and whose operational semantics is to match the current set of constraints against both $S_1$ and $S_2$ and, if the clause $G$ holds, replace the constraints in $S_2$ with the ones in $S_3$.


\begin{verbatim}
constraint leq ltn {
  % incompatibility
  rule (leq X Y) (ltn Y X) <=> false.
  rule (ltn X Y) (ltn Y X) <=> false.
  rule (ltn X X) <=> false.
  
  % reflexivity
  rule \ (leq X X).

  % atisymmetry
  rule (leq X Y) \ (leq Y X) <=> (Y = X).

  % transitivity
  rule (leq X Y) (leq Y Z) <=> (leq X Z).
  rule (leq X Y) (ltn Y Z) <=> (ltn X Z).
  rule (ltn X Y) (leq Y Z) <=> (ltn X Z).
  rule (ltn X Y) (ltn Y Z) <=> (ltn X Z).

  % idempotence
  rule (leq X Y) \ (leq X Y).
  rule (ltn X Y) \ (ltn X Y).
}
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ELPI = $\lambda$Prolog + (ho)CHR}

\subsection{incomplete terms, delay}

The hi level directive is

\begin{verbatim}
delay (of X T) on X.
\end{verbatim}

The low level one (implemented)

\begin{verbatim}
mode (of i o).
of X T :- var X, delay (of X T) [X].
\end{verbatim}

Note that the mode makes it so that \verb+i+ arguments are never
instantiated by backchaining, so other rules (generative) would not
apply.  This is so common we provide the following syntactic sugar.

\begin{verbatim}
of (?? as X) T :- delay (of X T) [X].
\end{verbatim}

where \verb+??+ is a symbol unifying only with flexible, and
\verb+f (t as X) :- c+ de-sugars to \verb+f X :- X = t, c+,
i.e. it is a syntax to name a subterm reminiscent of what OCaml
pattern matching let one do.

Delayed goals become constraints keyed on the flexible variable
(\verb+X+ in the example above).  \emph{As soon} as \verb+X+ gets
instantiated, the delayed goal is resumed.  Symmetrically, as soon
as a contraint is added propagation is triggered.

If we go back to our previous example

\begin{verbatim}
goal> of (lam a\P a) (arr (sort I) a\ arr a _\ a).
\end{verbatim}

would end with a delayed goal

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{verbatim}

a subsequent query

\begin{verbatim}
goal> P = a\ lam pa\ Q a pa .
\end{verbatim}

would resume the goal

\begin{verbatim}
of x (sort I) ?- of (lam pa\ Q x pa) (arr x _\ x)
\end{verbatim}

that would progress into the again suspended goal

\begin{verbatim}
of x (sort I), of y x ?- of (Q x y) x
\end{verbatim}

ITPs build the proof term by instantiation, and one has to check
if it fits typing.  Here one cannot forget the check, since it is
the programming language that resumes type checking whenever there is
proof progress (by uvar instantiation).

\subsection{delayed goals, HO constraint propagation}

CHR seen before does 1st order constraints, here one wants to deal
with $\lambda$Prolog goals as constraints.  Eg one wants to write  propagation
rules like this one

\begin{verbatim}
constraint of {
  rule (G ?- of X T1) \ (G ?- of X T2) <=> (G ?- conv T1 T2).
}
\end{verbatim}

it reflects uniqueness of typing: if \verb+X+ is used non linearly,
then each occurrence must have the same type.

the proof theoretic semantics of $\lambda$Prolog suggests that G has to be unified
up to commutativity of cunjunction, and that heigen variables have to
be unified as in equivariate unification (i.e. an injective map).

For example,


\begin{verbatim}
goal> of (lam a\ lam pa\ P a pa) (arr (sort I) a\ arr a _\ a),
      of (lam a\ lam pa\ P a pa) (arr (sort J) a\ arr a _\ a).
\end{verbatim}

generates 2 suspended goals

\begin{verbatim}
of x (sort I), of y x ?- of (P x y) x
of w (sort J), of z w ?- of (P w z) w
\end{verbatim}

........

of course one would like G to be compared like that

\begin{verbatim}
constraint of {
  map _ [] [].
  map F [X|XS] [Y|YS] :- F X Y, map F XS YS.
  conv-of (of X T1) (of X T2) :- conv T1 T2.

  rule (nabla xv\ G2 xv ?- of (X xv) T1)
     \ (nabla yv\ G2 yv ?- of (X yv) T2)
     > (xv ~ yv)
   % after that point, G1 G2 T1 T2 are aligned on the names
   % and G1 G2 are sorted accordingly (or map is made more complex)
     | (pi zv\ map conv-of (G1 zv) (G2 zv))
     <=> (nabla zv\ G1 zv ?- conv T1 T2).
}
\end{verbatim}

or even better to have such semantics built-in in the former example
but what if outside llam?


DIRE CHE BASTA AGGIUNGERE un \$delay MA FORSE QUI CI VORREBBE QUELLO AD ALTISSIMO LIVELLO CHE NON ABBIAMO

FARLO VEDERE SULL'ESEMPIO?

AGGIUNGERE ESEMPIO SU PROPAGAZIONE CONSTRAINTS A LA CHR?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application}

\subsection{Kernel for CIC}

\input{kernel}

\subsection{Prototype elaborator}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related works}

\section{SCALETTA}

Prima possibilità:
\begin{enumerate}
\item Si introduce per bene (??) la sintassi/semantica di CHR
\item Si fa notare quanto sia costoso il passo di firing di una regola CHR
      e si conclude che è meglio identificare sotto-frammenti
\item Si introducono i mode + il costrutto delay + i pattern sulle metavariabili
      spiegandoli come il caso comune nel quale un constraint sia immediatamente
      propagabile senza doverne cercare altri
\item (??) Si aggiungono gli altri costrutti di ELPI
\item Kernel modulare di Ferruccio ed esperimenti con Matita
\item Estensione con universi flottanti e refiner
\end{enumerate}

Seconda possibilità: si parte dal kernel di Ferruccio e poi ``on-demand'' si introduce il refiner descrivendo i comandi che usiamo 

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
