\documentclass{easychair}

\usepackage{lipsum}

\title{Higher Order Constraint Logic Programming with Applications to Interactive Theorem Proving}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science, University of Bologna \email{...} \and
  Department of Computer Science, University of Bologna \email{...} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{Enrico,..}
\titlerunning{whatever}

\begin{document}
\maketitle

\begin{abstract}
Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

In this paper we present a first proposal for an higher order constraint logic
programming language obtained as an extension of $\lambda$Prolog. We also present
applications to the implementation of an elaborator for the Calculus of
Inductive Constructions, i.e. the core of a modern interactive theorem prover
in the style of Coq, Agda or Matita.
\end{abstract}

\section{Introduction}

In~\cite{jlp98} Belleannée et. alt. propose a pragmatic reconstruction of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order Logic Programming (HOLP) language introduced by Dale Miller and Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the minimal extension of Prolog that allows to program by structural induction on $\lambda$-terms. According to their reconstruction, in order to achieve that goal, Prolog needs to first be augmented with $\lambda$-abstractions in the term syntax; then types are added to drive full higher-order unification; then universal quantification in goals and $\eta$-equivalence are required to express relations between $\lambda$-abstractions and their bodies; and finally implication in goals is needed to allow for structural induction definitions of predicates.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can easily encode all kind of binders without the need to take care of binding representation, $\alpha$-conversion, renaming and instantiation. Structural induction over syntax with binders is also made trivial by combining universal quantification and implication following the very same pattern used in Logical Frameworks like LF (also called $\lambda$P). Indeed, LF, endowed with an HOLP semantics like in ???(ELF? Beluga? MMT?) is just a sub-language of $\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the following two lines program to compute the simple type of a $\lambda$-expression:

\begin{verbatim}
kind ...
type ...
of ...
of ...
\end{verbatim}

According to the Curry-Howard isomorphism, the program above can also be interpreted as a proof-checker for minimal propositional logic. By escalating the encoded $\lambda$-calculus to more complex terms and types, it is possible to obtain a proof-checker for a much richer logic, like the Calculus of Inductive Constructions that, up to some variations, is the common logic shared by the interactive theorem provers (ITPs) Coq~\cite{}, Lean~\cite{}, Matita~\cite{} and Agda~\cite{}. For example, in~\cite{us} we implemented in $\lambda$Prolog a type-checker for Landau's Grundlagen XXXXXXXX.

Indeed, all the ITPs mentioned above are implemented following basically the same architecture. At the core of the system there is the \emph{kernel}, that is the trusted code base (together with the compiler and run-time of the programming language the system is written on). The kernel just implements the type-checker together with all the judgements required for type-checking, namely: well formation of contexts and environments, substitution, reduction, convertibility. The last three judgements are necessary because the type system has dependent types and therefore types need to be compared up to computation.

The kernel is ultimately responsible for guaranteeing that a proof built using an ITP is fully sound. However, in practice the user never interacts with the kernel and the remaining parts of the system do not depend on the behaviour of the kernel. Where the real intelligence of the system lies is instead the second layer, called \emph{elaborator} or \emph{refiner}~\cite{??,??,??}. An elaborator takes in input a \emph{partial term} and optionally a type, and returns the closest term similar to the input one such that the term has the expected type. Both the input and output terms are partial in the sense that subterms can be omitted and replaced with named holes to be later filled in or, in logic programming terminology, with \emph{existentially quantified metavariables}. For example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are quantified outside the term represents the $\lambda$-abstraction over $x$ of a type yet to be determined of the application of $f$ to two arguments, both to be yet determined and such that $x$ can happear free only in the first. Elaborating the term versus the expected type $\mathcal{N} \to \mathcal{B}$ will instantiate $T$ with $\mathcal{N}$ and verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it allows to interpret the terms that are input by the user, usually by means of a user-friendly syntax where information can be omitted, mathematical notation is used and abused, subtyping is assumed even if elements of the first type can only be coerced to elements of the second by inserting a function call in the elaborated term. A better elaborator therefore gives to the user the feeling of a more intelligent and user friendly system. On the other hand, via Curry-Howard, a partial term is a partial proof and an ITP is all about instantiating holes in partial proofs with new partial terms to advance in the proof. The elaborator is thus the mechanism that takes a partial sub-proof and makes it fit in the global one to progress on a particular proof obligation. In other words, all tactics of the ITP ultimately produce partial proof terms that are elaborated. The more advanced is the elaborator, the simpler the code implementing tactics can be.

CONTINUARE COSÌ:

1) dire cosa fa e a cosa serve l'elaborator
2) dire che usa metavariabili
3) dire che $\lambda$Prolog funziona male
4) passare a Constraint Programming

\begin{itemize}
\item Cheap implementation of PA via semi shallow embedding in LP
\item ELPI = LP + CHR (metalevel for suspended goals)
\item A (modular) kernel for CC+univ (with pluggable rewriting rules to get to CIC)
\item A conversion machine in CSP to cover CIC
\item A refiner in 5 lines (including universe inference)
\item Thin line between level and meta level (suspend = modes + ??)
\item Experiments with Matita
\end{itemize}


\section{Introduction}

\section{Related works}

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
