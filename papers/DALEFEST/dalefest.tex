\documentclass{easychair}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

% \title{Higher Order Constraint Logic Programming with Applications to Interactive Theorem Proving}
\title{The pragmatic construction of ELPI}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science and Engineering, University of Bologna \email{ferruccio.guidi@unibo.it} \and
  Department of Computer Scienc and Engineeringe, University of Bologna \email{claudio.sacerdoticoen@unibo.it} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{F. Guidi, C. Sacerdoti Coen, E. Tassi}
\titlerunning{The pragmatic construction of ELPI}

\begin{document}
\maketitle

\begin{abstract}
Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

In this paper we present a first proposal for an higher order constraint logic
programming language obtained as an extension of $\lambda$Prolog. We also present
applications to the implementation of an elaborator for the Calculus of
Inductive Constructions, i.e. the core of a modern interactive theorem prover
in the style of Coq, Agda or Matita.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

\paragraph{A pragmatic reconstruction of $\lambda$Prolog.}

In~\cite{jlp98} Belleannée et. alt. propose a pragmatic reconstruction
of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order
Logic Programming (HOLP) language introduced by Dale Miller and
Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the
minimal extension of Prolog that allows to program by structural
induction on $\lambda$-terms. According to their reconstruction, in
order to achieve that goal, Prolog needs to first be augmented with
$\lambda$-abstractions in the term syntax; then types are added to
drive full higher-order unification; then universal quantification in
goals and $\eta$-equivalence are required to express relations between
$\lambda$-abstractions and their bodies; and finally implication in
goals is needed to allow for structural induction definitions of
predicates.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can
easily encode all kind of binders without the need to take care of
binding representation, $\alpha$-conversion, renaming and
instantiation. Structural induction over syntax with binders is also
made trivial by combining universal quantification and implication
following the very same pattern used in Logical Frameworks like LF
(also called $\lambda$P). Indeed, LF, endowed with an HOLP semantics
like in ???(ELF? Beluga? MMT?) is just a sub-language of
$\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the
following two lines program to compute the simple type of a
$\lambda$-expression:

\begin{verbatim}
kind term type.
type app term -> term -> term.
type lam typ -> (term -> term) -> term.

type arr typ -> typ -> typ.

type of term -> typ -> o.

of (app M N) B :- of M (arr A B), of N A.
of (lam A F) (arr A B) :- pi x\ of x A => of (F x) B.
\end{verbatim}

\paragraph{$\lambda$Prolog for proof-checking.}

According to the Curry-Howard isomorphism, the program above can also
be interpreted as a proof-checker for minimal propositional logic. By
escalating the encoded $\lambda$-calculus to more complex terms and
types, it is possible to obtain a proof-checker for a much richer
logic, like the Calculus of Inductive Constructions that, up to some
variations, is the common logic shared by the interactive theorem
provers (ITPs) Coq~\cite{}, Lean~\cite{}, Matita~\cite{} and
Agda~\cite{}.

Indeed, all the ITPs mentioned above are implemented following
basically the same architecture. At the core of the system there is
the \emph{kernel}, that is the trusted code base (together with the
compiler and run-time of the programming language the system is
written on). The kernel just implements the type-checker together with
all the judgements required for type-checking, namely: well formation
of contexts and environments, substitution, reduction, convertibility.
The last three judgements are necessary because the type system has
dependent types and therefore types need to be compared up to
computation.

However, realistic implementations of kernels employ complex techniques to improve the performance, like reduction machines to speed up reduction and heuristics to completely avoid reduction during conversion when possible. Is $\lambda$Prolog suitable to implement such techniques? The question is not totally trivial because, for example, reduction machines typically rely on the representation of terms via De Brujin indexes.

We already gave a positive answer in~\cite{elpiLPAR} where we describe
ELPI, a fast $lambda$Prolog interpreter, and implement in
a type-checker for Landau's Grundlagen XXXXXXXX.
In Section~\ref{sec:kernel} we confirm the answer again by implementing a
kernel for a generic PTS that supports cumulativity between sorts as well. We
then instantiate the PTS to the one of Matita and we modularly add to the
kernel support for globally defined constants and form primitive inductive
types, following the variant of the Calculus of Inductive Constructions of
Matita. Finally, we embed ELPI in Matita and divert all calls to the kernel
written in $\lambda$Prolog in order to test it on the arithmetic library of
the system.

\paragraph{From proof-checking to interactive proving.}

The kernel is ultimately responsible for guaranteeing that a proof
built using an ITP is fully sound. However, in practice the user never
interacts with the kernel and the remaining parts of the system do not
depend on the behaviour of the kernel. Where the real intelligence of
the system lies is instead the second layer, called \emph{elaborator}
or \emph{refiner}~\cite{??,??,??}.

In Section~\ref{sec:problems} we pose again the same question we posed for the kernel. Is $\lambda$Prolog suitable as a very high level programming language to implement an elaborator? If not, what shall be added?
We will start by detailing what an elaborator is and why the state of the art is not satisfactory, which motivated at the very beginning our interest in using HOLP languages to implement elaborators. We then present the issues we faced when coding an elaborator in $\lambda$Prolog and how similar issues are dealt with in first order Prolog. We then present ELPI, our extension to $\lambda$Prolog.  Finally we present a kernel and a prototype refiner for CIC written in ELPI.

The final Section~\ref{sec:conclusions} contains comparisons with related work and future works.

\section{A modular kernel for CIC}\label{sec:kernel}

We now scale the type-checker for simply typed $\lambda$-calculus of Section~\ref{sec:introduction} to a type-checker for a generic Pure Type System (PTS) that also allows cumulativity between universes and dependent products that are covariant in the second argument. We will later instantiate it to obtain the predicative universal fragment of Luo's ECC~\cite{???}, that is a fragment of the Calculus of Inductive Constructions as well.

\subsection{A kernel for PTSs with cumulativity of universes}\label{sec:kernelpts}
\paragraph{Term representation and type-checking rules}
We start identifying syntactically types and terms, adding a new constructor for sorts of the PTS and refining \verb+arr+ to the dependently typed product.
\verb+@univ+ is a macro (in the syntax of the ELPI interpreter~\cite{??}) that will be instantiated in the file that implements the PTS, for example with the type of integers.

\begin{verbatim}
kind term type.
type sort @univ -> term.
type app term -> term -> term.
type lam term -> (term -> term) -> term.
type arr term -> (term -> term) -> term.
\end{verbatim}

We introduce the typing rules for the new constructors and we refine the
ones for abstraction and application. In particular, the types inferred and
expected for the argument of an application are meant to be compared up to $\beta$-reduction and cumulativity of universes. The $\verb+sub+$ predicate, that will be instantiated later, does the check. The \verb+match_sort+ and the \verb+match_arr+ predicates, also to be instantiated later, are used to check if the weak head normal form of the first argument is respectively a sort or a dependent product. In both cases the sub-terms are returned by the predicate. For example, \verb+match_arr (arr nat x \ x) A F+ is meant to instantiate \verb+A+ with \verb+nat+ and \verb+F+ with \verb+x \ x+. Finally, the \verb+succ+ and \verb+max+ predicates are meant to be later instantiated with the corresponding rules of a PTS.

\begin{verbatim}
type of term -> term -> o.

of (sort I) (sort J) :- succ I J.

of (app M N) BN :- of M TM, match_arr TM A1 Bx, of N A2, sub A2 A1, BN = Bx N.

of (lam A F) (arr A B) :-
  of A SA, conv SA (sort I),
  (pi x\ of x A => of (F x) (B x)),
  of (arr A B) _. % can't put this early, since B flexible

of (arr A Bx) (sort K) :-
  of A TA, (pi x\ of x A => of (Bx x) TB),
  match_sort TA I, match_sort TB J, max I J K.
\end{verbatim}

Note that the rules are syntax directed, i.e. with explicit calls to
\verb+sub+.  The on-paper presentation is more elegant and compact but 
uses in a fundamental way non determinism (hence results in too much
backtracking) by simply adding:

\begin{verbatim}
of A C :- of A B, sub B C .
\end{verbatim}

Moreover, integrating \verb+sub+ into \verb+of+ infers multiple types for a term. The syntax directed rules, instead, always infer just its most general type, in the sense of Luo~\cite{luo2}.

\paragraph{Reduction and conversion rules: a trivial but inefficient implementation. }\label{sec:inefficient}
To implement \verb+sub,match_sort,math_arr+, we first implement weak head
reduction in one step \verb+whd1+ and weak head normalization \verb+whd*+.
The former is trivial because we reuse the $\beta$-reduction of $\lambda$Prolog for capture avoiding substitution. The two former predicates are then defined by levels: to compare two terms, we first reduce both to their weak head normal forms via \verb+whd*+ and the compare the two heads. If they match, the comparison is called recursively on the subterms.

\begin{verbatim}
type whd* term -> term -> prop.
type whd1 term -> term -> prop.

whd1 (app M N) R :- whd* M (lam _ F), R = F N.
whd* A B :- whd1 A A1, !, whd* A1 B.
whd* X X.

type match_sort term -> @univ -> prop.
match_sort T I :- whd* T (sort I).

type match_arr term -> term -> (term -> term) -> prop.
match_arr T A F :- whd* T (arr A F).

type conv term -> term -> prop.
type conv-whd term -> term -> prop.

conv A B :- whd* A A1, whd* B B1, conv-whd A1 B1, !.

% fast path + axiom rule for sorts and bound variables
conv-whd X X :- !.
% congruence rules
conv-whd (app M1 N1) (app M2 N2) :- conv M1 M2, conv N1 N2.
conv-whd (lam A1 F1) (lam A2 F2) :- conv A1 A2, pi x\ conv (F1 x) (F2 x).
conv-whd (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ conv (F1 x) (F2 x).

type sub term -> term -> prop.
type sub-whd term -> term -> prop.

sub A B :- whd* A A1, whd* B B1, sub-whd A1 B1, !.

sub-whd A B :- conv A B, !.
sub-whd (sort I) (sort J) :- lt I J.
sub-whd (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
\end{verbatim}

%Both \verb+arr+ and \verb+sort+ are rigid (in head normal form), hence the need to extend the \verb+whd+ predicate.  The conversion test, in presence of
%a universe hierarchy needs to take into account variance and cumulativity.
%With little effort we define \verb+sub+, the entry point for the conversion 
%test, taking care of comparing sorts appropriately.

%where \verb+succ+ and \verb+max+ are the predicates governing sorts in
%PTS.  We now have the predicative, universal fragment of Luo's
%ECC~\cite{luo}, in 22 lines, wow!

\paragraph{Defining the PTS}\label{sec:eccpts}
To obtain the kernel, the programmer just needs to accumulate together the $\lambda$Prolog file that implements the type-checking rules, the one that deals with reduction and conversion and a final file containing the definition of a particular PTS. The latter file just needs to implement the \verb+succ,max+ and \verb+lt+ predicate over universe.

For example, the following lines define the predicative hierarchy of types of Luo's ECC, using the integer $i$ to represent the universe $Type_i$. The \verb+macro+ directive is again recognized by the ELPI interpreter only.
\begin{verbatim}
macro @univ :- int.
lt I J :- I < J.
succ  I J :- J is I + 1.
max N M M :- N =< M.
max N M N :- M < N.
\end{verbatim}

\paragraph{Reduction an conversion rules: an efficient implementation via a reduction machine for call-by-need}\label{sec:kernelmachine}

\input{kernel}

\section{The elaborator component of today's ITPs}\label{sec:problems}

An elaborator takes in input a \emph{partial term} and optionally a
type, and returns the closest term similar to the input one such that
the term has the expected type. Both the input and output terms are
partial in the sense that subterms can be omitted and replaced with
named holes to be later filled in or, in logic programming
terminology, with \emph{existentially quantified metavariables}. For
example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are
quantified outside the term represents the $\lambda$-abstraction over
$x$ of a type yet to be determined of the application of $f$ to two
arguments, both to be yet determined and such that $x$ can happear
free only in the first. Elaborating the term versus the expected type
$\mathbb{N} \to \mathbb{B}$ will instantiate $T$ with $\mathbb{N}$ and
verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it
allows to interpret the terms that are input by the user, usually by
means of a user-friendly syntax where information can be omitted,
mathematical notation is used and abused, subtyping is assumed even if
elements of the first type can only be coerced to elements of the
second by inserting a function call in the elaborated term. A better
elaborator therefore gives to the user the feeling of a more
intelligent and user friendly system. On the other hand, via
Curry-Howard, a partial term is a partial proof and an ITP is all
about instantiating holes in partial proofs with new partial terms to
advance in the proof. The elaborator is thus the mechanism that takes
a partial sub-proof and makes it fit in the global one to progress on
a particular proof obligation. In other words, all tactics of the ITP
ultimately produce partial proof terms that are elaborated. The more
advanced is the elaborator, the simpler the code implementing tactics
can be.

\subsection{Implementing an elaborator: state of the art}  %%%%%%

The elaborators of the majority of the provers mentioned above are all
implemented according to the same schema: the syntax of terms is augmented with
explicitly substituted existential variables
and the judgements of the kernel are re-implemented from scratch, generalizing
them to take in account metavariables and elaboration.

In particular one no more works with terms, but always with partial terms and
a metasenv: a data structure assigning to metavariable a typing judgement
and, eventually, an assignment.

All algorithms manipulating terms are extended to open
terms, in particular conversion (implemented by the 
\verb+sub+  and \verb+conv+ predicates in the running example)
becomes narrowing, i.e. higher order unification under a
mixed prefix~\cite{???} in presence of rewriting rules.
Unification requires metavariable instantiation, that is
implemented lazily for efficiency.  Type checking \verb+of+ is generalized to
elaboration, for example by replacing all calls to conversion with calls to
narrowing and by threading around the metasenv.

The approach is sub-optimal in many ways.  We here identify the main problems
in the state of the art.

\begin{description}
\item[programming platform]
Much of this new code has very little to do with the prover or the implemented
logic. In particular code that deals with binders ($\alpha$-conversion,
capture avoiding substitution, renaming) and code that deals with existentially
quantified metavariables (explicit substitution management, name capturing
instantiation).
\item[intricacy of algorithms]
Such code is far from being trivial, since it tackles problems that, like
higher order unification, are only semi-decidable. For efficiency reasons a lot
of incomplete heuristics are implemented to speed up the system and reduce
backtracking. The heuristics, however, are quite ad-hoc and they interact with
one another in unpredictable ways. Because they are hidden in the code, the
whole system becomes unpredictable to the user.
\item[code duplication]
Given such complexity in the elaborator, and the safety requirements of interactive provers, the kernel of the system is kept simple by making it unaware of partial terms.  As a consequence a lot of code is duplicated, and
the elaborator ends up being a very complicated \emph{twin brother of the
kernel} (Huet's terminology).
\item[twins' disagreement]
Worse than that, such twin components need to agree on ground terms.
Typically a proof term is incrementally built by the elaborator:
starting from a meta variable that has the type of
the conjecture the proof commands make progress by instantiating such
meta with partial terms.  Once there are no unresolved meta variables left,
the ground term is checked, again and in its totality, by the kernel.
Bugs in the elaborator are detected by the kernel, letting the user 
\item[extensibility of the elaborator]
Finally, the elaborator is the brain of the system, but it is oblivious of the
pragmatic ways to use the knowledge in the prover library, e.g.  to
automatically fill in gaps~\cite{mathcomponents}, to coerce data from one type
to another~\cite{coercivesubtyping} or to enrich data to resolve mathematical
abuse of notation~\cite{nonuniformunificationhints}. Therefore systems provide
ad-hoc extension points to increase the capabilities of the elaborator.  The
languages to write this code are typically high level, declarative, and
try to hide the intricaces of bound variables, metavariables, etc. to the user.
The global algorithm is therefore split in multiple languages, defying the hope
for static analysis and documentation of the elaborator.
\end{description}

\subsection{The proposed approach: semi shallow embedding}

The motivation of our research is to try to improve over the latter issues.  We
propose to identifying an high level (logic) programming language suitable for
the implementation of elaborators. In particular:
\begin{enumerate}
\item The programming language takes care of the representation of
	\emph{bindings and metavariables} in the spirit of semi
	shallow embedding~\cite{Dunchev:2016:IHH:2966268.2966272},
	solving the \textbf{programming platform} issue.
	It also improves on \textbf{extensibility} by allowing
	both the core implementors and the users to work on the same,
	high level code.
	Finally the \textbf{intricacy of elaboration} is be mitigated.
\item The programming language features a primitive and powerful
	notion of extensibility: programs are organized into clauses,
	and new clauses can be freely added.
	In this way the rules of the kernel are not be re-implemented in the
	elaborator. On the contrary they are extended to cover partial terms,
	solving the \textbf{code duplication} issue.
	Also, the \textbf{twins' disagreement} problem becomes less severe,
	since most code is shared.
	At the same time \textbf{extensibility of the elaborator}
	become less ad-hoc: the user simply declares new clauses.
\item Finally the programming language has a clean semantics, making it
	easy to prove that the extensions to the kernel
	only accept partial terms that are, once completed,
	well-typed according
	to the core set of rules of the kernel.  This completely
	solve the \textbf{twins' disagreement} problem, making it possible to
	merge the kernel and the elaborator.
\end{enumerate}

We envisage such language to be a logic one for two reasons:
first we hope to reuse or at least extend $\lambda$Prolog;
second we observe that another component of each ITP, the one
implementing proof commands, can take real advantage from a 
programming language where backtracking is built in, in
particular to write proof commands performing proof search.

The Higher order Abstract Syntax approach identifies the object
language binders and the meta language ones, obtaining $\alpha$-conversion
and $\beta$-reduction for free.  The semi shallow embedding we propose
\emph{identifies the metavariables of the object language with the 
meta variables of $\lambda$Prolog}.  Indeed such meta variables already
come with automatic instantiation and context management.

At a first sight, the runtime of $\lambda$Prolog seems to already provide
an implementation of the metasenv data structure, and all related operations.
Does this idea work out of the box? Unfortunately not quite.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\lambda$Prolog meets partial terms}

We already know from~\cite{jlp98} that $\lambda$Prolog is the minimal
extension of Prolog that allows to implement inductive predicates over
syntax containing binders. Does it work when applied to data that is
meant to constraint existentially quantified metavariables too?

Consider the $\lambda$-term \verb+(lam a\ P a)+ that encodes a
partial proof of $\forall A, A \to A$.
If we run the following query, the computation diverges:

\begin{verbatim}
?- of (lam T a\P a) (arr (sort I) a\ arr a _\ a)
\end{verbatim}

\subsection{Generative semantics and constraint declaration} %%%%%%%%%%%
\label{sec:delay}

Indeed, \verb+P+ is flexible and the \verb+of (app M N) BN+
rule applies indefinitely.

Indeed the \verb+of+ predicate inherits from Prolog a generative semantics:
when called recursively on a flexible input, it enumerates all
instances trying to find the ones that are well typed. Even when proof
search does not diverge, the behaviour obtained is not the one
expected from an elaborator for an \emph{interactive} prover: the
elaborator is not meant to fill in the term, unless the choice is
obliged. On the contrary, it should leave the meta variable not
instantiated and should \emph{remember} the need for
veryfing if the predicate holds later on, when the metavariable gets
instantiated. In the example above, type-checking \verb+(lam T a\P a)+ forces
the system to remember that term of type \verb+(arr a _\ a)+ has to be
provided, that in turn corresponds to the proof obligation
$A : o \vdash A \to A $.

This is not a problem specific of $\lambda$Prolog since Prolog behaves the same.
Nevertheless, all modern Prolog engines provide a \verb+var/1+ built-in to
test/guard predicates against flexible input, provide one/many variants of
\verb+delay/2+ to suspend a goal until the input becomes rigid, and provide
modes declarations to both statically/dynamically detect problematic goals
and to (semi)automatically delay (suspend) them.
These mechanisms, however, have never been standardized.

For example, by using the \verb+delay+ pack of SWI-Prolog~\cite{SWI}, the goal
\verb+plus(X,1,Y)+ is delayed until either \verb+X+ or \verb+Y+ are
instantiated.  Delayed goals can be thought as \emph{constraints} over the
metavariables occurring in them. In the example above, the programmer is
imposing a constraint between \verb+X+ and \verb+Y+.

At the light of that, we extend the ELPI $\lambda$Prolog interpreter
with the following directive:

\begin{verbatim}
delay (of X T) on X.
\end{verbatim}

The interpreter now delays goals of the form \verb+of X T+ whenever \verb+X+
is flexible.  As a result, instead of diverging, the running example
terminates leaving the following (suspended) goal unsolved:

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{verbatim}

Such suspended goal is to be seen as a typing constraint on
assignments to \verb+P+: when \verb+P+ gets instantiated by a
term \verb+t+, the goal \verb+(of (t x) (arr x _\ x))+ is resumed
hence \verb+(t x)+ is checked to be of type \verb+(arr x _\ x)+.
In turn such check can either:
\begin{itemize}
\item terminate successfully if \verb+t+ has the right type and is ground,
	i.e. \verb+t = x\lam x w\ w+.  This corresponds to a proof step
	that closed the goal, i.e. \verb+t+ is generated by a
	proof command ending a proof branch.
\item fail, rejecting the proposed assignment for \verb+P+, e.g. 
	\verb+t = x\lam x w\ x+.  This corresponds to an erroneous proof step,
	i.e. the user invoked command fails.
\item result in one or more new constraints if \verb+t+ is partial,
	e.g. \verb+t = x\lam x w\Q x w+.  This corresponds to progress
	in the interactive proof construction.
\end{itemize}

Extending $\lambda$Prolog with delay directive makes it possible to represent
the metasenv as the set of delayed typing constraints and, at the same time,
avoiding the generative semantics of predicates like \verb+of+.

\subsection{Meta theory and constraint propagation} %%%%%%%%%%%%%%%%%%%%%%%%%%

In many situations, the constraints that accumulate over time are
not independent.  Indeed sets of constraints can be rewritten in order to
simplify them, or detect their unsatisfiability and backtrack.

For example, if our meta variable \verb+P+ does not occur linearly, one 
can end up with two distinct constraints on it:

\begin{verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
of z (sort I) ?- of (P z) (arr (T z) y\ S z y)
\end{verbatim}

If the object language features uniqueness of typing, as it is the case
for CIC, one surely wants to get rid of the second constraint, and
force \verb+T = a\ a+, \verb+S = a\ b\ a+, and \verb+J = I+.
This way, when \verb+P+ gets instantiated only one goal is resumed and hence
its assigned term is checked only once.  Alternatively,
if the two typing constraints clash, one wants the elaborator to backtrack.

Prolog implementations that provide the notion of constraint typically come
with ad-hoc constraint solvers that take care of propagating specific
class of constraints (arithmetic, finite domain, etc\ldots), and are
then called \emph{Constraint Logic Programming} (CLP) languages.
For example, the set $\{0 \leq N \leq 4, 2 \leq N \leq 5\}$ can be rewritten
into $\{2 \leq N \leq 4\}$ without changing the set of ground solutions.

Most CLPs do not allow the user to define new constraints and propagation
rules in user space. A notable exception is the first order language
CHR~\cite{chr}. In CHR the user declares predicates and then gives a set of
rewriting rules of the form $S_1 \setminus S_2 ~|~ G \iff S_3$ whose
declarative semantics is that $\bigwedge S_1 \wedge G \Rightarrow (\bigwedge
S_2 \iff \bigwedge S_3)$ and whose operational semantics is to match the
current set of constraints against both $S_1$ and $S_2$ and, if the clause $G$
holds, replace the constraints in $S_2$ with the ones in $S_3$. All syntactic
components can be omitted: $G$ defaults ot \verb+true+ and the sets
$S_1,S_2,S_3$ to the emptyset. The $\iff$ symbol is also omitted when $S_3$
is, and $|$ is omitted when $G$ is. The semantics of the language is completed
by a strategy that fixes the order in which propagation rules are fired and in
what cases propagation rules can be backtracked.

Adding to $\lambda$Prolog a CHR like system enables
\begin{itemize}
\item forward reasoning during search. When propagation rules are theorems,
	then they fix well the proof theoretic semantics of $\lambda$Prolog
	and their application happens forward, while the SLD resolution engine
	goes backward
\item implement the optimizations given by the meta theory of the object
	language.  In the running example uniqueness of typing is key to keep
	the evarmap linear in the number of missing sub-proofs
\item easily turn checking into inference.  For example by replacing the
	predicates \verb+<,max,succ+ with generic order constraints
	\verb+leq,ltn+ one gets an elaborator that works with floating sorts,
	rather than fixed integers, and produces an acyclic graph of
	constraints linking these.
\end{itemize}

At the light of that we exend the ELPI $\lambda$Prolog interpreter
with a higher order constraint propagation language inspired by CHR.

The following propagation rule is valid code, and implements
uniqueness of typing:

\begin{verbatim}
constraint of {
  rule (G1 ?- of X T1) \ (G2 ?- of Y T2) > X ~ Y
       | (equiv G1 G2) <=> (G1 => conv T1 T2).
}
\end{verbatim}

Here \verb+G1 ?- of X T1+ is a constraint (dellayed goal) required
to be present, \verb+G2 ?- of Y T2+ is another one we want to remove.
The type of \verb+G1+ and \verb+G2+ is \verb+o list+, i.e.
it is a first class representation of the $\lambda$Prolog context
(that augments the original program).  
The guard \verb+equiv G1 G2+ checks that the two context are equivalent (i.e.
compare equal as sets).  \verb+G1 => conv T1 T2+ is
is sugar for \verb+H1 => ... => HN => conv T1 T2+ where
\verb+G1 = [H1, ..., HN]+, and is a new goal generated by the rule.
The alignment expressions, \verb+> X ~ Y+, asserts \verb+X+ and \verb+Y+
are the same meta variable and finds a bijection between the
eigenvariables (introduced by \verb+pi+ rules) in the two goals.

We say that CHR rules run the \emph{meta level}, i.e. they have access
to the syntax of goals and to their context.  Semantically, all flexible
terms are replace by fresh constants (we call this operation freezing).
The bijection described by the alignment expression is key to
manipulate in the same guard (or combine in the new goal) terms
living in different contexts.  The new goal is defrost before being
scheduled.  As an implementation detail, heads are \emph{matched} against the
constraints, and only when matching + alignment works, freezing takes place.


For example, if we take the two typing constraints on \verb+P+ above
and we freeze them we obtain

\begin{verbatim}
of x (sort I) ?- of (c1 x) (arr x _\ x)
of z (sort I) ?- of (c1 z) (arr (c2 z) y\ c3 z y)
\end{verbatim}

Now the alignment can easily verify that \verb+cP x+ and \verb+cP z+
are terms with the same head (constraints on the same meta variable)
and also find a bijection \verb+x -> z+.  The following query (guard)
is run

\begin{verbatim}
?- equiv [of x (sort c0)] [of x (sort c0)]
\end{verbatim}

and the goal \verb+of z (sort I) => conv (arr z _\ z) (arr (T z) y\ S z y)+

\subsubsection{Automatic alignment and L$_\lambda$}

% A rule operates on the subset of constraints that have \verb+NAME+
% as the head predicate.  Triggering of a rule happens as follows:
% \begin{enumerate}
% \item a new constraint $c$ is added to the global store (delay)
% \item all permutations of goals in the store of size $|heads|$
% 	containing $c$ are candidates, one by one matched against the
% 	heads. The ones that are successfully matched are $\bar{cs}$
% \item constraints are HO aligned (see below) according to the key and
% 	byjection $j$ is found
% \item all constraints are frozen, i.e. flexible terms are replaced by
% 	fresh constants and the alignment byjection is applied
% \item the guard is run in a $\lambda$Prolog runtime, at depth
% 	$max(\bar{cs})$
% \item to remove are removed
% \item the new goal is added to the low runtime at depth max with no
% 	augmented program (the user can use implication)
% \end{enumerate}

\label{sec:realchr}

Matching of HO is problematic in the general case, it is closely
related to equivariate unification that is known to be NP
hard~\cite{Cheney2004}.
In particular, matching the head H against the goal G
amounts to solving this equation:

\begin{math}
 \nabla x_1\ldots x_n, \mathcal{H} = \nabla y_1\ldots y_m, \mathcal{G}
\end{math}

CHR is already a very expensive language, its elegance is estimated
to cost a factor of 100.  If we make matching expensive, we are likely to
be out of any game.

Moreover we observed that in out use case all relevant names are
visible by the key of the delayed goal: in curry howard a meta variable
represents a sequent, and its proof (and type) can only variables in scope.

What we do is to forbid the nabla in the pattern, and as many as the
ones in the goal are implicit. This way such injection is trivial.
All patterns use distinct variables, so all injections are trivial.
The we align.  We support two alignments.  One intended to be used 
for constraints in llam, where all lists are distinct.  The cost is O(n1 * n2 * .. nk) where constraint i has ni variables (assuming the key sees them all).

Another alignment mode, the one that interests us the most, is the one where
we inject into the disjoint union.  Again the injection is trivial to compute.
This works for non llam, leave on the user full control but little support.
We later use this mode for the refiner.

introduce \verb+(?? K L)+ syntax

\subsection{Meta level: elegant but maybe overkill}

The CHR is expensive, freezing/unfreezing is expensive and in many cases
the multi-goal matching facility is not needed.  In particular
if we look back the CHR rule syntax
$S_1 \setminus S_2 ~|~ G \iff S_3$ and its
declarative semantics $\bigwedge S_1 \wedge G \Rightarrow (\bigwedge
S_2 \iff \bigwedge S_3)$ we remark that one could
weaken the double implication obtaining
 $\bigwedge S_1 \wedge G \Rightarrow (\bigwedge
S_2 \Leftarrow \bigwedge S_3)$.  One obtains a semantics that
is still useful, an intuitively says that $S_3$ is not the most general
constraint but is a still a sound one.  In other words it lets
us code heuristics, that is something the user surely wants to
do when extending the elaborator.  Our running example
is the following emblematic conversion (or better unification) problem

\begin{verbatim}
conv (app carrier_of R) integers
\end{verbatim}

In CIC a term can pack together types, terms and properties.  In the
example above \verb+R+ is a single constructor inductive (a record)
that packs together the group carrier, group operations and their properties.
In this setting one can clearly see that the generative mode of $\lambda$Prolog corresponds to proof search.  Of course the user may have already built,
in his library, the ring of integers (a theorem giving one of the many
solutions to the problem above).

\begin{verbatim}
delay (conv (app carrier_of X) Y) on X.
constraint conv {
  rule \ (conv (carrier_of X) integers) <=> (X = integer_group).
  rule \ (conv (carrier_of X) (intersection A B)) <=>
    (conv (carrier_of X1) A, conv (carrier_of X2) B,
     X = intersection_group X1 X2).
}
\end{verbatim}

The second rule is a recursive one that represent the theorem saying
that the intersection of 2 groups is a group.
Remark how little we use of CHR (one head, no alignment, no need to freeze).
Still, these simple rules are key to outstanding formalizations like
the odd order theorem~\cite{...}.  The efficiency of such rules is critical.

At the light of that we made the low level primitives that implement
constraint generation an propagation available  in the $\lambda$Prolog
directly.

The first primitive is called \verb+mode+ and lets one tag arguments of
a predicate as input ones.

\begin{verbatim}
mode (of i o).
\end{verbatim}

The result is that arguments marked as input are \emph{matched}
against the corresponding arguments in the goal.
This means the goal is not instantiated.

\begin{verbatim}
of (lam T F) T :- ...
of (app M N) T :- ...
of X T :- var X, delay (of X T) on X.
\end{verbatim}

Note that this is stronger than placing the following clause on top
and not using \verb+mode+

\begin{verbatim}
of X T :- var A, !, ...
\end{verbatim}

since hypothetical clauses may be placed by the runtime before this one,
defeating the purpose of the clause.

The syntax seen in \ref{sec:realchr} is also available, and lets one access
the explicit substitution of the flexible term.

\begin{verbatim}
conv (app carrier_of (?? as X)) T :- heuristic-assignment L T X.
\end{verbatim}

This ease the extension of the elaborator, providing a single entry point.
Also, by resorting to the more sophisticated syntax introduced 
in~\ref{recrealchr}, one can also extend (conversion) unification
outside the L$_\lambda$.  An emblematic example is

\begin{verbatim}
?- conv (?? F [nat_of_ord x]) T
\end{verbatim}

where \verb+x+ never occurs in \verb+T+ alone, only inside the
\verb+nat_of_ord+ context.

Such example is a PITA in Coq, since the mathcomp bigop library
mixes the sub type of ordinals.

\subsection{ELPI = $\lambda$Prolog + CHR}\label{sec:elpi}

In addition to the new programming constructs that deal with constraints, it
slightly differs from the version of $\lambda$Prolog implemented in Teyjus in a
few minor aspects:
\begin{itemize}
\item In ELPI all types and type and sort declarations can be omitted, whereas
	they are mandatory in Teyjus. Originally, and according
	to~\cite{jlp98}, types were necessary to implement Huet's algorithm for
	higher order unification. However, for several years now the
	unification algorithm of Teyjus only solves equations in the pattern
	fragment discovered by Miller~\cite{patternfrag}, which admits most
	general unifiers and reduce unification to a decidable problem. All
	other equations are automatically delayed by Teyjus to be fired only
	when the equation is instantiated to one in the pattern fragment. At
	the level of the implementation, the code of ELPI that implements this
	delay is shared with the one to delay arbitrary predicates.
\item The module system of Teyjus is not implemented. Only the
	\verb+accumulate+ directive is honored for backward compatibility and
	with a different semantics. In ELPI we provide instead explicit
	existentially quantified local constants whose scope spans over a set
	of program clauses. This mechanism gives in a simple way predicate and
	constructor hiding that are provided differently by the module system
	of Teyjus.
\item In a few corner cases, the parsing of an expression by Teyjus is
	influenced by the types. In particular, types are used to disambiguate
	between lists of elements and a singleton list of conjunctions. The
	syntax is disambiguated in a different way in ELPI.
\end{itemize}

Despite the differences above, we tried very hard to maintain backward
compatibility with Teyjus and its standard library. Indeed, we are able to
execute in ELPI all the code from Teyjus that we collected from the Web, up to
a very few minor changes to the source code, mostly due to 2 and 3.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Towards an elaborator for CIC}\label{sec:elaborator}
We are developing an elaborator for CIC as a modular extension of the kernel described in Sect.~\ref{sec:kernel}. The elaborator mimics as close as possible the behaviour of the one of Matita 0.99.1~\ref{xxx}, with the exception of the handling of universe constraints that follows Coq XXX~\ref{yyy}. For the time being, however, the elaborator can only instantiate metavariables that happear in the term, lacking the possibility of entirely modifying the input term, for example introducing coercions. To overcome this limitation, it is no longer possible to reuse the kernel type-checking rules are they are now: the type-checking judgement itself must give in output a new term, that is the elaboration of the term in input. All the rules of the kernel would just copy the input term in output verbatim, leaving to additional rules in the elaborator the introduction of coercions.

In place of following Matita and Coq algorithm, an alternative very promising choice would have been to mimic the elaborator described in~\ref{zzz} and already presented via typing rules that yield a set of higher order unification constraints to be later solved. In the future, the choice to be closer to Matita will allow us to easily compare the performances of the elaborator written in ELPI with the one of Matita written in OCaml, in order to further optimize the ELPI interpreter.

To implement the elaborator, the following steps need to be taken at the beginning of a file that accumulates at the end the code of the kernel and of the file implementing the PTS rules.
\begin{itemize}
\item The accumulated code for the PTS is not the ones accumulated by the kernel that just verifies the typing inequations, but the one based on the first order CHR rules presented in Section~\ref{chruniverses}.
\item The following predicates defined by structural recursion over a term
 must be given a suitable mode to prevent the generative behaviour:
 AAA (type checking), BBB (reduction), CCC (conversion), plus all derived
 predicates like DDD that infers a type and checks that the inferred type is
 a sort.
\item A line is inserted to delay type checking a flexible term. A suitable set of propagation rules in CHR style can be added to capture unicity of typing.
\item Conversion when at least one of the arguments is a flexible term amounts to higher order narrowing. Instead of delaying the goal, an heuristic already used in Matita immediately rewrites the constraint by solving the unification problem. The heuristic, in the case $M~t = s$ tries to instantiates $M$ with
$\lambda x. s[x/t]$ where the latter operation replaces with $x$ subterms of $s$ that unify with $t$. In practice, in Huet's terminology it always prefer projection to mimic. When $M$ is applied to multiple $t$s or to flexible $t$s, further heuristics are used to pick one of the solution. These heuristics are clearly incomplete, discarding all solutions but one and sometimes failing to find a solution when it exists. However, they are more general than the ones implemented in Coq and, from a practical perspective, they guess most of the time the unifier that is expected by the user when interactive with the ITP.
\item In place of delaying reducing a flexible term, the term is returned in the current state, weakening the semantics of the the BBB predicate. In practice, this is not a problem because the result of reduction is then always passed to the conversion predicate.
\end{itemize}

\begin{verbatim}
mode (t+step i o).
mode (has+sort i o).
mode (conv+whnf i i i i i).
t+step (?? as K) T :- !, $constraint (t+step K T) K.
has+sort (?? as S) T :- !, $constraint (has+sort S T) S.

conv+whnf A B C D E :- $print "##" (conv+whnf A B C D E), fail.

% Bug1: ci possono essere let-in sulle var in Ti
% Bug2: potrebbe servire un <= se il secondo è una sorta
%conv+whnf (?? as T1) [] _M T2 L2 :- !, $llam_unif T1 {zip T2 L2}.
conv+whnf T1 L1 _M (?? as V) L2 :- !, bind-list L2 {zip T1 L1} V.

conv+whnf (?? as T1) L1 M T2 L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T1.
conv+whnf T1 L1 M (?? as T2) L2 :- !, $constraint (conv+whnf T1 L1 M T2 L2) T2.

constraint t+step has+sort conv+whnf r+step {
}

%%% library
mode (zip i i o).
zip HD [] HD :- !.
zip (appl HD TL) Args (appl HD TLArgs) :- !, append TL Args TLArgs.
zip HD Args (appl HD Args).

is_rigid C :- $is_name C. % ; C = const _ ; C = indt _ ; C = indc _

mode (bind-list i i o).
bind-list [] T T' :- copy T T'.
bind-list [ ?? |VS] T R :- !, pi x\ bind-list VS T (R x).
bind-list [appl C AS | VS] T R :- is_rigid C, !,
  pi x\ (pi L\ copy (app[C|L]) x :- conv+args L AS) => bind-list VS T (R x).
bind-list [C|VS] T R :- is_rigid C, !,
  pi x\ copy C x => bind-list VS T (R x).

mode (copy i o).
copy A B :- $print "AA" (copy A B), fail.
copy X Y :- r+step X Y _. % Bang or not???
copy X Y :- $is_name X, X = Y, !.
copy X Y :- $is_name X, r+step X T _, !, copy T Y.
copy (sort _ as C) C :- !.
copy (appl X1 L1) (appl X2 L2) :- copy X1 X2, $print "UU", map copy L1 L2.
\end{verbatim}

\paragraph{Inference of universes via constraints}

XXX DA RISCRIVERE XXX

Implementing in $\lambda$Prolog the part of the elaborator for ECC that deals
with universe constraints is much harder and less elegant than the previous
approach based on CHR. Indeed, the only solution is to keep an explicit list
of constraints that become a third argument of the type-checking judgement and
that needs to be threaded around the program, polluting the code and not
allowing to reuse the type-checking code for the elaborator. Moreover, every
time an additional constraint is added to the list, some special predicate to
fire the propagation rules according to some strategy needs to be called, de
facto forcing the user to implement the boilerplate code for constraint
propagation.


For example, the rules for type-checking ECC call the predicates \verb+<,max,succ+ over integers. The corresponding elaborator may be faced with sorts like \verb+sort M+ where \verb+M+ is a metavariable. The elaborator is asked to compute a suitable assignment to the metavariables that respects all constraints. Without breaking consistency, the successor predicate can be relaxed to a strictly less than, the max to a generic upper bound and the set of integers to an unspecified partial order, de facto replacing the predicates \verb+<,max,succ+ with generic predicates \verb+leq,ltn+ (lax and strict inequality over an unspecified set). Finally, the two predicates, once turned into constraints, admit the following complete set of CHR propagation rules able to detect strict cicles that correspond to unsatisfiability of the constraints.

\label{chruniverses}
\begin{verbatim}
succ I J :- ltn I J.
I < J :- ltn I J.
max I J K :- leq I K, leq J K.

constraint leq ltn {
  % incompatibility
  rule (leq X Y) (ltn Y X) <=> false.
  rule (ltn X Y) (ltn Y X) <=> false.
  rule (ltn X X) <=> false.
  
  % reflexivity
  rule \ (leq X X).

  % antisymmetry
  rule (leq X Y) \ (leq Y X) <=> (Y = X).

  % transitivity
  rule (leq X Y) (leq Y Z) <=> (leq X Z).
  rule (leq X Y) (ltn Y Z) <=> (ltn X Z).
  rule (ltn X Y) (leq Y Z) <=> (ltn X Z).
  rule (ltn X Y) (ltn Y Z) <=> (ltn X Z).

  % idempotence
  rule (leq X Y) \ (leq X Y).
  rule (ltn X Y) \ (ltn X Y).
}
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and related works}\label{sec:conclusion}
Related works:
\begin{enumerate}
\item Costrutti tipo delay
\item Logical frameworks (Dedukti, MMT, etc.) e brrr Isabelle
\item Da qualche parte: il pre-print by Abel sull'elaborator basato su
 constraint e lean che implementa un constraint solver di qualche tipo
\end{enumerate}

Future works:
\begin{enumerate}
\item Parlare di Abella
\end{enumerate}

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
