\documentclass{easychair}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

\usepackage{fancyvrb}
\fvset{baselinestretch=0.75}

\title{Implementing Type Theory in Higher Order Constraint Logic Programming}
\author{Ferruccio Guidi \and Claudio Sacerdoti Coen \and Enrico Tassi}
\institute{
  Department of Computer Science and Engineering, University of Bologna \email{ferruccio.guidi@unibo.it} \and
  Department of Computer Scienc and Engineeringe, University of Bologna \email{claudio.sacerdoticoen@unibo.it} \and
  Inria Sophia-Antipolis, \email{Enrico.Tassi@inria.fr}}

\authorrunning{F. Guidi, C. Sacerdoti Coen, E. Tassi}
\titlerunning{Implementing Type Theory in HOCLP}

\begin{document}
\maketitle

\begin{abstract}
In this paper we are interested in high-level programming languages to implement the core components of an interactive theorem prover for a dependently typed language: the kernel --- responsible for type-checking closed terms --- and the elaborator --- that manipulates terms with holes or, equivalently, partial proof terms.

In the first part of the paper we confirm that $\lambda$Prolog, the language developed by Miller and Nadathur since the 80s, is extremely suitable for implementing the kernel, even when efficient techniques like reduction machines are employed.

In the second part of the paper we turn our attention to the elaborator and we observe that the eager generative semantics inherited by Prolog makes it impossible to reason by induction over terms containing metavariables. We also conclude that the minimal extension to $\lambda$Prolog that allows to do so is the possibility to delay inductive predicates over flexible terms, turning them into (set of) constraints to be propagated according to user provided constraint propagation rules.

Therefore we propose extensions to $\lambda$Prolog to declare and manipulate
higher order constraints, and we implement the proposed extensions in the ELPI
system.  Our test case is the implementation of an elaborator for a type theory
as a CLP extension to a kernel written in plain $\lambda$Prolog.

%Therefore we propose our syntax and implementation of the first Higher Order Constraint Logic Programming (HOCLP) language and we apply it to our first major test case: the implementation of an elaborator for a dependently typed theory as a CLP extension to a kernel written in plain $\lambda$Prolog.


%Higher Order Logic Programming and its main incarnation, which is $\lambda$Prolog, were introduced by Nadathur and Miller in the 80s. Like logical frameworks, it allows to write type-checkers extremely naturally, just encoding the derivation rules in an almost Verbatim way. In particular, thanks to a shallow encoding of binders, the implementor is relieved from dealing with binders, capture avoiding substitution, alpha-conversion and the like. By delegating them to the language, it can focus on the logic of the program he is implementing and he can hope in future optimizations of the compiler to improve the efficiency of code. Thanks to the Curry-Howard isomorphism, proof-checkers --- e.g. for dependently typed languages --- can also be encoded in the same way.

%To implement an interactive theorem prover following the Curry-Howard correspondence, the programmer basically has to generalize the type-checker by allowing incomplete terms whose holes are represented by existentially quantified metavariables. Adopting a deep encoding, the user is forced to implement non-capture-avoiding instantiation, explicit substitutions and higher order unification under a mixed prefix. Therefore, we would prefer a shallow encoding of metavariables, where the existentially quantified metavariables of the logic programming language directly represent metavariables of the encoded typed calculus, and the previously mentioned operations can be largely inherited from the metalevel.

%Such deep encodings do not work for $\lambda$Prolog and, more generally, for all higher order logic programming languages and logical frameworks. Indeed, the standard operational semantics of $\lambda$Prolog is generative, i.e. when a predicate like well-typedness inspects a flexible term, all possible instantiations of the latter are tried blindly. On the contrary, in an interactive theorem prover a predicate over a flexible term is supposed to be delayed and turned into a constraint to be solved later. When constraints accumulate, they can be simplified and propagated exactly in the spirit of constraint programming.

%In this paper we present a first proposal for an higher order constraint logic programming language obtained as an extension of $\lambda$Prolog. We also present applications to the implementation of an elaborator for the Calculus of Inductive Constructions, i.e. the core of a modern interactive theorem prover in the style of Coq, Agda or Matita.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

\paragraph{A pragmatic reconstruction of $\lambda$Prolog.}

In~\cite{jlp98} BelleannÃ©e et. al. propose a pragmatic reconstruction
of $\lambda$Prolog~\cite{lambdap1,lambdap2,lambdap3}, the Higher Order
Logic Programming (HOLP) language introduced by Dale Miller and
Gopalan Nadathur in the '80s.
Their conclusion is that $\lambda$Prolog can be characterized as the
minimal extension of Prolog that allows to program by structural
induction on $\lambda$-terms. According to their reconstruction, in
order to achieve that goal, Prolog needs to be first augmented with
$\lambda$-abstractions in the term syntax; then types are added to
drive full higher-order unification; then universal quantification in
goals and $\eta$-equivalence are required to express relations between
$\lambda$-abstractions and their bodies; and finally implication in
goals is needed to allow for definitions of predicates by structural induction.

By means of $\lambda$-abstractions in terms, $\lambda$Prolog can
easily encode all kind of binders without the need to take care of
binding representation, $\alpha$-conversion, renaming and
instantiation. Structural induction over syntax with binders is also
made trivial by combining universal quantification and implication
following the very same pattern used in Logical Frameworks like LF
(also called $\lambda$P). Indeed, LF endowed with an HOLP semantics,
% like \textcolor{red}{in ???(ELF? Beluga? MMT?)} 
is just a sub-language of
$\lambda$Prolog.

The ``hello world'' example of $\lambda$Prolog is therefore the
following two lines program to compute the simple type of a
$\lambda$-expression:

\begin{Verbatim}
kind term,typ type.             type arr typ -> typ -> typ.
type app term -> term -> term.  type lam typ -> (term -> term) -> term.

type of term -> typ -> o.
of (app M N) B :- of M (arr A B), of N A.
of (lam A F) (arr A B) :- pi x\ of x A => of (F x) B.
\end{Verbatim}

\paragraph{$\lambda$Prolog for proof-checking.}

According to the Curry-Howard isomorphism, the program above can also
be interpreted as a proof-checker for minimal propositional logic. By
escalating the encoded $\lambda$-calculus to more complex terms and
types, it is possible to obtain a proof-checker for a richer
logic, like the Calculus of Inductive Constructions (CIC) that, up to some
variations, is the common logic shared by the interactive theorem
provers (ITPs) Coq, Matita and Lean. %, Matita and Agda.

All the ITPs mentioned above are implemented following
basically the same architecture. At the core of the system there is
the \emph{kernel}, that is the trusted code base (together with the
compiler and run-time of the programming language the system is
written on). The kernel just implements the type-checker together with
all the judgements required for type-checking, namely: well-formation
of contexts and environments, substitution, reduction, convertibility.
The last three judgements are necessary because the type system has
dependent types and therefore types need to be compared up to
computation.

Realistic implementations of kernels employ complex techniques to improve the performance, like reduction machines to speed up reduction and heuristics to completely avoid reduction during conversion when possible. Is $\lambda$Prolog suitable to implement such techniques? The question is not totally trivial because, for example, reduction machines typically rely on the representation of terms via De Bruijn indexes.

We already gave a positive answer in~\cite{elpiLPAR} where we describe
ELPI, a fast $\lambda$Prolog interpreter, and implement 
a type-checker for a dependently type lambda calculus.
ELPI is able to run such type checker on the Automath's proof terms of
Landau's Grundlagen reasonably fast.
In Section~\ref{sec:kernel} we confirm the answer again by implementing a
kernel for a generic Pure Type System (PTS) that supports cumulativity between
sorts. We then instantiate the PTS to the one of Matita and we
modularly add to the kernel support for globally defined constants and
primitive inductive types, following the variant of CIC of Matita. Finally, we embed ELPI in Matita and divert all calls
to the $\lambda$Prolog kernel in order to test it on the arithmetic
library of the system.

\paragraph{From proof-checking to interactive proving.}

The kernel is ultimately responsible for guaranteeing that a proof
built using an ITP is fully sound. However, in practice the user never
interacts with the kernel and the remaining parts of the system do not
depend on the behaviour of the kernel. Where the real intelligence of
the system lies is instead in the second layer, called \emph{elaborator}
or \emph{refiner}~\cite{Lean,bidir}.
In Section~\ref{sec:elaborator} we recall what an elaborator does and we explain why the state of the art is not satisfactory, which motivated at the very beginning our interest in using HOLP languages to implement elaborators

In Section~\ref{sec:problems} we pose again the same question we posed for the kernel. Is $\lambda$Prolog suitable as a very high-level programming language to implement an elaborator? If not, what shall be added? We conclude that $\lambda$Prolog is not suitable and, combining and extending existing ideas in the literature, we introduce the first Constraint Programming extension of $\lambda$Prolog. We also implemented the language extensions in the ELPI system.

In Section~\ref{sec:cicelaborator} we work towards an implementation of an elaborator for CIC written in ELPI, obtained extending modularly a kernel presented in Section~\ref{sec:kernel}. The implementation is the first major use case for the language.
The final Section~\ref{sec:conclusion} contains comparisons with related work and future works.

\section{A modular kernel for CIC}\label{sec:kernel}

We now scale the type-checker for simply typed $\lambda$-calculus of Section~\ref{sec:introduction} to a type-checker for a generic PTS that also allows cumulativity between universes and dependent products that are covariant in the second argument. Then we instantiate it to obtain the predicative universal fragment of Luo's ECC~\cite{ecc} and the PTS of CIC, and then we modularly extend the type-checker to the whole CIC by adding inductive types as well.

\paragraph{Term representation and type-checking rules}
We start identifying syntactically types and terms, adding a new constructor for sorts of the PTS and by refining \verb+arr+ to the dependently typed product.
\verb+@univ+ is a macro (in the syntax of the ELPI interpreter~\cite{elpiLPAR}) that will be instantiated in the file that implements the PTS, for example with the type of integers.

\begin{Verbatim}
kind term type.
type sort @univ -> term.        type arr term -> (term -> term) -> term.
type app term -> term -> term.  type lam term -> (term -> term) -> term.
\end{Verbatim}

We introduce the typing rules for the new constructors and we refine the
ones for abstraction and application. In particular, the types inferred and
expected for the argument of an application are meant to be compared up to $\beta$-reduction and cumulativity of universes. The $\verb+sub+$ predicate, that will be instantiated later, implements this check. The \verb+match_sort+ and the \verb+match_arr+ predicates, also to be instantiated later, are used to check if the weak head normal form of the first argument is respectively a sort or a dependent product. In both cases the sub-terms are returned by the predicate. For example, \verb+match_arr (arr nat x \ x) A F+ is meant to instantiate \verb+A+ with \verb+nat+ and \verb+F+ with \verb+x \ x+. Finally, the \verb+succ+ and \verb+max+ predicates are meant to be later instantiated with the corresponding rules of a PTS.

\begin{Verbatim}
type of term -> term -> o.

of (sort I) (sort J) :- succ I J.

of (app M N) BN :- of M TM, match_arr TM A1 Bx, of N A2, sub A2 A1, BN = Bx N.

of (lam A F) (arr A B) :-
 of A SA, match_sort SA (sort _),
 (pi x\ of x A => of (F x) (B x)), of (arr A B) _.

of (arr A Bx) (sort K) :-
 of A TA, (pi x\ of x A => of (Bx x) TB),
 match_sort TA I, match_sort TB J, max I J K.
\end{Verbatim}

% \textcolor{red}{<BEGIN>}
The rules above have the merit of being syntax-directed, never requiring backtracking and always inferring the most general type, in the sense of Luo~\cite{ecc}.
% , i.e. with explicit calls to
% \verb+sub+.  The more widesprad presentation on-paper presentation is more elegant and compact but 
% uses in a fundamental way non determinism (hence results in too much
% backtracking) by simply adding:
% 
% \begin{Verbatim}
% of A C :- of A B, sub B C .
% \end{Verbatim}
% 
% Moreover, integrating \verb+sub+ into \verb+of+ infers multiple types for a term. The syntax directed rules, instead, always infer just its most general type, in the sense of Luo~\cite{ecc}.
% 
% \textcolor{red}{<END>
% CSC: NON MI PIACE TANTO IL TESTO. IN ALTERNATIVA CITARE ARTICOLO SU SYNTAX DIRECTED RULES + QUELLO DI LUO}

\paragraph{Reduction and conversion rules: a trivial but inefficient implementation. }\label{sec:inefficient}
To implement \verb+sub+, \verb+match_sort+ and
\verb+math_arr+ we first implement weak head
reduction in one step, predicate \verb+whd1+, and then 
its transitive closure \verb+whd*+.
The former is trivial because we reuse the $\beta$-reduction of $\lambda$Prolog for capture avoiding substitution. The predicates \verb+sub+ is then defined by levels: to compare two terms, we reduce both to their weak head normal forms via \verb+whd*+ and then compare the two heads. If they match, the comparison is called recursively on the subterms.

\begin{Verbatim}
type whd1,whd* term -> term -> prop.

whd1 (app M N) R :- whd* M (lam _ F), R = F N.
whd* A B :- whd1 A A1, !, whd* A1 B.
whd* X X.

type match_sort term -> @univ -> prop.
match_sort T I :- whd* T (sort I).

type match_arr term -> term -> (term -> term) -> prop.
match_arr T A F :- whd* T (arr A F).

type conv,conv-whd term -> term -> prop.

conv A B :- whd* A A1, whd* B B1, conv-whd A1 B1, !.

% fast path + axiom rule for sorts and bound variables
conv-whd X X :- !.
% congruence rules
conv-whd (app M1 N1) (app M2 N2) :- conv M1 M2, conv N1 N2.
conv-whd (lam A1 F1) (lam A2 F2) :- conv A1 A2, pi x\ conv (F1 x) (F2 x).
conv-whd (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ conv (F1 x) (F2 x).

type sub,sub-whd term -> term -> prop.

sub A B :- whd* A A1, whd* B B1, sub-whd A1 B1, !.

sub-whd A B :- conv A B, !.
sub-whd (sort I) (sort J) :- lt I J.
sub-whd (arr A1 F1) (arr A2 F2) :- conv A1 A2, pi x\ sub (F1 x) (F2 x).
\end{Verbatim}

%Both \verb+arr+ and \verb+sort+ are rigid (in head normal form), hence the need to extend the \verb+whd+ predicate.  The conversion test, in presence of
%a universe hierarchy needs to take into account variance and cumulativity.
%With little effort we define \verb+sub+, the entry point for the conversion 
%test, taking care of comparing sorts appropriately.

%where \verb+succ+ and \verb+max+ are the predicates governing sorts in
%PTS.  We now have the predicative, universal fragment of Luo's
%ECC~\cite{luo}, in 22 lines, wow!

\paragraph{Defining the PTS}\label{sec:eccpts}
To obtain the kernel, the programmer just needs to accumulate together the $\lambda$Prolog file that implements the type-checking rules, the one that deals with reduction and conversion, and a final file containing the definition of a particular PTS. The latter file just needs to implement the \verb+succ+, \verb+max+ and \verb+lt+ predicates over universes.

For example, the following lines define the predicative hierarchy of Luo's ECC,
using the integer $i$ to represent the universe Type$_i$. The \verb+macro+
directive is recognized by the ELPI interpreter that transparently replaces all
occurrences of \verb+@univ+ by \verb+int+.
\begin{Verbatim}
macro @univ :- int.
max N M M :- N =< M.     lt I J :- I < J.
max N M N :- M < N.      succ  I J :- J is I + 1.
\end{Verbatim}

\paragraph{Reduction and conversion rules: an efficient implementation via a reduction machine for call-by-need.}\label{sec:kernelmachine}

\input{kernel}

\section{The elaborator component of today's ITPs}\label{sec:elaborator}

An elaborator takes in input a \emph{partial term} and optionally a
type, and returns the closest term similar to the input one such that
the term has the expected type. Both the input and output terms are
partial in the sense that subterms can be omitted and replaced with
named holes to be later filled in or, in logic programming
terminology, with \emph{existentially quantified metavariables}. For
example, the partial term $\lambda x: T. f~(X~x)~Y$ where $T,X,Y$ are
quantified outside the term represents the $\lambda$-abstraction over
$x$ of a type yet to be determined of the application of $f$ to two
arguments, both to be yet determined and such that $x$ can happear
free only in the first. Elaborating the term versus the expected type
$\mathbb{N} \to \mathbb{B}$ will instantiate $T$ with $\mathbb{N}$ and
verify that $f$ is a binary function returning a boolean.

The importance of the elaborator is twofold. On the first hand, it
allows to interpret the terms that are input by the user, usually by
means of a user-friendly syntax where information can be omitted,
mathematical notation is used and abused, subtyping is assumed even if
elements of the first type can only be coerced to elements of the
second by inserting a function call in the elaborated term. A better
elaborator therefore gives to the user the feeling of a more
intelligent and user friendly system. On the other hand, via
Curry-Howard, a partial term is a partial proof and an ITP is all
about instantiating holes in partial proofs with new partial terms to
advance in the proof. The elaborator is thus the mechanism that takes
a partial sub-proof and makes it fit in the global one to progress on
a particular proof obligation. In other words, all tactics of the ITP
ultimately produce partial proof terms that are elaborated. The more
advanced is the elaborator, the simpler the code implementing tactics
can be.

%\subsection{Implementing an elaborator: state of the art}  %%%%%%
\paragraph{Implementing an elaborator: state of the art.}  %%%%%%

The elaborators of the majority of the provers mentioned above are all
implemented according to the same schema: the syntax of terms is augmented with
explicitly substituted existential variables
and the judgements of the kernel are re-implemented from scratch, generalizing
them to take into account metavariables and elaboration.

In particular the elaborator code works on two new data types: one for partial terms and one, called \emph{metasenv}, that assigns to metavariables a typing judgement (a sequent) and, eventually, an assignment. E.g. a metasenv containing \verb+x:nat, y:bool +$\vdash$\verb+ X x y : nat+ declares \verb+X+ to be an hole to be instantiated only with terms of type \verb+nat+ in the context \verb+x:nat, y:bool+.

All algorithms manipulating terms are extended to partial
terms. For example, conversion (the \verb+sub+ predicates in the running
example) becomes narrowing, i.e. higher order unification under a
mixed prefix~\cite{mixedprefix} in presence of rewriting rules (and cumulativity of universes as subtyping).
Unification requires metavariable instantiation, that is
implemented lazily for efficiency.  Type checking \verb+of+ is generalized to
elaboration, for example by replacing all calls to conversion with calls to
narrowing and by threading around the metasenv.

The state-of-the-art approach is sub-optimal in many ways:

\begin{description}
\item[programming platform weakness]
Much of this new code has very little to do with the prover or the implemented
logic: in particular code that deals with binders ($\alpha$-conversion,
capture avoiding substitution, renaming) and code that deals with existentially
quantified metavariables (explicit substitution management, name capturing
instantiation).
\item[intricacy of algorithms]
Such code is far from being trivial, since it tackles problems that, like
higher order unification, are only semi-decidable. For efficiency reasons a lot
of incomplete heuristics are implemented to speed up the system and reduce
backtracking. The heuristics are quite ad-hoc and they interact with
one another in unpredictable ways. Because they are hidden in the code, the
whole system becomes unpredictable to the user.
\item[code duplication]
Given such complexity in the elaborator, and the safety requirements of interactive provers, the kernel of the system is kept simple by making it unaware of partial terms.  As a consequence a lot of code is duplicated, and
the elaborator ends up being a very complicated \emph{twin brother of the
kernel} (Huet's terminology).
\item[twins' disagreement]
Worse than that, such twin components need to agree on ground terms.
Typically a proof term is incrementally built by the elaborator:
starting from a metavariable that has the type of
the conjecture the proof commands make progress by instantiating it
with partial terms.  Once there are no unresolved metavariables left,
the ground term is checked, again and in its totality, by the kernel.
%Bugs in the elaborator are detected by the kernel, letting the user 
%\textcolor{red}{??? SENTENZA INCOMPLETA}
\item[extensibility of the elaborator]
Finally, the elaborator is the brain of the system, but it is oblivious of the
pragmatic ways to use the knowledge in the prover library, e.g.  to
automatically fill in gaps~\cite{oothm,unificationhints}, to coerce data from one type
to another~\cite{coercivesubtyping} or to enrich data to resolve mathematical
abuse of notation~\cite{nonuniformcoercions}. Therefore systems provide
ad-hoc extension points to increase the capabilities of the elaborator.  The
languages to write this code are typically high-level, declarative, and
try to hide the intricacies of bound variables, metavariables, etc. to the user.
The global algorithm is therefore split in multiple languages, defying the hope
for static analysis and documentation of the elaborator.
\end{description}

%\subsection{The proposed approach: semi shallow embedding}
\paragraph{The proposed approach: the semi-shallow embedding.}

The motivation of our research is to improve over the latter issues by
identifying an high-level (logic) programming language suitable for
the implementation of elaborators. In particular:
\begin{enumerate}
\item The programming language takes care of the representation of
	\emph{bindings and metavariables} in the spirit of semi-shallow embedding~\cite{holsuperlight},
	solving the \textbf{programming platform weakness} issue.
	It also improves on \textbf{extensibility} by allowing
	both the core implementors and the users to work on the same,
	high-level code.
	Finally the \textbf{intricacy of elaboration} is mitigated.
\item The programming language features a primitive and powerful
	notion of extensibility: programs are organized into clauses,
	and new clauses can be freely added.
	In this way the rules of the kernel need not to be re-implemented in the
	elaborator. On the contrary, they are extended to cover partial terms,
	solving the \textbf{code duplication} issue.
	Also, the \textbf{twins' disagreement} problem becomes less severe,
	since most code is shared, and
	\textbf{extensibility of the elaborator}
	become less ad-hoc: the user simply declares new clauses.
\item Finally the programming language has a clean semantics, making it
	easy to prove that the extensions to the kernel
	only accept partial terms that are, once completed,
	well-typed according
	to the core set of rules of the kernel.  This completely
	solves the \textbf{twins' disagreement} problem, making it possible to
	merge the kernel and the elaborator.
\end{enumerate}

We envisage such language to be a logic one for two reasons:
first we hope to reuse or at least extend $\lambda$Prolog;
second we observe that another component of each ITP, the one
implementing proof commands, can take real advantage from a 
programming language where backtracking is built-in, in
particular to write proof commands performing proof search.

The Higher Order Abstract Syntax approach identifies the object
language binders and the meta-language ones, obtaining $\alpha$-conversion
and $\beta$-reduction for free.  The semi-shallow embedding~\cite{holsuperlight} we expect to use in our language
\emph{identifies the metavariables of the object language with the 
metavariables of $\lambda$Prolog}. Such metavariables already
come in $\lambda$Prolog with automatic instantiation, context management and forms of higher order unification.

At a first sight, the runtime of $\lambda$Prolog seems to already provide
metavariables and all related operations required for the semi-shallow embedding. So does the technique work out of the box? Unfortunately not quite, as we will see in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\lambda$Prolog meets partial terms}\label{sec:problems}

We already know from~\cite{jlp98} that $\lambda$Prolog is the minimal
extension of Prolog that allows to implement inductive predicates over
syntax containing binders. Does it work when applied to data that is
meant to contain existentially quantified metavariables too?

% \subsection{From generative semantics to constraints} %%%%%%%%%%%
\paragraph{From generative semantics to constraints.} %%%%%%%%%%%
\label{sec:delay}

Consider the $\lambda$-term \verb+(lam a\ P a)+ that encodes a
partial proof of $\forall A, A \Rightarrow A$.
If we run the following query, the computation diverges:

\begin{Verbatim}
?- of (lam T a\ P a) (arr (sort I) a\ arr a _\ a)
\end{Verbatim}

The rule for $\lambda$-abstraction applies fine, but generates the problematic
goal \verb+of P (arr x _\ x)+ for some fresh \verb+x+.  Then, the type checking
rule for application, which head is \verb+of (app M N) BN+, applies
indefinitely.  

The \verb+of+ predicate inherits from Prolog a generative semantics:
when called recursively on a flexible input, it enumerates all
instances trying to find the ones that are well-typed. Even when 
the computation does not diverge, the behaviour obtained is not the one
expected from an elaborator for an \emph{interactive} prover: the
elaborator is not meant to fill in the term, unless the choice is
obliged. On the contrary, it should leave the metavariable not
instantiated and should \emph{remember} (in some kind of metasenv) the need for verifying if the
typing judgement holds later on, when the metavariable gets instantiated. In
the example above, type-checking \verb+(lam T a\ P a)+ forces the
system to remember that term of type \verb+(arr x _\ x)+ has to be
provided (in a context where \verb+of x (sort I)+ holds), that in turn
corresponds to the proof obligation $A : \mbox{sort I} \vdash A \Rightarrow A $.

As we mentioned earlier the sometimes undesired generative semantics
is inherited from Prolog.
Nevertheless, all modern Prolog engines provide a \verb+var/1+
built-in to test/guard predicates against flexible input, provide
one/many variants of \verb+delay/2+ to suspend a goal until the ``input''
becomes rigid, and provide modes declarations to both
statically/dynamically detect problematic goals and to (semi)
automatically suspend them.
For example, by using the \verb+delay+ pack of SWI-Prolog~\cite{SWI}, the goal
\verb+plus(X,1,Y)+ is delayed until either \verb+X+ or \verb+Y+ are
instantiated.  Delayed goals can be thought as \emph{constraints} over the
metavariables occurring in them. In the example above, the programmer is
imposing a constraint between \verb+X+ and \verb+Y+.

These mechanisms, however, have never been standardized. Some of them
break the clean declarative semantics of $\lambda$Prolog, others
respect it. $\lambda$Prolog does not provide any of these facilities,
or at least, does not expose them to the programmer. For example, the Teyjus
system delays unification problems outside L$_\lambda$ and implements a complex
machinery to wake up delayed goals that re-enter the fragment, but does not
expose any primitive to delay other kind of goals.

In the light of all these considerations we extend the ELPI
$\lambda$Prolog interpreter with a \verb+delay+ directive that we can use
as follows:

\begin{Verbatim}
delay (of X T) on X.
\end{Verbatim}

The interpreter now delays goals of the form \verb+of X T+ whenever \verb+X+
is flexible. Instead of diverging, the running example now
terminates leaving the following (suspended) goal unsolved:

\begin{Verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
\end{Verbatim}

Such suspended goal is to be seen as a typing \emph{constraint} on
assignments to \verb+P+: when \verb+P+ gets instantiated by a
term \verb+t+, the goal \verb+of (t x) (arr x _\ x)+ is resumed
and \verb+(t x)+ is checked to be of type \verb+(arr x _\ x)+.
In turn such check can either:
\begin{description}
\item[terminate] successfully if \verb+t+ has the right type and is ground,
	e.g. \verb+t = x\lam x w\ w+.  If such assignment for \verb+P+
	comes from a proof command, then it corresponds to a proof
	step that closes the goal.
\item[fail] rejecting the proposed assignment for \verb+P+, e.g. 
	\verb+t = x\lam x w\ x+.  This corresponds to an erroneous proof
	step.
\item[advance] and generate one or more new constraints if \verb+t+ is partial
	e.g. \verb+t = x\lam x w\Q x w+.  If \verb+t+ is generated by
	a proof command, then this corresponds to a proof step that
	opens one or more new goals.
\end{description}
The three scenarios above perfectly match the desired behavior of an
elaborator.  In addition to that, the set of delayed goals implicitly kept
by the language interpreter
represents faithfully the metasenv data structure, relieving the
programmer to manage it himself, threading it through all typing rules and
managing explicitly backtracking and goal resumption.  Moreover, the automatic resumption of typing
constraints makes it impossible to obtain an ill-typed term by
instantiation: the code is correct by construction.  Lastly, it lets one selectively disable the undesired generative semantics of predicates like \verb+of+, \verb+lt+, \ldots.

%\subsection{Constraint propagation as meta-theorems on ground terms} %%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Constraint propagation as meta-theorems on ground terms.} %%%%%%%%%%%%%%%%%%%%%%%%%%

In many situations, the constraints that accumulate over time are not
independent, and sometimes sets of constraints can be rewritten in
order to simplify them, or detect their unsatisfiability and
backtrack.
For example, if our metavariable \verb+P+ does not occur linearly, one 
can end up with two distinct typing constraints on it:

\begin{Verbatim}
of x (sort I) ?- of (P x) (arr x _\ x)
of z (sort I) ?- of (P z) (arr (T z) y\ S z y)
\end{Verbatim}

If the object language features uniqueness of typing, as it is the case
for CIC, one surely wants to get rid of the second constraint, and
force \verb+T = a\ a+ and \verb+S = a\ b\ a+.
This way, when \verb+P+ gets instantiated \emph{only one goal}
is resumed (to type check the assigned term).  Alternatively,
if the two typing constraints clash, one wants the elaborator to
backtrack immediately.

Languages (or libraries for existing languages) that allow constraints declaration usually
come with ad-hoc \emph{constraint solvers} that take care of
propagating a specific class of constraints (arithmetic, finite
domain, etc\ldots), and are then called \emph{Constraint
Programming} (CP) languages.  For example, the set $\{0 \leq N \leq
4, 2 \leq N \leq 5\}$ can be rewritten into $\{2 \leq N \leq 4\}$
without changing the set of ground solutions.

Most CPs do not allow the user to define new constraints and
propagation rules in user space: only constraints belonging to well-known categories can effectively be used.  In our case the constraints
originate from the meta-theory of the object language we are implementing,
hence the programmer must be able to declare new kind of constraints and
specify how constraints are to rewritten according to meta-theorems about the
object language (e.g. unicity of typing).

Languages to describe sets of rules to manipulate a set of
constraints have been studied in the literature, and the most well-known one is certainly CHR (Constraint Handling Rules~\cite{chr}).
CHR is a first-order language in which the user declares predicates
and then gives a set of rewriting rules of the form $S_1 \setminus S_2 ~|~
G \iff S_3$ whose declarative semantics is that $\bigwedge S_1 \wedge
G \Rightarrow (\bigwedge S_2 \iff \bigwedge S_3)$ and whose
operational semantics is to match the current set of constraints
against both $S_1$ and $S_2$ and, if the clause $G$ holds, replace the
constraints in $S_2$ with the ones in $S_3$. All syntactic components
can be omitted: $G$ defaults to \verb+true+ and the sets $S_1,S_2,S_3$
to the empty set. The $\iff$ symbol is also omitted when $S_3$ is, and
$|$ is omitted when $G$ is. The semantics of the language is completed
by a strategy that fixes the order in which propagation rules are
fired and in what cases propagation rules can be backtracked.

Adding to a $\lambda$Prolog with a \verb+delay+ construct (to declare constraints) a CHR-like language to declare constraint propagation rules
enables the following applications:
\begin{description}
\item[forward reasoning during search] When propagation rules are theorems
        over the ground instances,
	then propagation respects the declarative, proof theoretic semantics
        of $\lambda$Prolog. In particular, constraint propagation is just a
        controlled form of forward reasoning, while SLD resolution only
	does backward reasoning.
\item[code optimizations given by the meta-theory] of the object
	language.  In the running example, uniqueness of typing is key to keep
	the implicit metasenv linear in the number of missing sub-proofs.
        In turn, this greatly reduces the number of checks to be performed when
        a metavariable is instantiated, and it allows to directly present to
        the user the set of constraints as the frontier of the proof search.
\item[turn checking into inference]  By replacing the
	predicates \verb+<+, \verb+max+, \verb+succ+ with generic 
	order constraints
	\verb+leq+, \verb+ltn+ one gets an elaborator that works with 
	floating sorts~\cite{algebraic},
	rather than fixed integers, and maintains an acyclic graph of
	constraints linking these. More generically, we can turn type checking into type inference and elaboration.
\end{description}

In the light of all these considerations we extend the ELPI
$\lambda$Prolog interpreter with a higher order constraint propagation
language inspired by CHR.

The following propagation rule is valid code, and implements
uniqueness of typing:

\begin{Verbatim}
constraint of {
  rule (G1 ?- of X T1) \ (G2 ?- of Y T2) > X ~ Y
       | (equiv G1 G2) <=> (G1 => conv T1 T2).
}
\end{Verbatim}

Here \verb+G1 ?- of X T1+ is a constraint (delayed goal) required
to be present, \verb+G2 ?- of Y T2+ is another one we want to remove.
The type of \verb+G1+ and \verb+G2+ is \verb+list o+, i.e.
it is a first class representation of the $\lambda$Prolog context
(that augments the original program).  
The guard \verb+equiv G1 G2+ is a user defined $\lambda$Prolog program
checking that the two contexts are equivalent (i.e.
contain the same items, disregarding order and multiplicity).  \verb+G1 => conv T1 T2+ (syntactic sugar for \verb+H1 => ... => HN => conv T1 T2+ where
\verb+G1 = [H1, ..., HN]+) is the new goal generated by the rule.
The alignment expressions, \verb+> X ~ Y+, asserts \verb+X+ and \verb+Y+
are the same metavariable and finds a bijection between the
eigenvariables (introduced by \verb+pi+ rules) in the two goals.

We say that CHR rules run at the \emph{meta-level}, i.e. they have access
to the syntax of goals and to their context.  In particular, all flexible
terms in a constraint are replaced by fresh constants before being inspected
by a CHR rule (we call this operation freezing). Therefore, the propagation
rules can inspect the goal, compare frozen metavariables, detect frozen flexible terms etc. without triggering the instantiation of the metavariable. As a comparison, detecting if a term is flexible cannot be done reliably in $\lambda$Prolog. An interpreter for the language can take the meta-level metaphor literally: we implemented constraint propagation by starting a new interpreter that takes in input the guard of the propagation rule and a reflection of the syntax of the goals of the interpreter below. In principle, we could even delay goals in the meta-level and propagate them using a meta-meta-level.

The bijection described by the alignment expression is key to
manipulate in the same guard (or combine in the new goal) terms
living in different contexts.

As an implementation detail, heads are \emph{matched} against the
constraints by the interpreter, and only when matching and alignment succeed,
freezing takes place, the meta-interpreter is started and the guard is run.

When the meta-interpreter halts successfully on the guard, the new goal is
defrost and it is immediately scheduled for execution in the interpreter.
Defrosting restores the metavariables originally present in the constraints, and it also creates a new metavariable (in the interpreter) for each new metavariable (in the meta-intepreter) that was generated by executing the guard.

For example, if we take the two typing constraints on \verb+P+ above
and we freeze them, we obtain:

\begin{Verbatim}
of x (sort c0) ?- of (c1 x) (arr x _\ x)
of z (sort c0) ?- of (c1 z) (arr (c2 z) y\ c3 z y)
\end{Verbatim}

Note that equal metavariables are frozen with the same fresh
constant.  Eigenvariables are left untouched by freezing, it is
the work of the alignment phase to make sense of them.
In this simple case there is only one possible bijection
between the two (singleton) sets of eigenvariables: 
$x \mapsto z$.

The following query is run in the meta-interpreter to validate the guard of the rule.

\begin{Verbatim}
?- equiv [of z (sort c0)] [of z (sort c0)]
\end{Verbatim}

Finally the following  goal is generated and scheduled for the next
SLD resolution step.

\begin{Verbatim}
of z (sort I) => conv (arr z _\ z) (arr (T z) y\ S z y)
\end{Verbatim}

The new goal eventually assigns \verb+T+ and \verb+S+, and its success
or failure is semantically equivalent to the satisfiability of the
second constraint on \verb+P+, that is removed.

%\subsubsection{Automatic alignment of goals}
\paragraph{Automatic alignment of goals.}
\label{sec:realchr}

Matching of $\lambda$Prolog goals is a delicate matter: eigenvariables
are fresh names, not fixed constants, and their semantics is quite
different~\cite{nabla}.  
In particular, matching goal $H$ against goal $G$ amounts
to solving this equation written using the nabla quantifier of~\cite{nabla}:
$
 \nabla x_1\ldots x_n, \mathcal{H} = \nabla y_1\ldots y_m, \mathcal{G}
$.

In the general case it is closely related to equivariate unification
that is known to be a NP hard~\cite{Cheney2004} problem.  While being
an elegant high-level language, CHR is not efficient.  It is estimated
be on average 100 times slower than conventional programming
languages.  If, by handling higher order constraints, we make its core
operation, matching, even more expensive, we are unlikely to obtain a
working system.

Providentially the use case that drove our design seems to be on a lucky
spot: all relevant names are visible by the key of the delayed goal.
Indeed, in the Curry-Howard homomorphism, a metavariable represents a
sequent, and its proof (and type) can only use variables in scope of
the metavariable.  The design choice we make is to forbid the
$\nabla$ quantifier in the patterns of CHR rules, and implicitly consider as many
$\nabla$ quantifications as eigenvariables visible to the key variable
of the constraint being matched by the CHR head.  This way we can impose
the trivial bijection between the names in the pattern and the names
in the constraint.  Also, all patterns use distinct metavariables, so
all injections are trivial.  In other words, names are only dealt with
in the alignment.
We support two form of alignments: one ``automatic'' based on the
L$_\lambda$ invariant and one ``manual'' for programs that go
outside that fragment.

In L$_\lambda$ the variables visible by each constraint key are distinct,
and equating the keys gives a bijection between the names.
If a CHR rue has $n$ patterns, and each corresponding goal has $m$
eigenvariables, then the alignment is computed in $O(m^n)$

The manual alignment injects all eigenvariables to the disjoint union
of the eigenvariables visible by each constraint key.  In other words
the terms bound by the CHR rule's heads share no names: it is
up to the programmer to eventually relate the names.
In this case we also let the programmer access the list of terms
to which the metavariable is applied with the following syntax:
\verb+(?? K L as X)+.  Here \verb+K+ is the (frozen) metavariable,
\verb+L+ is the list of terms to which it is applied and \verb+X+ is just
\verb+K+ applied to \verb+L+. Because we are at the meta-level, this kind of
inspection of the syntax is again perfectly reasonable and allowed.
We put this feature to good use in the next section.

% \subsection{Meta level: elegant but overkill}
\paragraph{Meta level: elegant but overkill.}

The operational semantics of the rule
$S_1 \setminus S_2 ~|~ G \iff S_3$ replaces $S_2$ with $S_3$.
Therefore its soundness only requires
$\bigwedge S_1 \wedge G \Rightarrow (\bigwedge
S_2 \Leftarrow \bigwedge S_3)$. When it is not the case that also
$\bigwedge S_1 \wedge G \Rightarrow (\bigwedge S_2 \Rightarrow \bigwedge S_3)$,
the rewriting rule is not complete or, equivalently, the user is only specifying and heuristic that potentially throws away solutions.

Heuristics are something the user surely wants to
do when extending the elaborator.  Our running example
is the following emblematic conversion (or, better, narrowing) problem

\begin{Verbatim}
conv (app carrier_of R) integers
\end{Verbatim}

In CIC a term can pack together types, terms and properties.  In the
example above \verb+R+ of type \verb+group+ is to be instantiated with a record
that packs together the group carrier, the group operations and their properties.
In this setting one can clearly see that the generative mode of $\lambda$Prolog corresponds to proof search, i.e. the blind enumeration of $\lambda$-terms to build (uninteresting) groups over the integers.  Of course the user is likely to have already built, in his/her library, the standard ring of integers, and he would like to instruct the system with the heuristics that pick for \verb+R+ that group. The code to do so is the first propagation rule below:

\begin{Verbatim}
delay (conv (app carrier_of X) Y) on X.
constraint conv {
  rule \ (conv (carrier_of X) integers) <=> (X = integer_group).
  rule \ (conv (carrier_of X) (intersection A B)) <=>
    (conv (carrier_of GA) A, conv (carrier_of GB) B,
     X = intersection_group GA GB).
}
\end{Verbatim}

The second rule is recursive and re-phrases the theorem saying
that the intersection of two groups is a group: in order to find a group \verb+X+ whose carrier is the intersection of the two sets \verb+A+ and \verb+B+, the heuristic suggests to recursively discover two groups \verb+GA+ and \verb+GB+ of carrier \verb+A+ and \verb+B+, and to instantiate \verb+X+ with the group intersection of \verb+GA+ and \verb+GB+.

Remark how little we use of CHR in this rule: only one head, no alignment,
no guard (i.e. no need to freeze).
Still, these simple rules faithfully model the extension to the elaborator
of Coq that proved to be key to outstanding formalizations like
the Odd Order Theorem~\cite{oothm}.  Such formalization declares a 26
hundreds of rules like these ones.  It goes without saying that
the efficiency of such rules is critical.

To achieve efficiency, in this simple scenario we would like to avoid delaying the goal, matching it with a propagation rule, execute the guard at the meta-level, etc. To do so, we will expose in the language the low level primitives that are used in the interpreter to implement constraint generation and propagation. The idea is that, in the restricted scenario above, the user will be able to directly write the propagation rule in the interpreter.

The first primitive is called \verb+mode+ and lets one tag arguments of
a predicate as input ones.

\begin{Verbatim}
mode (of i o).
\end{Verbatim}

The result is that arguments marked as input are \emph{matched}
against the corresponding arguments in the goal, exactly as the
CHR engine matches the heads of a rule against constraints (without
instantiating any metavariable occurring in the constraints).
This let us write programs like the following one without risking that
the first two rules ``generate'' the input.  Of course the right
hand side of the clauses can instantiate metavariables.

\begin{Verbatim}
of (lam T F) T :- ...
of (app M N) T :- ...
of X T :- var X, $delay (of X T) [X].
\end{Verbatim}

The last rule is the actual implementation of the global directive
\verb+delay .. on ..+ we described before.  Note that this way the
condition to identify goals to delay can be made arbitrarily
sophisticated.
Also remark that the mode declaration for \verb+of+
is not equivalent to
place a clause like \verb+(of X T :- var A, !, ...)+
on top, 
since hypothetical clauses may be placed by the runtime before this one,
and they can be generative as well.

The syntax \verb+(?? K L as X)+ seen in \ref{sec:realchr} is also
available, and lets one access the arguments of a flexible term.
This lets one extend the conversion predicate (unification in the
elaborator) to cover terms outside L$_\lambda$.  An emblematic example is
the following one:

\begin{Verbatim}
rule (G ?- conv (?? F [nat_of_ord x]) T)
  | ((pi y\ copy (nat_of_ord x) y => copy T (T' y)),
     (pi w\ copy (T' x) (T' w))) <=> (F = T')
\end{Verbatim}

\noindent
where \verb+x+ never occurs in \verb+T+ alone, only inside the
\verb+nat_of_ord+ context: \verb+T'+ is \verb+T+ where all occurrences
of \verb+nat_of_ord x+ are bound.  Such example occurs very frequently
in the library of big operators of Coq~\cite{bigop}, where theorems about iterated
operations over finite domains are provided.
For example the $\sum_{i < 7} (F~i)$ expression hides
the \verb+nat_of_ord+ injection around $i$ (a term of the
type of $I_7$, a finite subset of the integers of cardinality $7$).
The impossibility to extend the elaborator of Coq with any heuristic
to solve the goal above makes the use of these lemmas painful, since
one may have to provide \verb+F+ by hand, instead of having it
automatically inferred.

In Section~\ref{sec:cicelaborator} we use the low level primitives \verb+mode+, \verb+delay+ and \verb+var+ to implement our elaborator. In some cases, we combine \verb+mode+ and \verb+delay+ just to implement the high-level \verb+delay+ construct. In some other cases we use \verb+mode+ and pattern matching over the syntax of goals to implement heuristics without triggering delay. 

In all cases, the low level primitives above can be understood in term of the high-level language provided before. In particular, semantically \verb+mode+ already acts as the request of delaying the goal when certain inputs are flexible. Then, in the goals that match metavariables, one can either confirm the intention of delaying (via \verb+$delay+), or he can provide a propagation rule by immediately issuing a new query to be executed by the interpreter. In the future we plan to study the elaboration of the syntax based on the low level primitives to the high-level syntax and its clean declarative semantics.

%\subsection{ELPI = $\lambda$Prolog + CHR}\label{sec:elpi}
\paragraph{ELPI = $\lambda$Prolog + CHR.}\label{sec:elpi}

We implemented the language described in the previous sections in an efficient interpreter, written in OCaml, that we called ELPI (Embedded Lambda Prolog Interpreter) and that is open source and downloadable from the Web.
In addition to the new programming constructs that deal with
constraints, ELPI slightly differs from the version of $\lambda$Prolog
implemented in Teyjus in a few minor aspects.

First, in ELPI all types and type and sort declarations can be omitted, whereas
	they are mandatory in Teyjus. Originally, and according
	to~\cite{jlp98}, types were necessary to implement Huet's algorithm for
	higher order unification. However, for several years now the
	unification algorithm of Teyjus only solves equations in the pattern
	fragment $L_\lambda$ discovered by Miller~\cite{patternfrag}, which admits most
	general unifiers and reduce unification to a decidable problem. All
	other equations are automatically delayed by Teyjus to be fired only
	when the equation is instantiated to one in the pattern fragment. At
	the level of the implementation, the code of ELPI that implements this
	delay is shared with the one to delay arbitrary predicates.

Second, the module system of Teyjus is not implemented. Only the
	\verb+accumulate+ directive is honored for backward compatibility and
	with a different semantics. In ELPI we provide instead explicit
	existentially quantified local constants whose scope spans over a set
	of program clauses. This mechanism gives in a simple way predicate and
	constructor hiding that are provided differently by the module system
	of Teyjus.

	Last, in a few corner cases, the parsing of an expression by Teyjus is
	influenced by the types. In particular, types are used to disambiguate
	between lists of elements and a singleton list of conjunctions. The
	syntax is disambiguated in a different way in ELPI.

Despite the differences above, we tried very hard to maintain backward
compatibility with Teyjus and its standard library. Indeed, we are able to
execute in ELPI all the code from Teyjus that we collected from the Web, up to
a very few minor changes to the source code.
Last, ELPI is a pure interpreter written in OCaml.  Embedding it into
larger applications like Coq or Matita is easy (no external program to
run, no compilation chain).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Towards an elaborator for CIC}\label{sec:cicelaborator}
We are developing an elaborator for CIC as a modular extension of the kernel described in Sect.~\ref{sec:kernel}. The elaborator mimics as close as possible the behaviour of the one of Matita 0.99.1~\cite{bidir}, with the exception of the handling of universe constraints that follows Coq~\cite{algebraic}. For the time being, however, the elaborator can only instantiate metavariables that appear in the term, lacking the possibility of entirely modifying the input term, for example introducing type casts. To overcome this limitation, it is no longer possible to reuse the kernel type-checking rules as they are now: the type-checking judgement itself must give in output a new term, that is the elaboration of the term in input. All the rules of the kernel would just copy the input term in output verbatim, leaving to additional rules in the elaborator the introduction of type casts.

In place of following the algorithm of Matita and Coq, an alternative very promising choice would have been to mimic the elaborator described in~\cite{abel} and already presented via typing rules that yield a set of higher order unification constraints to be solved later. In the future, the choice to be closer to Matita will allow us to easily compare the performances of the elaborator written in ELPI with the one of Matita written in OCaml, in order to further optimize the ELPI interpreter.

To implement the elaborator, we need to consider all the predicates defined by induction over the shape of terms, and either turn them into constraints to be propagated, or immediately suggest solutions.

\paragraph{Type-checking: the \texttt{of} predicate.}
Using a mode declaration plus \verb+$delay+, we delay type-checking a
metavariable, turning it into a proof obligation, i.e. a sequent of the form
\verb+G ?- of X t+
where \verb+G+ holds type declarations for variables (\verb+of x t+) or value assignments (\verb+val x t v nf+).

\begin{Verbatim}
mode (of i o).
of (?? as K) T :- !, $delay (of K T) K.
\end{Verbatim}

We then declare one constraint propagation rule that corresponds to a special case of uniqueness of typing in the case of dependently typed languages: one of the two obligations is \emph{canonical}, i.e. it is of the form \verb+G ?- of (X x1 .. xn) t+ where all the \verb+xi+ are distinct variables or, equivalently, when \verb+X x1 .. xn+ is in the pattern fragment $L_\lambda$ discovered by Miller.

The restriction may seems severe, but, once a canonical typing constraint enters the set of delayed goals, it is simple to reduce the whole set to just the canonical one plus additional conversion constraints. Moreover, the first time a metavariable is generated in an interactive prover like Matita its typing constraint is canonical by construction. Therefore, it is customary both in theory~\cite{jojgov} and implementation~\cite{bidir} to always keep only canonical constraints, that are collected in a set called \emph{metavariable environment} (metasenv) and that are the only proof obligations presented to the user.

The propagation works as follow: let \verb+G1 ?- of (X x1 .. xn) u1+ be the canonical sequent and \verb+G2 ?- of (X t1 .. tn) u2+ be the one to be simplified. First we compute the type of \verb+x1+ in \verb+G1+ and of \verb+t1+ in \verb+G2+, imposing as a new constraint their convertibility in the union of \verb+G1+ and \verb+G2+. Then we proceed recursively over \verb+x2 .. xn+ and \verb+t2 .. tn+ after assigning \verb+t1+ to \verb+x1+ via \verb+val+ in \verb+G1+. When the two lists become empty, we generate a final constraint to check if \verb+u1+ is convertible with \verb+u2+.

To detect canonicity, we use the ad-hoc extension predicate \verb+name+ of ELPI that holds iff the argument is a universal variable. Recall tat the syntax \verb+G => L+ when \verb+G+ is a list of propositions is equivalent in ELPI to assuming the conjunction of the predicates in \verb+G+.

\begin{Verbatim}
constraint of val {
  rule (G1 ?- of (?? X1 L1) T1) \ (G2 ?- of (?? X2 L2) T2) > X1 ~ X2
   | (is_canonical L1, compat G1 L1 T1 G2 L2 T2 L3) <=> L3.
}

is_canonical [].
is_canonical [X|XS] :- name X, not (mem X XS), is_canonical XS.

% (G2 ?- T2) is the canonical one
compat G2 [] T2 G1 [] T1 H :- append G1 G2 G12, H = (G12 => conv T2 T1).

compat G2 [X2|XS2] T2 G1 [X1|XS1] T1 K :-
 H1 = (G1 => of X1 U1),
 H2 = (G2 => of X2 U2),
 append G1 G2 G12, H3 = (G12 => conv U1 U2),
 compat G1 XS1 T1 [val X2 U2 X1 _NF|G2] XS2 T2 K2,
 K = (H1, H2, H3, K2).
\end{Verbatim}

Last, remark that the alignment mode selected only checks
that \verb+X1+ and \verb+X2+ are the same metavariable and
makes all eigenvariables distinct.

\paragraph{Universe constraints: the \texttt{lt}, \texttt{succ}, \texttt{max} predicates.}
All three predicates must be delayed when at least one of the arguments is flexible. However, satisfiability of the constraints is a necessary requirement for logical consistency. In case of violation of the constraints, it is indeed easy to encode a form of Russel's paradox.

In order to verify satisfiability, we could devise a set of propagation rules
for constraints over integers induced by the last three predicates. However, to preserve the logical consistency of the system, it is not necessary to keep the total, discrete order of integers. On the contrary, it is more flexible for the user to assume a generic partially ordered set \verb+(univt,lteq)+, and to relax the successor relation to being strictly greater and the max to a generic upper bound. Satisfiability of the set is now equivalent to the absence of a strict cycle, i.e. to the possibility to derive \verb+ltn U U+ for some universe \verb+U+.

Detecting an inconsistency from a set of constraints expressed using strict and lax inequalities is such an easy job for CHR that we basically just had to modify the ``hello world'' example for CHR given on Wikipedia, that simplifies constraints over lax inequalities only. Each constraint propagation rule corresponds to an instance of a characterizing property of the order, i.e. reflexivity, transivity and antisimmetry of \verb+leq+, transitivity and antiriflexivity of \verb+ltn+, and finally inconsistency of \verb+ltn X Y+ with both \verb+ltn Y X+ and \verb+leq Y X+. We only show in the following code a few propagation rules.

\begin{Verbatim}
kind univt type.
macro @univ :- univt.

lt    I J :- ltn I J.
succ  I J :- ltn I J.           % succ relaxed to <
max N M X :- leq N X, leq M X.  % max relaxed to any upper bound

... /* boilerplate code to delay leq and ltn over flexible terms */

constraint leq ltn {
  % incompatibility and irreflexivity    
  rule (leq X Y) (ltn Y X) <=> false.  rule (ltn X X) <=> false.
  % reflexivity, antisymmetry, ...
  rule \ (leq X X). rule (leq X Y) \ (leq Y X) <=> (Y = X). ...
}
\end{Verbatim}

\paragraph{Reduction: the \texttt{whd1}, \texttt{whd*} predicates.}
The predicate \verb+whd*+, defined in terms of \verb+whd1+, computes the normal form of the input. It is used by \verb+match_sort+ and \verb+match_arr+ to verify if the normal form has a given shape. Moreover, it calls itself recursively to compute the normal form of arguments according to the call-by-need strategy.

The typing system we are implementing cannot distinguish between a term and its normal form. Therefore, when computing the normal form of a flexible input \verb+X+, it is meaningless to delay a goal stating that \verb+Y+ is the normal form of \verb+X+, because typing does not distinguish them.
Therefore, we just return \verb+X+ as the normal form of \verb+X+ by letting \verb+whd1+ fail over flexible terms.
\begin{Verbatim}
mode (whd1 i i o).
whd1 ?? _ _ :- fail.
\end{Verbatim}

The consequence on the strategy is that, under certain alternations of reductions and instantiations of metavariables, the implemented strategy will be intermediate between call-by-name and call-by-need, recomputing the normal form of arguments that were metavariables at the time of their first use, with no consequences on typability.

On \verb+match_sort+ and \verb+match_arr+ there is no consequence: the metavariable is immediately assigned the wanted shape, meaning that we have decided to instantiate the metavariable immediately with its normal form.

%\paragraph{Term matching: the \texttt{match\_sort,match\_arr} predicates.}
%The predicates check if the normal form of the input has a particular shape.
%If the input is flexible, it is sufficient to instantiate it with the wanted normal form, because the type system cannot distinguish between a term and its reduct, and therefore it is pointless to give in output any solution that is not in normal form. In the code we can immediately provide the answer without delaying the goal.
%
%\begin{Verbatim}
%mode (match_sort i o).
%match_sort (?? as T) U :- !, T = sort U.
%
%mode (match_arr i o o).
%match_arr (?? as T) A F :- !, T = arr A F.
%
%mode (whd1 i i o).
%whd1 T S K :- fail.
%\end{Verbatim}

\paragraph{Term comparison: the \texttt{sub}, \texttt{conv} predicates.}

Conversion when at least one of the arguments is a flexible term amounts to higher order narrowing. Instead of delaying the goal, an heuristic already used in Matita immediately rewrites the constraint by solving the unification problem. The heuristic always favours projections to mimic (in Huet's terminology). Further heuristics are used in Matita to avoid projecting over flexible arguments, to make unification more predictable to the user. These heuristics are clearly incomplete, discarding all solutions but one and sometimes failing to find a solution when it exists. However, they are more general than the ones implemented in Coq and, from a practical perspective, they guess most of the time the unifier that is expected by the user when interacting with the ITP.

The following code shows the clauses that deal with a flexible term (on the left-hand side) to be unified with a rigid one on the right-hand side. A simmetric set of rules is required for the dual case, and the code can obviously be unified with minor effort. Additional rules, omitted here, are required to deal with the flexible-flexible case.

\begin{Verbatim}
mode (comp i i i i i).

% T1 :- lam TYA F  + beta step
comp (?? as T1) [A|AS] M T2 L2 :-
 of A TYA, T1 = lam TYA F, pi x \ val x TYA A _NF => comp (F x) AS M T2 L2.

% PROJECTION
comp (?? as V1) [] M T2 S2 :- val X _ _ _, V1 = X, comp V1 [] M T2 S2, !.

% MIMIC
% non empty stack = application
comp (?? as V1) [] _ T2 S2 :-
 append S21 [S2l] S2,
 V1 = app X Y, comp X [] eq T2 S21, comp Y [] eq S2l [].
% regular mimic rules
comp (?? as V1) [] M T2 [] :- mcomp V1 M T2. % mcomp used generatively
% variables and constants
comp (?? as V1) [] M T2 [] :- V1 = T2.

% REDUCTION (same rule as the kernel)
comp (?? as V1) [] D T2 S2 :- whd1 T2 S2 (_ \ t2 \ s2 \ comp V1 [] D t2 s2).

% FAIL
comp ?? [] _ _ _ :- !, fail. % to avoid the fast path of the kernel

% MIMIC RULES (same rules as the kernel)
mcomp (sort I) eq (sort I) :- !.
mcomp (sort J) leq (sort I) :- !, leq J I.
mcomp (app A1 B1) _ (app A2 B2) :- !, comp A1 [] eq A2 [], comp B1 [] eq B2 [].
mcomp (lam TY1 F1) _ (lam TY2 F2) :-
 !, comp TY1 [] eq TY2 [], (pi x \ comp (F1 x) [] eq (F2 x) []).
mcomp (arr TY1 F1) M (arr TY2 F2) :-
 !, comp TY1 [] eq TY2 [], (pi x \ comp (F1 x) [] M (F2 x) []).
\end{Verbatim}

The code assumes all metavariable occurrences to be in $L_\lambda$. Otherwise, calls like \verb+V1 = app X Y+ in the code may be automatically delayed by ELPI, thus breaking the algorithm.

Several tests already pass, but we are currently still developing the full set of rules that reflect the algorithm developed by Matita. Once completed and made functional equivalent, we plan to test the elaborator written in $\lambda$Prolog against the original one for debugging and performance analysis.

\section{Conclusions and related works}\label{sec:conclusion}

In this paper we validated $\lambda$Prolog as a good language to implement
the type checker (kernel) of a fully fledged type theory like the Calculus of
Inductive Constructions.  In order to implement an elaborator, i.e. a type
checker for partial terms, we extended the $\lambda$Prolog with constructs to
turn goals into constraints and we add a CHR like language to manipulate
the set of constraints.  We implemented the proposed extensions in the ELPI
system and used them to extend, in a fully modular way, the kernel into an elaborator.

To our knowledge ELPI is the first $\lambda$Prolog implementation extended
with first class constraints.  The situation is very different for Prolog, where all
mainstream implementations integrate some constraint solvers.  Moreover,
the CHR language is typically compiled to Prolog, hence Prolog systems
can quite easily provide a CHR package.  It is for example the case for
SWI Prolog~\cite{SWI} and SICStus Prolog~\cite{SICStus}.  Providing to
the Prolog language primitives of the meta-level, like matching, has also
been tried before.  For example the Picat language, based on the
B-Prolog~\cite{Bprolog} engine, lets the user chose, for each clause,
if the head has to be matched or unified with the goal.

While basing the implementation of the kernel of an interactive prover on a
logical framework (LF) eventually animated by SLD resolution is not new, 
little has been done to reuse the same technology for the elaborator
component.  For example MMT~\cite{mmt} ``meta'' system lets one define kernel
rules in LF, but resorts to the Scala language for the elaborator.
Isabelle lets one describe the axioms of a logic in an LF framework and
exposes the higher order unification algorithm to the higher layers of
the system, like the proof language one.  Incidentally the dominant
logic implemented in Isabelle is HOL, that lacks dependent types, hence
needs no term comparison algorithm based on narrowing.  As a consequence,
no need to extend such algorithm to encompass more rewriting is perceived,
and no way to extend such algorithm is given to the user.
The Dudukti~\cite{dedukti} language lets one describe the axioms of a logic
in LF modulo equational theories.  Unfortunately, at the time of writing,
the system lacks an elaborator.

Describing the elaborator in terms of constraints was a choice first 
made in Agda version~2~\cite{Agda} and more recently in Lean~\cite{Lean}.
Both system implement they own, ad-hoc, constraint solver, the former system in
Haskell, while the latter in C++.  The approach we follow in the paper
is to provide a programming platform where constraints and their propagation
rules are first class, to ease documentation, experimentation and extension
of the implemented system.

A big advantage of writing the code in $\lambda$Prolog is the possibility of
using the Abella~\cite{Abella} theorem prover to mechanically check
its correctness.
In particular, Abella can be used to prove the soundness of all constraint propagation rules. For example, for an early prototype of the elaborator presented
in this paper, the typing constraint propagation rule implementing uniqueness of
typing was proved correct in Abella. However, the current version of Abella reasons on the big step operational semantics of $\lambda$Prolog programs without cuts. Because delaying a goal means interrupting execution in the middle, the small step version of the operational semantics would be the appropriate tool to use instead. For example, in the aforementioned proof of uniqueness of typing, we had to assume a certain invariant on the $\lambda$Prolog context that is the context of the delayed typing sequent. In order to know that the invariant really holds during computation, we need to prove that, if it holds initially, it holds after any number of small execution steps. Currently, this statement cannot even be stated in Abella. In the future, we would like to re-design Abella around a small step operational semantics to overcome this limitation and also to be able to reason over cuts.

We plan to compare in a disciplined way the performance of the elaborator
of Matita with the one presented in this paper, to precisely size the
trade-off between the programming abstractions offered by the
HOCLP language we have implemented and their runtime cost.

As another future work we hope in Abella to fully prove correct, although not complete, the
elaborator component, so to call into question the standard design of
interactive provers: what is the point of having a kernel component if the
elaborator is proved to only generate terms accepted by it?

\label{sect:bib}
\bibliographystyle{plain}
\bibliography{bib}

\end{document}
